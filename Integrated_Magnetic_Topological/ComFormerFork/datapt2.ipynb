{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining 3D dataset 76k ...\n",
      "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
      "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
      "Loading the zipfile...\n",
      "Loading completed.\n",
      "\n",
      "--- Merged DataFrame (from OPTIMADE) ---\n",
      "           jid mpid_optimade\n",
      "0      JVASP-1          None\n",
      "1      JVASP-2          None\n",
      "2   JVASP-1002          None\n",
      "3  JVASP-99999          None\n"
     ]
    },
    {
     "ename": "MPRestError",
     "evalue": "invalid fields requested: ['topological_type']. Available fields: ['builder_meta', 'nsites', 'elements', 'nelements', 'composition', 'composition_reduced', 'formula_pretty', 'formula_anonymous', 'chemsys', 'volume', 'density', 'density_atomic', 'symmetry', 'property_name', 'material_id', 'deprecated', 'deprecation_reasons', 'last_updated', 'origins', 'warnings', 'structure', 'task_ids', 'uncorrected_energy_per_atom', 'energy_per_atom', 'formation_energy_per_atom', 'energy_above_hull', 'is_stable', 'equilibrium_reaction_energy_per_atom', 'decomposes_to', 'xas', 'grain_boundaries', 'band_gap', 'cbm', 'vbm', 'efermi', 'is_gap_direct', 'is_metal', 'es_source_calc_id', 'bandstructure', 'dos', 'dos_energy_up', 'dos_energy_down', 'is_magnetic', 'ordering', 'total_magnetization', 'total_magnetization_normalized_vol', 'total_magnetization_normalized_formula_units', 'num_magnetic_sites', 'num_unique_magnetic_sites', 'types_of_magnetic_species', 'bulk_modulus', 'shear_modulus', 'universal_anisotropy', 'homogeneous_poisson', 'e_total', 'e_ionic', 'e_electronic', 'n', 'e_ij_max', 'weighted_surface_energy_EV_PER_ANG2', 'weighted_surface_energy', 'weighted_work_function', 'surface_anisotropy', 'shape_factor', 'has_reconstructed', 'possible_species', 'has_props', 'theoretical', 'database_IDs']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMPRestError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    179\u001b[39m mp_ids = df_merged_optimade[\u001b[33m\"\u001b[39m\u001b[33mmpid_optimade\u001b[39m\u001b[33m\"\u001b[39m].dropna().tolist()\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MPRester(API_KEY) \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     docs = \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaterials\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaterial_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmp_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaterial_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopological_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# docs = m.materials.summary.search(\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m#     material_ids=df[\"mpid\"].tolist(),\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m#     fields=[\"material_id\",\"topological_type\"]\u001b[39;00m\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    193\u001b[39m label_df = pd.DataFrame([{\n\u001b[32m    194\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmpid\u001b[39m\u001b[33m\"\u001b[39m: d.material_id,\n\u001b[32m    195\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtopo_type\u001b[39m\u001b[33m\"\u001b[39m: d.topological_type\n\u001b[32m    196\u001b[39m } \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/mp_api/client/routes/materials/summary.py:279\u001b[39m, in \u001b[36mSummaryRester.search\u001b[39m\u001b[34m(self, band_gap, chemsys, crystal_system, density, deprecated, e_electronic, e_ionic, e_total, efermi, elastic_anisotropy, elements, energy_above_hull, equilibrium_reaction_energy, exclude_elements, formation_energy, formula, g_reuss, g_voigt, g_vrh, has_props, has_reconstructed, is_gap_direct, is_metal, is_stable, k_reuss, k_voigt, k_vrh, magnetic_ordering, material_ids, n, num_elements, num_sites, num_magnetic_sites, num_unique_magnetic_sites, piezoelectric_modulus, poisson_ratio, possible_species, shape_factor, spacegroup_number, spacegroup_symbol, surface_energy_anisotropy, theoretical, total_energy, total_magnetization, total_magnetization_normalized_formula_units, total_magnetization_normalized_vol, uncorrected_energy, volume, weighted_surface_energy, weighted_work_function, include_gnome, num_chunks, chunk_size, all_fields, fields)\u001b[39m\n\u001b[32m    271\u001b[39m     query_params.update({\u001b[33m\"\u001b[39m\u001b[33mbatch_id_not_eq\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgnome_r2scan_statics\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m    273\u001b[39m query_params = {\n\u001b[32m    274\u001b[39m     entry: query_params[entry]\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m query_params\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_params[entry] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    277\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/mp_api/client/core/client.py:1191\u001b[39m, in \u001b[36mBaseRester._search\u001b[39m\u001b[34m(self, num_chunks, chunk_size, all_fields, fields, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"A generic search method to retrieve documents matching specific parameters.\u001b[39;00m\n\u001b[32m   1170\u001b[39m \n\u001b[32m   1171\u001b[39m \u001b[33;03mArguments:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m \u001b[33;03m    A list of documents.\u001b[39;00m\n\u001b[32m   1187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[38;5;66;03m# This method should be customized for each end point to give more user friendly,\u001b[39;00m\n\u001b[32m   1189\u001b[39m \u001b[38;5;66;03m# documented kwargs.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_all_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/mp_api/client/core/client.py:1264\u001b[39m, in \u001b[36mBaseRester._get_all_documents\u001b[39m\u001b[34m(self, query_params, all_fields, fields, chunk_size, num_chunks)\u001b[39m\n\u001b[32m   1250\u001b[39m list_entries = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m   1251\u001b[39m     (\n\u001b[32m   1252\u001b[39m         (key, \u001b[38;5;28mlen\u001b[39m(entry.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m   (...)\u001b[39m\u001b[32m   1259\u001b[39m     reverse=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1260\u001b[39m )\n\u001b[32m   1262\u001b[39m chosen_param = list_entries[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_entries) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query_resource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_param\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchosen_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/mp_api/client/core/client.py:464\u001b[39m, in \u001b[36mBaseRester._query_resource\u001b[39m\u001b[34m(self, criteria, fields, suburl, use_document_model, parallel_param, num_chunks, chunk_size, timeout)\u001b[39m\n\u001b[32m    460\u001b[39m         invalid_fields = [\n\u001b[32m    461\u001b[39m             f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields \u001b[38;5;28;01mif\u001b[39;00m f.split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.available_fields\n\u001b[32m    462\u001b[39m         ]\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m invalid_fields:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m MPRestError(\n\u001b[32m    465\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid fields requested: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Available fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.available_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m             )\n\u001b[32m    468\u001b[39m     criteria[\u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(fields)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mMPRestError\u001b[39m: invalid fields requested: ['topological_type']. Available fields: ['builder_meta', 'nsites', 'elements', 'nelements', 'composition', 'composition_reduced', 'formula_pretty', 'formula_anonymous', 'chemsys', 'volume', 'density', 'density_atomic', 'symmetry', 'property_name', 'material_id', 'deprecated', 'deprecation_reasons', 'last_updated', 'origins', 'warnings', 'structure', 'task_ids', 'uncorrected_energy_per_atom', 'energy_per_atom', 'formation_energy_per_atom', 'energy_above_hull', 'is_stable', 'equilibrium_reaction_energy_per_atom', 'decomposes_to', 'xas', 'grain_boundaries', 'band_gap', 'cbm', 'vbm', 'efermi', 'is_gap_direct', 'is_metal', 'es_source_calc_id', 'bandstructure', 'dos', 'dos_energy_up', 'dos_energy_down', 'is_magnetic', 'ordering', 'total_magnetization', 'total_magnetization_normalized_vol', 'total_magnetization_normalized_formula_units', 'num_magnetic_sites', 'num_unique_magnetic_sites', 'types_of_magnetic_species', 'bulk_modulus', 'shear_modulus', 'universal_anisotropy', 'homogeneous_poisson', 'e_total', 'e_ionic', 'e_electronic', 'n', 'e_ij_max', 'weighted_surface_energy_EV_PER_ANG2', 'weighted_surface_energy', 'weighted_work_function', 'surface_anisotropy', 'shape_factor', 'has_reconstructed', 'possible_species', 'has_props', 'theoretical', 'database_IDs']"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jarvis.core.atoms import Atoms\n",
    "import graph\n",
    "#from ComFormerFork.graph import PygGraph, PygStructureDataset\n",
    "from jarvis.db.figshare import data as jdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from jarvis.db.jsonutils import dumpjson\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# use pandas progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\"))\n",
    "API_KEY = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "dft3d = jdata(\"dft_3d\")               \n",
    "df = pd.DataFrame(dft3d)     \n",
    "\n",
    "# print(df.columns)\n",
    "# print(type(dft3d[0]))\n",
    "# try:\n",
    "#     print(dft3d[0].keys())          # if it’s a dict-like\n",
    "# except:\n",
    "#     print(vars(dft3d[0]))           # if it’s an object with attrs\n",
    "\n",
    "# from jarvis.db.restapi import Api\n",
    "\n",
    "# mp_map = []\n",
    "# for jid in df[\"jid\"]:\n",
    "#     entry = Api.get_data_by_template_id(\"dft_3d\", jid)\n",
    "#     # entry.db_ids is guaranteed to be a dict if it exists\n",
    "#     mp_map.append({\n",
    "#         \"jid\": jid,\n",
    "#         \"mpid\": entry.db_ids.get(\"materialsproject\")  # maybe None\n",
    "#     })\n",
    "# mp_df = pd.DataFrame(mp_map)\n",
    "\n",
    "# df = df.merge(mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "# 3) (Optional) drop any rows where mpid is missing\n",
    "#df = df.dropna(subset=[\"mpid\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df[\"mpid\"] = df[\"db_ids\"].apply(lambda x: x.get(\"materialsproject\") if isinstance(x, dict) else None)\n",
    "# df = df.dropna(subset=[\"mpid\"])        # keep only entries with an MP match\n",
    "\n",
    "\n",
    "# with MPRester(api_key = API_KEY) as m:\n",
    "#     docs = m.materials.summary.search(\n",
    "#         jid__in=df[\"jid\"].tolist(),\n",
    "#         fields=[\"jid\",\"topological_type\"]\n",
    "#     )\n",
    "\n",
    "#optimade\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# --- Start: MODIFIED Definition of jarvisdft_optimade ---\n",
    "def jarvisdft_optimade(\n",
    "    prefix=\"https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=\",\n",
    "    query=\"elements HAS ALL C,Si\",\n",
    "):\n",
    "    url = prefix + query\n",
    "    vals = []\n",
    "    print(f\"Querying URL: {url}\")\n",
    "    current_url_for_error = url # Store initial URL for error reporting\n",
    "\n",
    "    while True:\n",
    "        if url is not None:\n",
    "            current_url_for_error = url # Update for current page\n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                response.raise_for_status() # Check for HTTP errors (4xx or 5xx)\n",
    "\n",
    "                json_response = None # Initialize\n",
    "                if not response.content: # Check if response body is empty\n",
    "                    print(f\"  Warning: Empty response from {url}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        json_response = response.json()\n",
    "                    except requests.exceptions.JSONDecodeError as je:\n",
    "                        print(f\"  Error decoding JSON from {url}: {je}\")\n",
    "                        print(f\"  Response text: {response.text[:500]}...\")\n",
    "                \n",
    "                if json_response is None:\n",
    "                    print(f\"  Could not parse JSON or response was empty/problematic for {url}. Stopping pagination for this query.\")\n",
    "                    break # Break from the while True loop\n",
    "\n",
    "                data = json_response.get(\"data\", []) # Safely get data, defaults to []\n",
    "                for i in data:\n",
    "                    vals.append(i)\n",
    "                \n",
    "                # Safely get the 'next' link for pagination\n",
    "                links_object = json_response.get(\"links\", {})\n",
    "                if isinstance(links_object, dict):\n",
    "                    url = links_object.get(\"next\")\n",
    "                else: # links object is not a dictionary or is missing\n",
    "                    url = None \n",
    "                \n",
    "                if url:\n",
    "                    print(f\"  Fetching next page: {url}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Request error for {current_url_for_error}: {e}\")\n",
    "                break # Stop if there's a request error\n",
    "            except Exception as e_inner: # Catch any other unexpected error within the try\n",
    "                print(f\"  Unexpected error processing {current_url_for_error}: {e_inner}\")\n",
    "                break\n",
    "        else: # url is None, so no more pages\n",
    "            break\n",
    "        \n",
    "        # Be polite to the API, even if a page was problematic but we continue\n",
    "        time.sleep(0.2) \n",
    "    return vals\n",
    "# --- End: Definition of jarvisdft_optimade ---\n",
    "\n",
    "# Simulate your initial DataFrame with 'jid' column\n",
    "data = {'jid': ['JVASP-1', 'JVASP-2', 'JVASP-1002', 'JVASP-99999']} # Example JIDs\n",
    "# For testing with a JID that might exist and have an MPID cross-reference:\n",
    "# data = {'jid': ['JVASP-1063']} # Example: JVASP-1063 (Si) is mp-149\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- batch all JIDs into one OPTIMADE filter, request up to 100 results at once ---\n",
    "jids = df[\"jid\"].tolist()\n",
    "or_filters = \" OR \".join(f'(immutable_id=\"{jid}\")' for jid in jids)\n",
    "base_url = \"https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/\"\n",
    "url = f\"{base_url}?page_limit=100&filter={or_filters}\"\n",
    "\n",
    "resp = requests.get(url, timeout=30)\n",
    "resp.raise_for_status()\n",
    "entries = resp.json().get(\"data\", [])\n",
    "\n",
    "# build a dict: JARVIS ID → MPID\n",
    "mp_map = {}\n",
    "for e in entries:\n",
    "    attrs = e.get(\"attributes\", {})\n",
    "    for db_ref in attrs.get(\"external_databases\", []):\n",
    "        if db_ref.get(\"name\", \"\").lower() == \"materials project\":\n",
    "            mp_map[e.get(\"id\")] = db_ref.get(\"id\")\n",
    "            break\n",
    "\n",
    "# turn it into a DataFrame and merge back into your df\n",
    "optimade_mp_df = pd.DataFrame([\n",
    "    {\"jid\": jid, \"mpid_optimade\": mp_map.get(jid)}\n",
    "    for jid in jids\n",
    "])\n",
    "df_merged_optimade = df.merge(optimade_mp_df, on=\"jid\", how=\"left\")\n",
    "#print(df_merged_optimade)\n",
    "\n",
    "\n",
    "\n",
    "#optimade_mp_df = pd.DataFrame(mp_map_optimade)\n",
    "# Merge with the original df. df_merged_optimade will have 'jid' and 'mpid_optimade'\n",
    "#df_merged_optimade = df.merge(optimade_mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "print(\"\\n--- Merged DataFrame (from OPTIMADE) ---\")\n",
    "print(df_merged_optimade)\n",
    "\n",
    "\n",
    "mp_ids = df_merged_optimade[\"mpid_optimade\"].dropna().tolist()\n",
    "\n",
    "with MPRester(API_KEY) as m:\n",
    "    docs = m.materials.summary.search(\n",
    "        material_ids= mp_ids, \n",
    "        fields = [\"material_id\", \"topological_type\"]\n",
    "    )\n",
    "    # docs = m.materials.summary.search(\n",
    "    #     material_ids=df[\"mpid\"].tolist(),\n",
    "    #     fields=[\"material_id\",\"topological_type\"]\n",
    "    # )\n",
    "\n",
    "\n",
    "label_df = pd.DataFrame([{\n",
    "    \"mpid\": d.material_id,\n",
    "    \"topo_type\": d.topological_type\n",
    "} for d in docs])\n",
    "\n",
    "# map to binary\n",
    "good = {\"Z2_INSULATOR\",\"CRYSTALLINE_INSULATOR\"}\n",
    "label_df[\"is_topo\"] = label_df[\"topo_type\"].isin(good).astype(int)\n",
    "\n",
    "df = df.merge(label_df[[\"mpid\",\"is_topo\"]], on=\"mpid\", how=\"left\")\n",
    "df[\"is_topo\"] = df[\"is_topo\"].fillna(0).astype(int)\n",
    "\n",
    "def load_dataset(\n",
    "    name: str = \"dft_3d\",\n",
    "    target=None,\n",
    "    limit: Optional[int] = None,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"Load jarvis data.\"\"\"\n",
    "    d = jdata(name)\n",
    "    data = []\n",
    "    for i in d:\n",
    "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
    "            if classification_threshold is not None:\n",
    "                if i[target] <= classification_threshold:\n",
    "                    i[target] = 0\n",
    "                elif i[target] > classification_threshold:\n",
    "                    i[target] = 1\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Check classification data type.\",\n",
    "                        i[target],\n",
    "                        type(i[target]),\n",
    "                    )\n",
    "            data.append(i)\n",
    "    d = data\n",
    "    if limit is not None:\n",
    "        d = d[:limit]\n",
    "    d = pd.DataFrame(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def mean_absolute_deviation(data, axis=None):\n",
    "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def load_pyg_graphs(\n",
    "    df: pd.DataFrame,\n",
    "    name: str = \"dft_3d\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    cutoff: float = 8,\n",
    "    max_neighbors: int = 12,\n",
    "    cachedir: Optional[Path] = None,\n",
    "    use_canonize: bool = False,\n",
    "    use_lattice: bool = False,\n",
    "    use_angle: bool = False,\n",
    "):\n",
    "    \"\"\"Construct crystal graphs.\n",
    "\n",
    "    Load only atomic number node features\n",
    "    and bond displacement vector edge features.\n",
    "\n",
    "    Resulting graphs have scheme e.g.\n",
    "    ```\n",
    "    Graph(num_nodes=12, num_edges=156,\n",
    "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
    "          edata_schemes={'r': Scheme(shape=(3,)})\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def atoms_to_graph(atoms):\n",
    "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
    "        structure = Atoms.from_dict(atoms)\n",
    "        return PygGraph.atom_dgl_multigraph(\n",
    "            structure,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            cutoff=cutoff,\n",
    "            atom_features=\"atomic_number\",\n",
    "            max_neighbors=max_neighbors,\n",
    "            compute_line_graph=False,\n",
    "            use_canonize=use_canonize,\n",
    "            use_lattice=use_lattice,\n",
    "            use_angle=use_angle,\n",
    "        )\n",
    "    \n",
    "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
    "    # graphs = df[\"atoms\"].apply(atoms_to_graph).values\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def get_id_train_val_test(\n",
    "    total_size=1000,\n",
    "    split_seed=123,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    n_train=None,\n",
    "    n_test=None,\n",
    "    n_val=None,\n",
    "    keep_data_order=False,\n",
    "):\n",
    "    \"\"\"Get train, val, test IDs.\"\"\"\n",
    "    if (\n",
    "        train_ratio is None\n",
    "        and val_ratio is not None\n",
    "        and test_ratio is not None\n",
    "    ):\n",
    "        if train_ratio is None:\n",
    "            assert val_ratio + test_ratio < 1\n",
    "            train_ratio = 1 - val_ratio - test_ratio\n",
    "            print(\"Using rest of the dataset except the test and val sets.\")\n",
    "        else:\n",
    "            assert train_ratio + val_ratio + test_ratio <= 1\n",
    "    # indices = list(range(total_size))\n",
    "    if n_train is None:\n",
    "        n_train = int(train_ratio * total_size)\n",
    "    if n_test is None:\n",
    "        n_test = int(test_ratio * total_size)\n",
    "    if n_val is None:\n",
    "        n_val = int(val_ratio * total_size)\n",
    "    ids = list(np.arange(total_size))\n",
    "    if not keep_data_order:\n",
    "        random.seed(split_seed)\n",
    "        random.shuffle(ids)\n",
    "    if n_train + n_val + n_test > total_size:\n",
    "        raise ValueError(\n",
    "            \"Check total number of samples.\",\n",
    "            n_train + n_val + n_test,\n",
    "            \">\",\n",
    "            total_size,\n",
    "        )\n",
    "\n",
    "    id_train = ids[:n_train]\n",
    "    id_val = ids[-(n_val + n_test) : -n_test]\n",
    "    id_test = ids[-n_test:]\n",
    "    return id_train, id_val, id_test\n",
    "\n",
    "\n",
    "def get_pyg_dataset(\n",
    "    dataset=[],\n",
    "    id_tag=\"jid\",\n",
    "    target=\"\",\n",
    "    neighbor_strategy=\"\",\n",
    "    atom_features=\"\",\n",
    "    use_canonize=\"\",\n",
    "    name=\"\",\n",
    "    line_graph=\"\",\n",
    "    cutoff=8.0,\n",
    "    max_neighbors=12,\n",
    "    classification=False,\n",
    "    output_dir=\".\",\n",
    "    tmp_name=\"dataset\",\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    data_from='Jarvis',\n",
    "    use_save=False,\n",
    "    mean_train=None,\n",
    "    std_train=None,\n",
    "    now=False, # for test\n",
    "):\n",
    "    \"\"\"Get pyg Dataset.\"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    vals = df[target].values\n",
    "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
    "        val_list = [vals[i].item() for i in range(len(vals))]\n",
    "        vals = val_list\n",
    "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
    "    print(\"data range\", np.max(vals), np.min(vals))\n",
    "    print(output_dir)\n",
    "    print('graphs not saved')\n",
    "    graphs = load_pyg_graphs(\n",
    "        df,\n",
    "        name=name,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "    )\n",
    "    if mean_train == None:\n",
    "        mean_train = np.mean(vals)\n",
    "        std_train = np.std(vals)\n",
    "        data = graph.PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    else:\n",
    "        data = graph.PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    return data, mean_train, std_train\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    dataset: str = \"dft_3d\",\n",
    "    dataset_array=[],\n",
    "    target: str = \"formation_energy_peratom\",\n",
    "    atom_features: str = \"cgcnn\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    n_train=None,\n",
    "    n_val=None,\n",
    "    n_test=None,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    batch_size: int = 5,\n",
    "    standardize: bool = False,\n",
    "    line_graph: bool = True,\n",
    "    split_seed: int = 123,\n",
    "    workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "    save_dataloader: bool = False,\n",
    "    filename: str = \"sample\",\n",
    "    id_tag: str = \"jid\",\n",
    "    use_canonize: bool = False,\n",
    "    cutoff: float = 8.0,\n",
    "    max_neighbors: int = 12,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "    target_multiplication_factor: Optional[float] = None,\n",
    "    standard_scalar_and_pca=False,\n",
    "    keep_data_order=False,\n",
    "    output_features=1,\n",
    "    output_dir=None,\n",
    "    matrix_input=False,\n",
    "    pyg_input=False,\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    use_save=True,\n",
    "    mp_id_list=None,\n",
    "):\n",
    "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
    "    # data loading\n",
    "    mean_train=None\n",
    "    std_train=None\n",
    "    assert (matrix_input and pyg_input) == False\n",
    "    \n",
    "    train_sample = filename + \"_train.data\"\n",
    "    val_sample = filename + \"_val.data\"\n",
    "    test_sample = filename + \"_test.data\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if (\n",
    "        os.path.exists(train_sample)\n",
    "        and os.path.exists(val_sample)\n",
    "        and os.path.exists(test_sample)\n",
    "        and save_dataloader\n",
    "    ):\n",
    "        print(\"Loading from saved file...\")\n",
    "        print(\"Make sure all the DataLoader params are same.\")\n",
    "        print(\"This module is made for debugging only.\")\n",
    "        train_loader = torch.load(train_sample)\n",
    "        val_loader = torch.load(val_sample)\n",
    "        test_loader = torch.load(test_sample)\n",
    "        if train_loader.pin_memory != pin_memory:\n",
    "            train_loader.pin_memory = pin_memory\n",
    "        if test_loader.pin_memory != pin_memory:\n",
    "            test_loader.pin_memory = pin_memory\n",
    "        if val_loader.pin_memory != pin_memory:\n",
    "            val_loader.pin_memory = pin_memory\n",
    "        if train_loader.num_workers != workers:\n",
    "            train_loader.num_workers = workers\n",
    "        if test_loader.num_workers != workers:\n",
    "            test_loader.num_workers = workers\n",
    "        if val_loader.num_workers != workers:\n",
    "            val_loader.num_workers = workers\n",
    "        print(\"train\", len(train_loader.dataset))\n",
    "        print(\"val\", len(val_loader.dataset))\n",
    "        print(\"test\", len(test_loader.dataset))\n",
    "        return (\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            train_loader.dataset.prepare_batch,\n",
    "        )\n",
    "    else:\n",
    "        if not dataset_array:\n",
    "            d = jdata(dataset)\n",
    "        else:\n",
    "            d = dataset_array\n",
    "            # for ii, i in enumerate(pc_y):\n",
    "            #    d[ii][target] = pc_y[ii].tolist()\n",
    "\n",
    "        dat = []\n",
    "        if classification_threshold is not None:\n",
    "            print(\n",
    "                \"Using \",\n",
    "                classification_threshold,\n",
    "                \" for classifying \",\n",
    "                target,\n",
    "                \" data.\",\n",
    "            )\n",
    "            print(\"Converting target data into 1 and 0.\")\n",
    "        all_targets = []\n",
    "\n",
    "        # TODO:make an all key in qm9_dgl\n",
    "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
    "            print(\"Making all qm9_dgl\")\n",
    "            tmp = []\n",
    "            for ii in d:\n",
    "                ii[\"all\"] = [\n",
    "                    ii[\"mu\"],\n",
    "                    ii[\"alpha\"],\n",
    "                    ii[\"homo\"],\n",
    "                    ii[\"lumo\"],\n",
    "                    ii[\"gap\"],\n",
    "                    ii[\"r2\"],\n",
    "                    ii[\"zpve\"],\n",
    "                    ii[\"U0\"],\n",
    "                    ii[\"U\"],\n",
    "                    ii[\"H\"],\n",
    "                    ii[\"G\"],\n",
    "                    ii[\"Cv\"],\n",
    "                ]\n",
    "                tmp.append(ii)\n",
    "            print(\"Made all qm9_dgl\")\n",
    "            d = tmp\n",
    "        for i in d:\n",
    "            if isinstance(i[target], list):  # multioutput target\n",
    "                all_targets.append(torch.tensor(i[target]))\n",
    "                dat.append(i)\n",
    "\n",
    "            elif (\n",
    "                i[target] is not None\n",
    "                and i[target] != \"na\"\n",
    "                and not math.isnan(i[target])\n",
    "            ):\n",
    "                if target_multiplication_factor is not None:\n",
    "                    i[target] = i[target] * target_multiplication_factor\n",
    "                if classification_threshold is not None:\n",
    "                    if i[target] <= classification_threshold:\n",
    "                        i[target] = 0\n",
    "                    elif i[target] > classification_threshold:\n",
    "                        i[target] = 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Check classification data type.\",\n",
    "                            i[target],\n",
    "                            type(i[target]),\n",
    "                        )\n",
    "                dat.append(i)\n",
    "                all_targets.append(i[target])\n",
    "    \n",
    "    if mp_id_list is not None:\n",
    "        if mp_id_list == 'bulk':\n",
    "            print('using mp bulk dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "        \n",
    "        if mp_id_list == 'shear':\n",
    "            print('using mp shear dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "\n",
    "    else:\n",
    "        id_train, id_val, id_test = get_id_train_val_test(\n",
    "            total_size=len(dat),\n",
    "            split_seed=split_seed,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            n_train=n_train,\n",
    "            n_test=n_test,\n",
    "            n_val=n_val,\n",
    "            keep_data_order=keep_data_order,\n",
    "        )\n",
    "        ids_train_val_test = {}\n",
    "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
    "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
    "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
    "        dumpjson(\n",
    "            data=ids_train_val_test,\n",
    "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
    "        )\n",
    "        dataset_train = [dat[x] for x in id_train]\n",
    "        dataset_val = [dat[x] for x in id_val]\n",
    "        dataset_test = [dat[x] for x in id_test]\n",
    "    \n",
    "    train_data, mean_train, std_train = get_pyg_dataset(\n",
    "        dataset=dataset_train,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"train_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "    )\n",
    "    val_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_val,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"val_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "    test_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_test,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"test_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "\n",
    "    \n",
    "    collate_fn = train_data.collate\n",
    "    if line_graph:\n",
    "        collate_fn = train_data.collate_line_graph\n",
    "\n",
    "    # use a regular pytorch dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    if save_dataloader:\n",
    "        torch.save(train_loader, train_sample)\n",
    "        torch.save(val_loader, val_sample)\n",
    "        torch.save(test_loader, test_sample)\n",
    "    \n",
    "    print(\"n_train:\", len(train_loader.dataset))\n",
    "    print(\"n_val:\", len(val_loader.dataset))\n",
    "    print(\"n_test:\", len(test_loader.dataset))\n",
    "    return (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_loader.dataset.prepare_batch,\n",
    "        mean_train,\n",
    "        std_train,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
