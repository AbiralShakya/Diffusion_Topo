{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining 3D dataset 76k ...\n",
      "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
      "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
      "Loading the zipfile...\n",
      "Loading completed.\n",
      "\n",
      "--- Fetching data using jarvisdft_optimade (Public OPTIMADE API) ---\n",
      "Processing JID: JVASP-1\n",
      "Querying URL: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id=\"JVASP-1\"\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=2\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=3\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=4\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=5\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=6\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=7\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=8\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=9\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=10\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=11\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=12\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=13\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=14\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=15\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=16\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=17\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=18\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=19\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=20\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=21\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=22\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=23\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=24\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=25\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=26\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=27\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=28\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=29\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=30\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=31\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=32\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=33\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=34\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=35\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=36\n",
      "  Fetching next page: https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=immutable_id%3D%22JVASP-1%22&page=37\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jarvis.core.atoms import Atoms\n",
    "import graph\n",
    "#from ComFormerFork.graph import PygGraph, PygStructureDataset\n",
    "from jarvis.db.figshare import data as jdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from jarvis.db.jsonutils import dumpjson\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# use pandas progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\"))\n",
    "API_KEY = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "dft3d = jdata(\"dft_3d\")               \n",
    "df = pd.DataFrame(dft3d)     \n",
    "\n",
    "# print(df.columns)\n",
    "# print(type(dft3d[0]))\n",
    "# try:\n",
    "#     print(dft3d[0].keys())          # if it’s a dict-like\n",
    "# except:\n",
    "#     print(vars(dft3d[0]))           # if it’s an object with attrs\n",
    "\n",
    "# from jarvis.db.restapi import Api\n",
    "\n",
    "# mp_map = []\n",
    "# for jid in df[\"jid\"]:\n",
    "#     entry = Api.get_data_by_template_id(\"dft_3d\", jid)\n",
    "#     # entry.db_ids is guaranteed to be a dict if it exists\n",
    "#     mp_map.append({\n",
    "#         \"jid\": jid,\n",
    "#         \"mpid\": entry.db_ids.get(\"materialsproject\")  # maybe None\n",
    "#     })\n",
    "# mp_df = pd.DataFrame(mp_map)\n",
    "\n",
    "# df = df.merge(mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "# 3) (Optional) drop any rows where mpid is missing\n",
    "#df = df.dropna(subset=[\"mpid\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df[\"mpid\"] = df[\"db_ids\"].apply(lambda x: x.get(\"materialsproject\") if isinstance(x, dict) else None)\n",
    "# df = df.dropna(subset=[\"mpid\"])        # keep only entries with an MP match\n",
    "\n",
    "\n",
    "# with MPRester(api_key = API_KEY) as m:\n",
    "#     docs = m.materials.summary.search(\n",
    "#         jid__in=df[\"jid\"].tolist(),\n",
    "#         fields=[\"jid\",\"topological_type\"]\n",
    "#     )\n",
    "\n",
    "#optimade\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# --- Start: MODIFIED Definition of jarvisdft_optimade ---\n",
    "def jarvisdft_optimade(\n",
    "    prefix=\"https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=\",\n",
    "    query=\"elements HAS ALL C,Si\",\n",
    "):\n",
    "    url = prefix + query\n",
    "    vals = []\n",
    "    print(f\"Querying URL: {url}\")\n",
    "    current_url_for_error = url # Store initial URL for error reporting\n",
    "\n",
    "    while True:\n",
    "        if url is not None:\n",
    "            current_url_for_error = url # Update for current page\n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                response.raise_for_status() # Check for HTTP errors (4xx or 5xx)\n",
    "\n",
    "                json_response = None # Initialize\n",
    "                if not response.content: # Check if response body is empty\n",
    "                    print(f\"  Warning: Empty response from {url}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        json_response = response.json()\n",
    "                    except requests.exceptions.JSONDecodeError as je:\n",
    "                        print(f\"  Error decoding JSON from {url}: {je}\")\n",
    "                        print(f\"  Response text: {response.text[:500]}...\")\n",
    "                \n",
    "                if json_response is None:\n",
    "                    print(f\"  Could not parse JSON or response was empty/problematic for {url}. Stopping pagination for this query.\")\n",
    "                    break # Break from the while True loop\n",
    "\n",
    "                data = json_response.get(\"data\", []) # Safely get data, defaults to []\n",
    "                for i in data:\n",
    "                    vals.append(i)\n",
    "                \n",
    "                # Safely get the 'next' link for pagination\n",
    "                links_object = json_response.get(\"links\", {})\n",
    "                if isinstance(links_object, dict):\n",
    "                    url = links_object.get(\"next\")\n",
    "                else: # links object is not a dictionary or is missing\n",
    "                    url = None \n",
    "                \n",
    "                if url:\n",
    "                    print(f\"  Fetching next page: {url}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Request error for {current_url_for_error}: {e}\")\n",
    "                break # Stop if there's a request error\n",
    "            except Exception as e_inner: # Catch any other unexpected error within the try\n",
    "                print(f\"  Unexpected error processing {current_url_for_error}: {e_inner}\")\n",
    "                break\n",
    "        else: # url is None, so no more pages\n",
    "            break\n",
    "        \n",
    "        # Be polite to the API, even if a page was problematic but we continue\n",
    "        time.sleep(0.2) \n",
    "    return vals\n",
    "# --- End: Definition of jarvisdft_optimade ---\n",
    "\n",
    "# Simulate your initial DataFrame with 'jid' column\n",
    "data = {'jid': ['JVASP-1', 'JVASP-2', 'JVASP-1002', 'JVASP-99999']} # Example JIDs\n",
    "# For testing with a JID that might exist and have an MPID cross-reference:\n",
    "# data = {'jid': ['JVASP-1063']} # Example: JVASP-1063 (Si) is mp-149\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "mp_map_optimade = []\n",
    "\n",
    "print(\"\\n--- Fetching data using jarvisdft_optimade (Public OPTIMADE API) ---\")\n",
    "\n",
    "for jid in df[\"jid\"]:\n",
    "    print(f\"Processing JID: {jid}\")\n",
    "    mpid = None\n",
    "    \n",
    "    # OPTIMADE filter:\n",
    "    # immutable_id is standard. If it fails, 'id' is another common field.\n",
    "    # The values in your nature paper (e.g. JVASP-X) are JARVIS IDs.\n",
    "    # How these map to OPTIMADE filterable fields can vary.\n",
    "    # Common attempts:\n",
    "    optimade_filter_query = f'immutable_id=\"{jid}\"'\n",
    "    # optimade_filter_query = f'id=\"{jid}\"'\n",
    "    # optimade_filter_query = f'_jarvis_id=\"{jid}\"' # (less standard, but possible)\n",
    "\n",
    "\n",
    "    entries = [] # Ensure entries is initialized outside try\n",
    "    try:\n",
    "        entries = jarvisdft_optimade(query=optimade_filter_query)\n",
    "\n",
    "        if entries:\n",
    "            if len(entries) > 1:\n",
    "                print(f\"  Warning: Found {len(entries)} entries for JID {jid} with filter '{optimade_filter_query}'. Using the first one.\")\n",
    "            \n",
    "            found_entry = entries[0] \n",
    "            attributes = found_entry.get(\"attributes\", {})\n",
    "\n",
    "            external_dbs = attributes.get(\"external_databases\")\n",
    "            if external_dbs and isinstance(external_dbs, list):\n",
    "                for db_ref in external_dbs:\n",
    "                    if isinstance(db_ref, dict) and \\\n",
    "                       db_ref.get(\"name\", \"\").lower().strip() == \"materials project\":\n",
    "                        mpid = db_ref.get(\"id\")\n",
    "                        if mpid:\n",
    "                            break \n",
    "            \n",
    "            if not mpid: # Fallback if not in external_databases\n",
    "                 # Check other common places or provider-specific fields if known\n",
    "                 pass # e.g. mpid = attributes.get(\"materialsprojectid\")\n",
    "\n",
    "            if mpid:\n",
    "                print(f\"  Found MPID: {mpid} for JID: {jid}\")\n",
    "            else:\n",
    "                print(f\"  MPID not found in the OPTIMADE entry for JID: {jid}.\")\n",
    "                # print(f\"  DEBUG: Attributes for {jid}: {attributes}\") # Uncomment to inspect\n",
    "        else:\n",
    "            print(f\"  No OPTIMADE entry found for JID: {jid} with filter '{optimade_filter_query}'.\")\n",
    "\n",
    "    except Exception as e: # Catch errors from the processing block itself\n",
    "        print(f\"  Error processing JID {jid} after OPTIMADE call: {e}\")\n",
    "        # This helps differentiate from errors within jarvisdft_optimade\n",
    "\n",
    "    mp_map_optimade.append({\n",
    "        \"jid\": jid,\n",
    "        \"mpid_optimade\": mpid # Note the column name here\n",
    "    })\n",
    "    time.sleep(0.5) \n",
    "\n",
    "optimade_mp_df = pd.DataFrame(mp_map_optimade)\n",
    "# Merge with the original df. df_merged_optimade will have 'jid' and 'mpid_optimade'\n",
    "df_merged_optimade = df.merge(optimade_mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "print(\"\\n--- Merged DataFrame (from OPTIMADE) ---\")\n",
    "print(df_merged_optimade)\n",
    "\n",
    "\n",
    "\n",
    "with MPRester(API_KEY) as m:\n",
    "    docs = m.materials.summary.search(\n",
    "        material_ids=df[\"mpid\"].tolist(),\n",
    "        fields=[\"material_id\",\"topological_type\"]\n",
    "    )\n",
    "label_df = pd.DataFrame([{\n",
    "    \"mpid\": d.material_id,\n",
    "    \"topo_type\": d.topological_type\n",
    "} for d in docs])\n",
    "# map to binary\n",
    "good = {\"Z2_INSULATOR\",\"CRYSTALLINE_INSULATOR\"}\n",
    "label_df[\"is_topo\"] = label_df[\"topo_type\"].isin(good).astype(int)\n",
    "\n",
    "df = df.merge(label_df[[\"mpid\",\"is_topo\"]], on=\"mpid\", how=\"left\")\n",
    "df[\"is_topo\"] = df[\"is_topo\"].fillna(0).astype(int)\n",
    "\n",
    "def load_dataset(\n",
    "    name: str = \"dft_3d\",\n",
    "    target=None,\n",
    "    limit: Optional[int] = None,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"Load jarvis data.\"\"\"\n",
    "    d = jdata(name)\n",
    "    data = []\n",
    "    for i in d:\n",
    "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
    "            if classification_threshold is not None:\n",
    "                if i[target] <= classification_threshold:\n",
    "                    i[target] = 0\n",
    "                elif i[target] > classification_threshold:\n",
    "                    i[target] = 1\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Check classification data type.\",\n",
    "                        i[target],\n",
    "                        type(i[target]),\n",
    "                    )\n",
    "            data.append(i)\n",
    "    d = data\n",
    "    if limit is not None:\n",
    "        d = d[:limit]\n",
    "    d = pd.DataFrame(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def mean_absolute_deviation(data, axis=None):\n",
    "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def load_pyg_graphs(\n",
    "    df: pd.DataFrame,\n",
    "    name: str = \"dft_3d\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    cutoff: float = 8,\n",
    "    max_neighbors: int = 12,\n",
    "    cachedir: Optional[Path] = None,\n",
    "    use_canonize: bool = False,\n",
    "    use_lattice: bool = False,\n",
    "    use_angle: bool = False,\n",
    "):\n",
    "    \"\"\"Construct crystal graphs.\n",
    "\n",
    "    Load only atomic number node features\n",
    "    and bond displacement vector edge features.\n",
    "\n",
    "    Resulting graphs have scheme e.g.\n",
    "    ```\n",
    "    Graph(num_nodes=12, num_edges=156,\n",
    "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
    "          edata_schemes={'r': Scheme(shape=(3,)})\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def atoms_to_graph(atoms):\n",
    "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
    "        structure = Atoms.from_dict(atoms)\n",
    "        return PygGraph.atom_dgl_multigraph(\n",
    "            structure,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            cutoff=cutoff,\n",
    "            atom_features=\"atomic_number\",\n",
    "            max_neighbors=max_neighbors,\n",
    "            compute_line_graph=False,\n",
    "            use_canonize=use_canonize,\n",
    "            use_lattice=use_lattice,\n",
    "            use_angle=use_angle,\n",
    "        )\n",
    "    \n",
    "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
    "    # graphs = df[\"atoms\"].apply(atoms_to_graph).values\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def get_id_train_val_test(\n",
    "    total_size=1000,\n",
    "    split_seed=123,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    n_train=None,\n",
    "    n_test=None,\n",
    "    n_val=None,\n",
    "    keep_data_order=False,\n",
    "):\n",
    "    \"\"\"Get train, val, test IDs.\"\"\"\n",
    "    if (\n",
    "        train_ratio is None\n",
    "        and val_ratio is not None\n",
    "        and test_ratio is not None\n",
    "    ):\n",
    "        if train_ratio is None:\n",
    "            assert val_ratio + test_ratio < 1\n",
    "            train_ratio = 1 - val_ratio - test_ratio\n",
    "            print(\"Using rest of the dataset except the test and val sets.\")\n",
    "        else:\n",
    "            assert train_ratio + val_ratio + test_ratio <= 1\n",
    "    # indices = list(range(total_size))\n",
    "    if n_train is None:\n",
    "        n_train = int(train_ratio * total_size)\n",
    "    if n_test is None:\n",
    "        n_test = int(test_ratio * total_size)\n",
    "    if n_val is None:\n",
    "        n_val = int(val_ratio * total_size)\n",
    "    ids = list(np.arange(total_size))\n",
    "    if not keep_data_order:\n",
    "        random.seed(split_seed)\n",
    "        random.shuffle(ids)\n",
    "    if n_train + n_val + n_test > total_size:\n",
    "        raise ValueError(\n",
    "            \"Check total number of samples.\",\n",
    "            n_train + n_val + n_test,\n",
    "            \">\",\n",
    "            total_size,\n",
    "        )\n",
    "\n",
    "    id_train = ids[:n_train]\n",
    "    id_val = ids[-(n_val + n_test) : -n_test]\n",
    "    id_test = ids[-n_test:]\n",
    "    return id_train, id_val, id_test\n",
    "\n",
    "\n",
    "def get_pyg_dataset(\n",
    "    dataset=[],\n",
    "    id_tag=\"jid\",\n",
    "    target=\"\",\n",
    "    neighbor_strategy=\"\",\n",
    "    atom_features=\"\",\n",
    "    use_canonize=\"\",\n",
    "    name=\"\",\n",
    "    line_graph=\"\",\n",
    "    cutoff=8.0,\n",
    "    max_neighbors=12,\n",
    "    classification=False,\n",
    "    output_dir=\".\",\n",
    "    tmp_name=\"dataset\",\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    data_from='Jarvis',\n",
    "    use_save=False,\n",
    "    mean_train=None,\n",
    "    std_train=None,\n",
    "    now=False, # for test\n",
    "):\n",
    "    \"\"\"Get pyg Dataset.\"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    vals = df[target].values\n",
    "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
    "        val_list = [vals[i].item() for i in range(len(vals))]\n",
    "        vals = val_list\n",
    "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
    "    print(\"data range\", np.max(vals), np.min(vals))\n",
    "    print(output_dir)\n",
    "    print('graphs not saved')\n",
    "    graphs = load_pyg_graphs(\n",
    "        df,\n",
    "        name=name,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "    )\n",
    "    if mean_train == None:\n",
    "        mean_train = np.mean(vals)\n",
    "        std_train = np.std(vals)\n",
    "        data = PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    else:\n",
    "        data = PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    return data, mean_train, std_train\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    dataset: str = \"dft_3d\",\n",
    "    dataset_array=[],\n",
    "    target: str = \"formation_energy_peratom\",\n",
    "    atom_features: str = \"cgcnn\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    n_train=None,\n",
    "    n_val=None,\n",
    "    n_test=None,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    batch_size: int = 5,\n",
    "    standardize: bool = False,\n",
    "    line_graph: bool = True,\n",
    "    split_seed: int = 123,\n",
    "    workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "    save_dataloader: bool = False,\n",
    "    filename: str = \"sample\",\n",
    "    id_tag: str = \"jid\",\n",
    "    use_canonize: bool = False,\n",
    "    cutoff: float = 8.0,\n",
    "    max_neighbors: int = 12,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "    target_multiplication_factor: Optional[float] = None,\n",
    "    standard_scalar_and_pca=False,\n",
    "    keep_data_order=False,\n",
    "    output_features=1,\n",
    "    output_dir=None,\n",
    "    matrix_input=False,\n",
    "    pyg_input=False,\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    use_save=True,\n",
    "    mp_id_list=None,\n",
    "):\n",
    "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
    "    # data loading\n",
    "    mean_train=None\n",
    "    std_train=None\n",
    "    assert (matrix_input and pyg_input) == False\n",
    "    \n",
    "    train_sample = filename + \"_train.data\"\n",
    "    val_sample = filename + \"_val.data\"\n",
    "    test_sample = filename + \"_test.data\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if (\n",
    "        os.path.exists(train_sample)\n",
    "        and os.path.exists(val_sample)\n",
    "        and os.path.exists(test_sample)\n",
    "        and save_dataloader\n",
    "    ):\n",
    "        print(\"Loading from saved file...\")\n",
    "        print(\"Make sure all the DataLoader params are same.\")\n",
    "        print(\"This module is made for debugging only.\")\n",
    "        train_loader = torch.load(train_sample)\n",
    "        val_loader = torch.load(val_sample)\n",
    "        test_loader = torch.load(test_sample)\n",
    "        if train_loader.pin_memory != pin_memory:\n",
    "            train_loader.pin_memory = pin_memory\n",
    "        if test_loader.pin_memory != pin_memory:\n",
    "            test_loader.pin_memory = pin_memory\n",
    "        if val_loader.pin_memory != pin_memory:\n",
    "            val_loader.pin_memory = pin_memory\n",
    "        if train_loader.num_workers != workers:\n",
    "            train_loader.num_workers = workers\n",
    "        if test_loader.num_workers != workers:\n",
    "            test_loader.num_workers = workers\n",
    "        if val_loader.num_workers != workers:\n",
    "            val_loader.num_workers = workers\n",
    "        print(\"train\", len(train_loader.dataset))\n",
    "        print(\"val\", len(val_loader.dataset))\n",
    "        print(\"test\", len(test_loader.dataset))\n",
    "        return (\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            train_loader.dataset.prepare_batch,\n",
    "        )\n",
    "    else:\n",
    "        if not dataset_array:\n",
    "            d = jdata(dataset)\n",
    "        else:\n",
    "            d = dataset_array\n",
    "            # for ii, i in enumerate(pc_y):\n",
    "            #    d[ii][target] = pc_y[ii].tolist()\n",
    "\n",
    "        dat = []\n",
    "        if classification_threshold is not None:\n",
    "            print(\n",
    "                \"Using \",\n",
    "                classification_threshold,\n",
    "                \" for classifying \",\n",
    "                target,\n",
    "                \" data.\",\n",
    "            )\n",
    "            print(\"Converting target data into 1 and 0.\")\n",
    "        all_targets = []\n",
    "\n",
    "        # TODO:make an all key in qm9_dgl\n",
    "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
    "            print(\"Making all qm9_dgl\")\n",
    "            tmp = []\n",
    "            for ii in d:\n",
    "                ii[\"all\"] = [\n",
    "                    ii[\"mu\"],\n",
    "                    ii[\"alpha\"],\n",
    "                    ii[\"homo\"],\n",
    "                    ii[\"lumo\"],\n",
    "                    ii[\"gap\"],\n",
    "                    ii[\"r2\"],\n",
    "                    ii[\"zpve\"],\n",
    "                    ii[\"U0\"],\n",
    "                    ii[\"U\"],\n",
    "                    ii[\"H\"],\n",
    "                    ii[\"G\"],\n",
    "                    ii[\"Cv\"],\n",
    "                ]\n",
    "                tmp.append(ii)\n",
    "            print(\"Made all qm9_dgl\")\n",
    "            d = tmp\n",
    "        for i in d:\n",
    "            if isinstance(i[target], list):  # multioutput target\n",
    "                all_targets.append(torch.tensor(i[target]))\n",
    "                dat.append(i)\n",
    "\n",
    "            elif (\n",
    "                i[target] is not None\n",
    "                and i[target] != \"na\"\n",
    "                and not math.isnan(i[target])\n",
    "            ):\n",
    "                if target_multiplication_factor is not None:\n",
    "                    i[target] = i[target] * target_multiplication_factor\n",
    "                if classification_threshold is not None:\n",
    "                    if i[target] <= classification_threshold:\n",
    "                        i[target] = 0\n",
    "                    elif i[target] > classification_threshold:\n",
    "                        i[target] = 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Check classification data type.\",\n",
    "                            i[target],\n",
    "                            type(i[target]),\n",
    "                        )\n",
    "                dat.append(i)\n",
    "                all_targets.append(i[target])\n",
    "    \n",
    "    if mp_id_list is not None:\n",
    "        if mp_id_list == 'bulk':\n",
    "            print('using mp bulk dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "        \n",
    "        if mp_id_list == 'shear':\n",
    "            print('using mp shear dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "\n",
    "    else:\n",
    "        id_train, id_val, id_test = get_id_train_val_test(\n",
    "            total_size=len(dat),\n",
    "            split_seed=split_seed,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            n_train=n_train,\n",
    "            n_test=n_test,\n",
    "            n_val=n_val,\n",
    "            keep_data_order=keep_data_order,\n",
    "        )\n",
    "        ids_train_val_test = {}\n",
    "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
    "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
    "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
    "        dumpjson(\n",
    "            data=ids_train_val_test,\n",
    "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
    "        )\n",
    "        dataset_train = [dat[x] for x in id_train]\n",
    "        dataset_val = [dat[x] for x in id_val]\n",
    "        dataset_test = [dat[x] for x in id_test]\n",
    "    \n",
    "    train_data, mean_train, std_train = get_pyg_dataset(\n",
    "        dataset=dataset_train,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"train_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "    )\n",
    "    val_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_val,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"val_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "    test_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_test,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"test_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "\n",
    "    \n",
    "    collate_fn = train_data.collate\n",
    "    if line_graph:\n",
    "        collate_fn = train_data.collate_line_graph\n",
    "\n",
    "    # use a regular pytorch dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    if save_dataloader:\n",
    "        torch.save(train_loader, train_sample)\n",
    "        torch.save(val_loader, val_sample)\n",
    "        torch.save(test_loader, test_sample)\n",
    "    \n",
    "    print(\"n_train:\", len(train_loader.dataset))\n",
    "    print(\"n_val:\", len(val_loader.dataset))\n",
    "    print(\"n_test:\", len(test_loader.dataset))\n",
    "    return (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_loader.dataset.prepare_batch,\n",
    "        mean_train,\n",
    "        std_train,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
