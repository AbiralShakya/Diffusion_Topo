{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining 3D dataset 76k ...\n",
      "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
      "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
      "Loading the zipfile...\n",
      "Loading completed.\n",
      "No valid Material IDs found to query for topological information.\n",
      "\n",
      "--- Merged DataFrame (from OPTIMADE) ---\n",
      "           jid mpid_optimade  is_topo topo_type\n",
      "0      JVASP-1          None        0      None\n",
      "1      JVASP-2          None        0      None\n",
      "2   JVASP-1002          None        0      None\n",
      "3  JVASP-99999          None        0      None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jarvis.core.atoms import Atoms\n",
    "import graph\n",
    "#from ComFormerFork.graph import PygGraph, PygStructureDataset\n",
    "from jarvis.db.figshare import data as jdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from jarvis.db.jsonutils import dumpjson\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# use pandas progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\"))\n",
    "API_KEY = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "dft3d = jdata(\"dft_3d\")               \n",
    "df = pd.DataFrame(dft3d)     \n",
    "\n",
    "# print(df.columns)\n",
    "# print(type(dft3d[0]))\n",
    "# try:\n",
    "#     print(dft3d[0].keys())          # if it’s a dict-like\n",
    "# except:\n",
    "#     print(vars(dft3d[0]))           # if it’s an object with attrs\n",
    "\n",
    "# from jarvis.db.restapi import Api\n",
    "\n",
    "# mp_map = []\n",
    "# for jid in df[\"jid\"]:\n",
    "#     entry = Api.get_data_by_template_id(\"dft_3d\", jid)\n",
    "#     # entry.db_ids is guaranteed to be a dict if it exists\n",
    "#     mp_map.append({\n",
    "#         \"jid\": jid,\n",
    "#         \"mpid\": entry.db_ids.get(\"materialsproject\")  # maybe None\n",
    "#     })\n",
    "# mp_df = pd.DataFrame(mp_map)\n",
    "\n",
    "# df = df.merge(mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "# 3) (Optional) drop any rows where mpid is missing\n",
    "#df = df.dropna(subset=[\"mpid\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df[\"mpid\"] = df[\"db_ids\"].apply(lambda x: x.get(\"materialsproject\") if isinstance(x, dict) else None)\n",
    "# df = df.dropna(subset=[\"mpid\"])        # keep only entries with an MP match\n",
    "\n",
    "\n",
    "# with MPRester(api_key = API_KEY) as m:\n",
    "#     docs = m.materials.summary.search(\n",
    "#         jid__in=df[\"jid\"].tolist(),\n",
    "#         fields=[\"jid\",\"topological_type\"]\n",
    "#     )\n",
    "\n",
    "#optimade\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# --- Start: MODIFIED Definition of jarvisdft_optimade ---\n",
    "def jarvisdft_optimade(\n",
    "    prefix=\"https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/?filter=\",\n",
    "    query=\"elements HAS ALL C,Si\",\n",
    "):\n",
    "    url = prefix + query\n",
    "    vals = []\n",
    "    print(f\"Querying URL: {url}\")\n",
    "    current_url_for_error = url # Store initial URL for error reporting\n",
    "\n",
    "    while True:\n",
    "        if url is not None:\n",
    "            current_url_for_error = url # Update for current page\n",
    "            try:\n",
    "                response = requests.get(url, timeout=30)\n",
    "                response.raise_for_status() # Check for HTTP errors (4xx or 5xx)\n",
    "\n",
    "                json_response = None # Initialize\n",
    "                if not response.content: # Check if response body is empty\n",
    "                    print(f\"  Warning: Empty response from {url}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        json_response = response.json()\n",
    "                    except requests.exceptions.JSONDecodeError as je:\n",
    "                        print(f\"  Error decoding JSON from {url}: {je}\")\n",
    "                        print(f\"  Response text: {response.text[:500]}...\")\n",
    "                \n",
    "                if json_response is None:\n",
    "                    print(f\"  Could not parse JSON or response was empty/problematic for {url}. Stopping pagination for this query.\")\n",
    "                    break # Break from the while True loop\n",
    "\n",
    "                data = json_response.get(\"data\", []) # Safely get data, defaults to []\n",
    "                for i in data:\n",
    "                    vals.append(i)\n",
    "                \n",
    "                # Safely get the 'next' link for pagination\n",
    "                links_object = json_response.get(\"links\", {})\n",
    "                if isinstance(links_object, dict):\n",
    "                    url = links_object.get(\"next\")\n",
    "                else: # links object is not a dictionary or is missing\n",
    "                    url = None \n",
    "                \n",
    "                if url:\n",
    "                    print(f\"  Fetching next page: {url}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  Request error for {current_url_for_error}: {e}\")\n",
    "                break # Stop if there's a request error\n",
    "            except Exception as e_inner: # Catch any other unexpected error within the try\n",
    "                print(f\"  Unexpected error processing {current_url_for_error}: {e_inner}\")\n",
    "                break\n",
    "        else: # url is None, so no more pages\n",
    "            break\n",
    "        \n",
    "        # Be polite to the API, even if a page was problematic but we continue\n",
    "        time.sleep(0.2) \n",
    "    return vals\n",
    "# --- End: Definition of jarvisdft_optimade ---\n",
    "\n",
    "# Simulate your initial DataFrame with 'jid' column\n",
    "data = {'jid': ['JVASP-1', 'JVASP-2', 'JVASP-1002', 'JVASP-99999']} # Example JIDs\n",
    "# For testing with a JID that might exist and have an MPID cross-reference:\n",
    "# data = {'jid': ['JVASP-1063']} # Example: JVASP-1063 (Si) is mp-149\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- batch all JIDs into one OPTIMADE filter, request up to 100 results at once ---\n",
    "jids = df[\"jid\"].tolist()\n",
    "or_filters = \" OR \".join(f'(immutable_id=\"{jid}\")' for jid in jids)\n",
    "base_url = \"https://jarvis.nist.gov/optimade/jarvisdft/v1/structures/\"\n",
    "url = f\"{base_url}?page_limit=100&filter={or_filters}\"\n",
    "\n",
    "resp = requests.get(url, timeout=30)\n",
    "resp.raise_for_status()\n",
    "entries = resp.json().get(\"data\", [])\n",
    "\n",
    "# build a dict: JARVIS ID → MPID\n",
    "mp_map = {}\n",
    "for e in entries:\n",
    "    attrs = e.get(\"attributes\", {})\n",
    "    for db_ref in attrs.get(\"external_databases\", []):\n",
    "        if db_ref.get(\"name\", \"\").lower() == \"materials project\":\n",
    "            mp_map[e.get(\"id\")] = db_ref.get(\"id\")\n",
    "            break\n",
    "\n",
    "# turn it into a DataFrame and merge back into your df\n",
    "optimade_mp_df = pd.DataFrame([\n",
    "    {\"jid\": jid, \"mpid_optimade\": mp_map.get(jid)}\n",
    "    for jid in jids\n",
    "])\n",
    "df_merged_optimade = df.merge(optimade_mp_df, on=\"jid\", how=\"left\")\n",
    "#print(df_merged_optimade)\n",
    "\n",
    "import pandas as pd\n",
    "from mp_api.client import MPRester\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# --- (Assuming previous parts of your script are here, including API_KEY loading and df_merged_optimade creation) ---\n",
    "\n",
    "# Load environment variables if not already done\n",
    "# load_dotenv() # General .env\n",
    "# load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\")) # Specific .env\n",
    "# API_KEY = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "# df_merged_optimade should be a DataFrame with a column like 'mpid_optimade' containing Material IDs\n",
    "# Example:\n",
    "# data_example = {'jid': ['JVASP-1063'], 'mpid_optimade': ['mp-149']} # JVASP-1063 (Si) is mp-149\n",
    "# df_merged_optimade = pd.DataFrame(data_example)\n",
    "\n",
    "mp_ids_raw = df_merged_optimade[\"mpid_optimade\"].dropna().tolist()\n",
    "# Ensure all IDs are strings and filter out any potential non-string/None types that dropna might miss if column isn't strictly strings\n",
    "mp_ids = [str(mid) for mid in mp_ids_raw if mid is not None and str(mid).startswith(\"mp-\")]\n",
    "\n",
    "label_df = pd.DataFrame(columns=[\"mpid\", \"topo_type\", \"is_topo\"]) # Initialize an empty DataFrame\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"Error: MP_API_KEY is not set. Please check your .env configuration.\")\n",
    "elif not mp_ids:\n",
    "    print(\"No valid Material IDs found to query for topological information.\")\n",
    "else:\n",
    "    try:\n",
    "        with MPRester(API_KEY) as m:\n",
    "            # Use the TopologyRester to get topological information\n",
    "            # Fields in TopologyDoc include 'type_of_topology_no_vdw' and 'type_of_topology_vdw'\n",
    "            # Choose the one relevant to your study, or fetch both.\n",
    "            # For this example, we'll use 'type_of_topology_no_vdw'.\n",
    "            docs = m.topology.search(\n",
    "                material_ids=mp_ids,\n",
    "                fields=[\"material_id\", \"type_of_topology_no_vdw\"] # Correct fields for topology\n",
    "            )\n",
    "\n",
    "        # Process the results\n",
    "        if docs:\n",
    "            processed_docs = []\n",
    "            for d in docs:\n",
    "                # Ensure the document (d) and its attributes exist before accessing\n",
    "                mpid = getattr(d, 'material_id', None)\n",
    "                topo_type_value = getattr(d, 'type_of_topology_no_vdw', None) # Or type_of_topology_vdw\n",
    "\n",
    "                if mpid is not None: # Only proceed if material_id is present\n",
    "                    processed_docs.append({\n",
    "                        \"mpid\": str(mpid), # Ensure mpid is a string for consistent merging\n",
    "                        \"topo_type\": topo_type_value\n",
    "                    })\n",
    "            \n",
    "            if processed_docs:\n",
    "                 label_df = pd.DataFrame(processed_docs)\n",
    "\n",
    "                 # Define \"good\" topological types\n",
    "                 good = {\"Z2_INSULATOR\", \"CRYSTALLINE_INSULATOR\"} # These values should match potential outputs of type_of_topology_no_vdw\n",
    "                 label_df[\"is_topo\"] = label_df[\"topo_type\"].isin(good).astype(int)\n",
    "            else:\n",
    "                print(\"No topological data returned for the given material IDs.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No documents returned from topology search.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during MPRester query or processing: {e}\")\n",
    "        # label_df remains empty or as previously defined if error occurs\n",
    "\n",
    "# --- Merging back with your main DataFrame ---\n",
    "# Ensure the column names for merging are correct.\n",
    "# df_merged_optimade has 'mpid_optimade'. label_df has 'mpid'.\n",
    "if not label_df.empty:\n",
    "    df_merged_optimade = df_merged_optimade.merge(\n",
    "        label_df[[\"mpid\", \"is_topo\", \"topo_type\"]], # Include \"topo_type\" if you need it later\n",
    "        left_on=\"mpid_optimade\",\n",
    "        right_on=\"mpid\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # Fill NaN values for 'is_topo' that might result from materials not found or not having topology data\n",
    "    df_merged_optimade[\"is_topo\"] = df_merged_optimade[\"is_topo\"].fillna(0).astype(int)\n",
    "    if \"topo_type_y\" in df_merged_optimade.columns : # if topo_type already existed. _y is from merge\n",
    "        df_merged_optimade.rename(columns={\"topo_type_y\": \"topo_type\"}, inplace=True)\n",
    "    elif \"topo_type_x\" in df_merged_optimade.columns and \"topo_type\" in df_merged_optimade.columns: # if topo_type was original, and new is topo_type\n",
    "        # decide which one to keep or rename appropriately\n",
    "        pass\n",
    "    elif \"topo_type\" not in df_merged_optimade.columns and \"topo_type_y\" not in df_merged_optimade.columns and \"topo_type_x\" not in df_merged_optimade.columns:\n",
    "         # If 'topo_type' wasn't in the original df_merged_optimade and label_df was empty or didn't merge\n",
    "         # we might need to add it if it's expected.\n",
    "         if \"mpid\" in label_df.columns: # only if label_df had mpid to begin with\n",
    "            df_merged_optimade[\"topo_type\"] = df_merged_optimade[\"mpid\"].map(label_df.set_index(\"mpid\")[\"topo_type\"])\n",
    "\n",
    "\n",
    "else: # If label_df is empty (e.g., no mp_ids or API error)\n",
    "    df_merged_optimade[\"is_topo\"] = 0\n",
    "    df_merged_optimade[\"topo_type\"] = None # Or np.nan\n",
    "\n",
    "# print(\"\\n--- DataFrame after merging topological data ---\")\n",
    "# print(df_merged_optimade.head())\n",
    "\n",
    "# The rest of your script (load_dataset, etc.) would follow\n",
    "\n",
    "#optimade_mp_df = pd.DataFrame(mp_map_optimade)\n",
    "# Merge with the original df. df_merged_optimade will have 'jid' and 'mpid_optimade'\n",
    "#df_merged_optimade = df.merge(optimade_mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "print(\"\\n--- Merged DataFrame (from OPTIMADE) ---\")\n",
    "print(df_merged_optimade)\n",
    "\n",
    "\n",
    "# mp_ids = df_merged_optimade[\"mpid_optimade\"].dropna().tolist()\n",
    "\n",
    "# with MPRester(API_KEY) as m:\n",
    "#     docs = m.materials.summary._search(\n",
    "#         suburl=\"atlas/topology\",\n",
    "#         material_ids=\",\".join(mp_ids),\n",
    "#         fields=[\"material_id\", \"topological_type\"],\n",
    "#     )\n",
    "#     # docs = m.materials.summary.search(\n",
    "#     #     material_ids= mp_ids, \n",
    "#     #     fields = [\"material_id\", \"topological_type\"]\n",
    "#     # )\n",
    "#     # docs = m.materials.summary.search(\n",
    "#     #     material_ids=df[\"mpid\"].tolist(),\n",
    "#     #     fields=[\"material_id\",\"topological_type\"]\n",
    "#     # )\n",
    "\n",
    "\n",
    "# label_df = pd.DataFrame([{\n",
    "#     \"mpid\": d.material_id,\n",
    "#     \"topo_type\": d.topological_type\n",
    "# } for d in docs])\n",
    "\n",
    "# # map to binary\n",
    "# good = {\"Z2_INSULATOR\",\"CRYSTALLINE_INSULATOR\"}\n",
    "# label_df[\"is_topo\"] = label_df[\"topo_type\"].isin(good).astype(int)\n",
    "\n",
    "# df = df.merge(label_df[[\"mpid\",\"is_topo\"]], on=\"mpid\", how=\"left\")\n",
    "# df[\"is_topo\"] = df[\"is_topo\"].fillna(0).astype(int)\n",
    "\n",
    "def load_dataset(\n",
    "    name: str = \"dft_3d\",\n",
    "    target=None,\n",
    "    limit: Optional[int] = None,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"Load jarvis data.\"\"\"\n",
    "    d = jdata(name)\n",
    "    data = []\n",
    "    for i in d:\n",
    "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
    "            if classification_threshold is not None:\n",
    "                if i[target] <= classification_threshold:\n",
    "                    i[target] = 0\n",
    "                elif i[target] > classification_threshold:\n",
    "                    i[target] = 1\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Check classification data type.\",\n",
    "                        i[target],\n",
    "                        type(i[target]),\n",
    "                    )\n",
    "            data.append(i)\n",
    "    d = data\n",
    "    if limit is not None:\n",
    "        d = d[:limit]\n",
    "    d = pd.DataFrame(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def mean_absolute_deviation(data, axis=None):\n",
    "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def load_pyg_graphs(\n",
    "    df: pd.DataFrame,\n",
    "    name: str = \"dft_3d\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    cutoff: float = 8,\n",
    "    max_neighbors: int = 12,\n",
    "    cachedir: Optional[Path] = None,\n",
    "    use_canonize: bool = False,\n",
    "    use_lattice: bool = False,\n",
    "    use_angle: bool = False,\n",
    "):\n",
    "    \"\"\"Construct crystal graphs.\n",
    "\n",
    "    Load only atomic number node features\n",
    "    and bond displacement vector edge features.\n",
    "\n",
    "    Resulting graphs have scheme e.g.\n",
    "    ```\n",
    "    Graph(num_nodes=12, num_edges=156,\n",
    "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
    "          edata_schemes={'r': Scheme(shape=(3,)})\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def atoms_to_graph(atoms):\n",
    "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
    "        structure = Atoms.from_dict(atoms)\n",
    "        return PygGraph.atom_dgl_multigraph(\n",
    "            structure,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            cutoff=cutoff,\n",
    "            atom_features=\"atomic_number\",\n",
    "            max_neighbors=max_neighbors,\n",
    "            compute_line_graph=False,\n",
    "            use_canonize=use_canonize,\n",
    "            use_lattice=use_lattice,\n",
    "            use_angle=use_angle,\n",
    "        )\n",
    "    \n",
    "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
    "    # graphs = df[\"atoms\"].apply(atoms_to_graph).values\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def get_id_train_val_test(\n",
    "    total_size=1000,\n",
    "    split_seed=123,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    n_train=None,\n",
    "    n_test=None,\n",
    "    n_val=None,\n",
    "    keep_data_order=False,\n",
    "):\n",
    "    \"\"\"Get train, val, test IDs.\"\"\"\n",
    "    if (\n",
    "        train_ratio is None\n",
    "        and val_ratio is not None\n",
    "        and test_ratio is not None\n",
    "    ):\n",
    "        if train_ratio is None:\n",
    "            assert val_ratio + test_ratio < 1\n",
    "            train_ratio = 1 - val_ratio - test_ratio\n",
    "            print(\"Using rest of the dataset except the test and val sets.\")\n",
    "        else:\n",
    "            assert train_ratio + val_ratio + test_ratio <= 1\n",
    "    # indices = list(range(total_size))\n",
    "    if n_train is None:\n",
    "        n_train = int(train_ratio * total_size)\n",
    "    if n_test is None:\n",
    "        n_test = int(test_ratio * total_size)\n",
    "    if n_val is None:\n",
    "        n_val = int(val_ratio * total_size)\n",
    "    ids = list(np.arange(total_size))\n",
    "    if not keep_data_order:\n",
    "        random.seed(split_seed)\n",
    "        random.shuffle(ids)\n",
    "    if n_train + n_val + n_test > total_size:\n",
    "        raise ValueError(\n",
    "            \"Check total number of samples.\",\n",
    "            n_train + n_val + n_test,\n",
    "            \">\",\n",
    "            total_size,\n",
    "        )\n",
    "\n",
    "    id_train = ids[:n_train]\n",
    "    id_val = ids[-(n_val + n_test) : -n_test]\n",
    "    id_test = ids[-n_test:]\n",
    "    return id_train, id_val, id_test\n",
    "\n",
    "\n",
    "def get_pyg_dataset(\n",
    "    dataset=[],\n",
    "    id_tag=\"jid\",\n",
    "    target=\"\",\n",
    "    neighbor_strategy=\"\",\n",
    "    atom_features=\"\",\n",
    "    use_canonize=\"\",\n",
    "    name=\"\",\n",
    "    line_graph=\"\",\n",
    "    cutoff=8.0,\n",
    "    max_neighbors=12,\n",
    "    classification=False,\n",
    "    output_dir=\".\",\n",
    "    tmp_name=\"dataset\",\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    data_from='Jarvis',\n",
    "    use_save=False,\n",
    "    mean_train=None,\n",
    "    std_train=None,\n",
    "    now=False, # for test\n",
    "):\n",
    "    \"\"\"Get pyg Dataset.\"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    vals = df[target].values\n",
    "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
    "        val_list = [vals[i].item() for i in range(len(vals))]\n",
    "        vals = val_list\n",
    "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
    "    print(\"data range\", np.max(vals), np.min(vals))\n",
    "    print(output_dir)\n",
    "    print('graphs not saved')\n",
    "    graphs = load_pyg_graphs(\n",
    "        df,\n",
    "        name=name,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "    )\n",
    "    if mean_train == None:\n",
    "        mean_train = np.mean(vals)\n",
    "        std_train = np.std(vals)\n",
    "        data = graph.PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    else:\n",
    "        data = graph.PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    return data, mean_train, std_train\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    dataset: str = \"dft_3d\",\n",
    "    dataset_array=[],\n",
    "    target: str = \"formation_energy_peratom\",\n",
    "    atom_features: str = \"cgcnn\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    n_train=None,\n",
    "    n_val=None,\n",
    "    n_test=None,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    batch_size: int = 5,\n",
    "    standardize: bool = False,\n",
    "    line_graph: bool = True,\n",
    "    split_seed: int = 123,\n",
    "    workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "    save_dataloader: bool = False,\n",
    "    filename: str = \"sample\",\n",
    "    id_tag: str = \"jid\",\n",
    "    use_canonize: bool = False,\n",
    "    cutoff: float = 8.0,\n",
    "    max_neighbors: int = 12,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "    target_multiplication_factor: Optional[float] = None,\n",
    "    standard_scalar_and_pca=False,\n",
    "    keep_data_order=False,\n",
    "    output_features=1,\n",
    "    output_dir=None,\n",
    "    matrix_input=False,\n",
    "    pyg_input=False,\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    use_save=True,\n",
    "    mp_id_list=None,\n",
    "):\n",
    "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
    "    # data loading\n",
    "    mean_train=None\n",
    "    std_train=None\n",
    "    assert (matrix_input and pyg_input) == False\n",
    "    \n",
    "    train_sample = filename + \"_train.data\"\n",
    "    val_sample = filename + \"_val.data\"\n",
    "    test_sample = filename + \"_test.data\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if (\n",
    "        os.path.exists(train_sample)\n",
    "        and os.path.exists(val_sample)\n",
    "        and os.path.exists(test_sample)\n",
    "        and save_dataloader\n",
    "    ):\n",
    "        print(\"Loading from saved file...\")\n",
    "        print(\"Make sure all the DataLoader params are same.\")\n",
    "        print(\"This module is made for debugging only.\")\n",
    "        train_loader = torch.load(train_sample)\n",
    "        val_loader = torch.load(val_sample)\n",
    "        test_loader = torch.load(test_sample)\n",
    "        if train_loader.pin_memory != pin_memory:\n",
    "            train_loader.pin_memory = pin_memory\n",
    "        if test_loader.pin_memory != pin_memory:\n",
    "            test_loader.pin_memory = pin_memory\n",
    "        if val_loader.pin_memory != pin_memory:\n",
    "            val_loader.pin_memory = pin_memory\n",
    "        if train_loader.num_workers != workers:\n",
    "            train_loader.num_workers = workers\n",
    "        if test_loader.num_workers != workers:\n",
    "            test_loader.num_workers = workers\n",
    "        if val_loader.num_workers != workers:\n",
    "            val_loader.num_workers = workers\n",
    "        print(\"train\", len(train_loader.dataset))\n",
    "        print(\"val\", len(val_loader.dataset))\n",
    "        print(\"test\", len(test_loader.dataset))\n",
    "        return (\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            train_loader.dataset.prepare_batch,\n",
    "        )\n",
    "    else:\n",
    "        if not dataset_array:\n",
    "            d = jdata(dataset)\n",
    "        else:\n",
    "            d = dataset_array\n",
    "            # for ii, i in enumerate(pc_y):\n",
    "            #    d[ii][target] = pc_y[ii].tolist()\n",
    "\n",
    "        dat = []\n",
    "        if classification_threshold is not None:\n",
    "            print(\n",
    "                \"Using \",\n",
    "                classification_threshold,\n",
    "                \" for classifying \",\n",
    "                target,\n",
    "                \" data.\",\n",
    "            )\n",
    "            print(\"Converting target data into 1 and 0.\")\n",
    "        all_targets = []\n",
    "\n",
    "        # TODO:make an all key in qm9_dgl\n",
    "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
    "            print(\"Making all qm9_dgl\")\n",
    "            tmp = []\n",
    "            for ii in d:\n",
    "                ii[\"all\"] = [\n",
    "                    ii[\"mu\"],\n",
    "                    ii[\"alpha\"],\n",
    "                    ii[\"homo\"],\n",
    "                    ii[\"lumo\"],\n",
    "                    ii[\"gap\"],\n",
    "                    ii[\"r2\"],\n",
    "                    ii[\"zpve\"],\n",
    "                    ii[\"U0\"],\n",
    "                    ii[\"U\"],\n",
    "                    ii[\"H\"],\n",
    "                    ii[\"G\"],\n",
    "                    ii[\"Cv\"],\n",
    "                ]\n",
    "                tmp.append(ii)\n",
    "            print(\"Made all qm9_dgl\")\n",
    "            d = tmp\n",
    "        for i in d:\n",
    "            if isinstance(i[target], list):  # multioutput target\n",
    "                all_targets.append(torch.tensor(i[target]))\n",
    "                dat.append(i)\n",
    "\n",
    "            elif (\n",
    "                i[target] is not None\n",
    "                and i[target] != \"na\"\n",
    "                and not math.isnan(i[target])\n",
    "            ):\n",
    "                if target_multiplication_factor is not None:\n",
    "                    i[target] = i[target] * target_multiplication_factor\n",
    "                if classification_threshold is not None:\n",
    "                    if i[target] <= classification_threshold:\n",
    "                        i[target] = 0\n",
    "                    elif i[target] > classification_threshold:\n",
    "                        i[target] = 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Check classification data type.\",\n",
    "                            i[target],\n",
    "                            type(i[target]),\n",
    "                        )\n",
    "                dat.append(i)\n",
    "                all_targets.append(i[target])\n",
    "    \n",
    "    if mp_id_list is not None:\n",
    "        if mp_id_list == 'bulk':\n",
    "            print('using mp bulk dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "        \n",
    "        if mp_id_list == 'shear':\n",
    "            print('using mp shear dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "\n",
    "    else:\n",
    "        id_train, id_val, id_test = get_id_train_val_test(\n",
    "            total_size=len(dat),\n",
    "            split_seed=split_seed,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            n_train=n_train,\n",
    "            n_test=n_test,\n",
    "            n_val=n_val,\n",
    "            keep_data_order=keep_data_order,\n",
    "        )\n",
    "        ids_train_val_test = {}\n",
    "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
    "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
    "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
    "        dumpjson(\n",
    "            data=ids_train_val_test,\n",
    "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
    "        )\n",
    "        dataset_train = [dat[x] for x in id_train]\n",
    "        dataset_val = [dat[x] for x in id_val]\n",
    "        dataset_test = [dat[x] for x in id_test]\n",
    "    \n",
    "    train_data, mean_train, std_train = get_pyg_dataset(\n",
    "        dataset=dataset_train,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"train_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "    )\n",
    "    val_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_val,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"val_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "    test_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_test,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"test_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "\n",
    "    \n",
    "    collate_fn = train_data.collate\n",
    "    if line_graph:\n",
    "        collate_fn = train_data.collate_line_graph\n",
    "\n",
    "    # use a regular pytorch dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    if save_dataloader:\n",
    "        torch.save(train_loader, train_sample)\n",
    "        torch.save(val_loader, val_sample)\n",
    "        torch.save(test_loader, test_sample)\n",
    "    \n",
    "    print(\"n_train:\", len(train_loader.dataset))\n",
    "    print(\"n_val:\", len(val_loader.dataset))\n",
    "    print(\"n_test:\", len(test_loader.dataset))\n",
    "    return (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_loader.dataset.prepare_batch,\n",
    "        mean_train,\n",
    "        std_train,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
