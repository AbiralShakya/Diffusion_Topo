{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining 3D dataset 76k ...\n",
      "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
      "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
      "Loading the zipfile...\n",
      "Loading completed.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'base_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m mp_map = []\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m jid \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mjid\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     entry = \u001b[43mApi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_data_by_template_id\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdft_3d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# entry.db_ids is guaranteed to be a dict if it exists\u001b[39;00m\n\u001b[32m     50\u001b[39m     mp_map.append({\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjid\u001b[39m\u001b[33m\"\u001b[39m: jid,\n\u001b[32m     52\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmpid\u001b[39m\u001b[33m\"\u001b[39m: entry.db_ids.get(\u001b[33m\"\u001b[39m\u001b[33mmaterialsproject\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# maybe None\u001b[39;00m\n\u001b[32m     53\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/jarvis/db/restapi.py:205\u001b[39m, in \u001b[36mApi.get_data_by_template_id\u001b[39m\u001b[34m(self, id, query, max_record)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03mGet data by template IDs.\u001b[39;00m\n\u001b[32m    199\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03mand is_disabled=False\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m xml_upload_url = \u001b[33m\"\u001b[39m\u001b[33m/rest/data/query/keyword/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m turl = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m + xml_upload_url\n\u001b[32m    206\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mturl\u001b[39m\u001b[33m\"\u001b[39m, turl)\n\u001b[32m    207\u001b[39m \u001b[38;5;28minput\u001b[39m = {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: query}\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'base_url'"
     ]
    }
   ],
   "source": [
    "\"\"\"Implementation based on the template of ALIGNN.\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jarvis.core.atoms import Atoms\n",
    "import graph\n",
    "#from ComFormerFork.graph import PygGraph, PygStructureDataset\n",
    "from jarvis.db.figshare import data as jdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from jarvis.db.jsonutils import dumpjson\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# use pandas progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "load_dotenv()\n",
    "load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\"))\n",
    "API_KEY = os.getenv(\"MP_API_KEY\")\n",
    "\n",
    "dft3d = jdata(\"dft_3d\")               \n",
    "df = pd.DataFrame(dft3d)     \n",
    "\n",
    "# print(df.columns)\n",
    "# print(type(dft3d[0]))\n",
    "# try:\n",
    "#     print(dft3d[0].keys())          # if it’s a dict-like\n",
    "# except:\n",
    "#     print(vars(dft3d[0]))           # if it’s an object with attrs\n",
    "\n",
    "from jarvis.db.restapi import Api\n",
    "\n",
    "mp_map = []\n",
    "for jid in df[\"jid\"]:\n",
    "    entry = Api.get_data_by_template_id(\"dft_3d\", jid)\n",
    "    # entry.db_ids is guaranteed to be a dict if it exists\n",
    "    mp_map.append({\n",
    "        \"jid\": jid,\n",
    "        \"mpid\": entry.db_ids.get(\"materialsproject\")  # maybe None\n",
    "    })\n",
    "mp_df = pd.DataFrame(mp_map)\n",
    "\n",
    "df = df.merge(mp_df, on=\"jid\", how=\"left\")\n",
    "\n",
    "# 3) (Optional) drop any rows where mpid is missing\n",
    "#df = df.dropna(subset=[\"mpid\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df[\"mpid\"] = df[\"db_ids\"].apply(lambda x: x.get(\"materialsproject\") if isinstance(x, dict) else None)\n",
    "# df = df.dropna(subset=[\"mpid\"])        # keep only entries with an MP match\n",
    "\n",
    "\n",
    "# with MPRester(api_key = API_KEY) as m:\n",
    "#     docs = m.materials.summary.search(\n",
    "#         jid__in=df[\"jid\"].tolist(),\n",
    "#         fields=[\"jid\",\"topological_type\"]\n",
    "#     )\n",
    "\n",
    "with MPRester(API_KEY) as m:\n",
    "    docs = m.materials.summary.search(\n",
    "        material_ids=df[\"mpid\"].tolist(),\n",
    "        fields=[\"material_id\",\"topological_type\"]\n",
    "    )\n",
    "label_df = pd.DataFrame([{\n",
    "    \"mpid\": d.material_id,\n",
    "    \"topo_type\": d.topological_type\n",
    "} for d in docs])\n",
    "# map to binary\n",
    "good = {\"Z2_INSULATOR\",\"CRYSTALLINE_INSULATOR\"}\n",
    "label_df[\"is_topo\"] = label_df[\"topo_type\"].isin(good).astype(int)\n",
    "\n",
    "df = df.merge(label_df[[\"mpid\",\"is_topo\"]], on=\"mpid\", how=\"left\")\n",
    "df[\"is_topo\"] = df[\"is_topo\"].fillna(0).astype(int)\n",
    "\n",
    "def load_dataset(\n",
    "    name: str = \"dft_3d\",\n",
    "    target=None,\n",
    "    limit: Optional[int] = None,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"Load jarvis data.\"\"\"\n",
    "    d = jdata(name)\n",
    "    data = []\n",
    "    for i in d:\n",
    "        if i[target] != \"na\" and not math.isnan(i[target]):\n",
    "            if classification_threshold is not None:\n",
    "                if i[target] <= classification_threshold:\n",
    "                    i[target] = 0\n",
    "                elif i[target] > classification_threshold:\n",
    "                    i[target] = 1\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"Check classification data type.\",\n",
    "                        i[target],\n",
    "                        type(i[target]),\n",
    "                    )\n",
    "            data.append(i)\n",
    "    d = data\n",
    "    if limit is not None:\n",
    "        d = d[:limit]\n",
    "    d = pd.DataFrame(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def mean_absolute_deviation(data, axis=None):\n",
    "    \"\"\"Get Mean absolute deviation.\"\"\"\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def load_pyg_graphs(\n",
    "    df: pd.DataFrame,\n",
    "    name: str = \"dft_3d\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    cutoff: float = 8,\n",
    "    max_neighbors: int = 12,\n",
    "    cachedir: Optional[Path] = None,\n",
    "    use_canonize: bool = False,\n",
    "    use_lattice: bool = False,\n",
    "    use_angle: bool = False,\n",
    "):\n",
    "    \"\"\"Construct crystal graphs.\n",
    "\n",
    "    Load only atomic number node features\n",
    "    and bond displacement vector edge features.\n",
    "\n",
    "    Resulting graphs have scheme e.g.\n",
    "    ```\n",
    "    Graph(num_nodes=12, num_edges=156,\n",
    "          ndata_schemes={'atom_features': Scheme(shape=(1,)}\n",
    "          edata_schemes={'r': Scheme(shape=(3,)})\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def atoms_to_graph(atoms):\n",
    "        \"\"\"Convert structure dict to DGLGraph.\"\"\"\n",
    "        structure = Atoms.from_dict(atoms)\n",
    "        return PygGraph.atom_dgl_multigraph(\n",
    "            structure,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            cutoff=cutoff,\n",
    "            atom_features=\"atomic_number\",\n",
    "            max_neighbors=max_neighbors,\n",
    "            compute_line_graph=False,\n",
    "            use_canonize=use_canonize,\n",
    "            use_lattice=use_lattice,\n",
    "            use_angle=use_angle,\n",
    "        )\n",
    "    \n",
    "    graphs = df[\"atoms\"].parallel_apply(atoms_to_graph).values\n",
    "    # graphs = df[\"atoms\"].apply(atoms_to_graph).values\n",
    "\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def get_id_train_val_test(\n",
    "    total_size=1000,\n",
    "    split_seed=123,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    n_train=None,\n",
    "    n_test=None,\n",
    "    n_val=None,\n",
    "    keep_data_order=False,\n",
    "):\n",
    "    \"\"\"Get train, val, test IDs.\"\"\"\n",
    "    if (\n",
    "        train_ratio is None\n",
    "        and val_ratio is not None\n",
    "        and test_ratio is not None\n",
    "    ):\n",
    "        if train_ratio is None:\n",
    "            assert val_ratio + test_ratio < 1\n",
    "            train_ratio = 1 - val_ratio - test_ratio\n",
    "            print(\"Using rest of the dataset except the test and val sets.\")\n",
    "        else:\n",
    "            assert train_ratio + val_ratio + test_ratio <= 1\n",
    "    # indices = list(range(total_size))\n",
    "    if n_train is None:\n",
    "        n_train = int(train_ratio * total_size)\n",
    "    if n_test is None:\n",
    "        n_test = int(test_ratio * total_size)\n",
    "    if n_val is None:\n",
    "        n_val = int(val_ratio * total_size)\n",
    "    ids = list(np.arange(total_size))\n",
    "    if not keep_data_order:\n",
    "        random.seed(split_seed)\n",
    "        random.shuffle(ids)\n",
    "    if n_train + n_val + n_test > total_size:\n",
    "        raise ValueError(\n",
    "            \"Check total number of samples.\",\n",
    "            n_train + n_val + n_test,\n",
    "            \">\",\n",
    "            total_size,\n",
    "        )\n",
    "\n",
    "    id_train = ids[:n_train]\n",
    "    id_val = ids[-(n_val + n_test) : -n_test]\n",
    "    id_test = ids[-n_test:]\n",
    "    return id_train, id_val, id_test\n",
    "\n",
    "\n",
    "def get_pyg_dataset(\n",
    "    dataset=[],\n",
    "    id_tag=\"jid\",\n",
    "    target=\"\",\n",
    "    neighbor_strategy=\"\",\n",
    "    atom_features=\"\",\n",
    "    use_canonize=\"\",\n",
    "    name=\"\",\n",
    "    line_graph=\"\",\n",
    "    cutoff=8.0,\n",
    "    max_neighbors=12,\n",
    "    classification=False,\n",
    "    output_dir=\".\",\n",
    "    tmp_name=\"dataset\",\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    data_from='Jarvis',\n",
    "    use_save=False,\n",
    "    mean_train=None,\n",
    "    std_train=None,\n",
    "    now=False, # for test\n",
    "):\n",
    "    \"\"\"Get pyg Dataset.\"\"\"\n",
    "    df = pd.DataFrame(dataset)\n",
    "    vals = df[target].values\n",
    "    if target == \"shear modulus\" or target == \"bulk modulus\":\n",
    "        val_list = [vals[i].item() for i in range(len(vals))]\n",
    "        vals = val_list\n",
    "    output_dir = \"./saved_data/\" + tmp_name + \"test_graph_angle.pkl\" # for fast test use\n",
    "    print(\"data range\", np.max(vals), np.min(vals))\n",
    "    print(output_dir)\n",
    "    print('graphs not saved')\n",
    "    graphs = load_pyg_graphs(\n",
    "        df,\n",
    "        name=name,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "    )\n",
    "    if mean_train == None:\n",
    "        mean_train = np.mean(vals)\n",
    "        std_train = np.std(vals)\n",
    "        data = PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    else:\n",
    "        data = PygStructureDataset(\n",
    "            df,\n",
    "            graphs,\n",
    "            target=target,\n",
    "            atom_features=atom_features,\n",
    "            line_graph=line_graph,\n",
    "            id_tag=id_tag,\n",
    "            classification=classification,\n",
    "            neighbor_strategy=neighbor_strategy,\n",
    "            mean_train=mean_train,\n",
    "            std_train=std_train,\n",
    "        )\n",
    "    return data, mean_train, std_train\n",
    "\n",
    "\n",
    "def get_train_val_loaders(\n",
    "    dataset: str = \"dft_3d\",\n",
    "    dataset_array=[],\n",
    "    target: str = \"formation_energy_peratom\",\n",
    "    atom_features: str = \"cgcnn\",\n",
    "    neighbor_strategy: str = \"k-nearest\",\n",
    "    n_train=None,\n",
    "    n_val=None,\n",
    "    n_test=None,\n",
    "    train_ratio=None,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    batch_size: int = 5,\n",
    "    standardize: bool = False,\n",
    "    line_graph: bool = True,\n",
    "    split_seed: int = 123,\n",
    "    workers: int = 0,\n",
    "    pin_memory: bool = True,\n",
    "    save_dataloader: bool = False,\n",
    "    filename: str = \"sample\",\n",
    "    id_tag: str = \"jid\",\n",
    "    use_canonize: bool = False,\n",
    "    cutoff: float = 8.0,\n",
    "    max_neighbors: int = 12,\n",
    "    classification_threshold: Optional[float] = None,\n",
    "    target_multiplication_factor: Optional[float] = None,\n",
    "    standard_scalar_and_pca=False,\n",
    "    keep_data_order=False,\n",
    "    output_features=1,\n",
    "    output_dir=None,\n",
    "    matrix_input=False,\n",
    "    pyg_input=False,\n",
    "    use_lattice=False,\n",
    "    use_angle=False,\n",
    "    use_save=True,\n",
    "    mp_id_list=None,\n",
    "):\n",
    "    \"\"\"Help function to set up JARVIS train and val dataloaders.\"\"\"\n",
    "    # data loading\n",
    "    mean_train=None\n",
    "    std_train=None\n",
    "    assert (matrix_input and pyg_input) == False\n",
    "    \n",
    "    train_sample = filename + \"_train.data\"\n",
    "    val_sample = filename + \"_val.data\"\n",
    "    test_sample = filename + \"_test.data\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    if (\n",
    "        os.path.exists(train_sample)\n",
    "        and os.path.exists(val_sample)\n",
    "        and os.path.exists(test_sample)\n",
    "        and save_dataloader\n",
    "    ):\n",
    "        print(\"Loading from saved file...\")\n",
    "        print(\"Make sure all the DataLoader params are same.\")\n",
    "        print(\"This module is made for debugging only.\")\n",
    "        train_loader = torch.load(train_sample)\n",
    "        val_loader = torch.load(val_sample)\n",
    "        test_loader = torch.load(test_sample)\n",
    "        if train_loader.pin_memory != pin_memory:\n",
    "            train_loader.pin_memory = pin_memory\n",
    "        if test_loader.pin_memory != pin_memory:\n",
    "            test_loader.pin_memory = pin_memory\n",
    "        if val_loader.pin_memory != pin_memory:\n",
    "            val_loader.pin_memory = pin_memory\n",
    "        if train_loader.num_workers != workers:\n",
    "            train_loader.num_workers = workers\n",
    "        if test_loader.num_workers != workers:\n",
    "            test_loader.num_workers = workers\n",
    "        if val_loader.num_workers != workers:\n",
    "            val_loader.num_workers = workers\n",
    "        print(\"train\", len(train_loader.dataset))\n",
    "        print(\"val\", len(val_loader.dataset))\n",
    "        print(\"test\", len(test_loader.dataset))\n",
    "        return (\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            train_loader.dataset.prepare_batch,\n",
    "        )\n",
    "    else:\n",
    "        if not dataset_array:\n",
    "            d = jdata(dataset)\n",
    "        else:\n",
    "            d = dataset_array\n",
    "            # for ii, i in enumerate(pc_y):\n",
    "            #    d[ii][target] = pc_y[ii].tolist()\n",
    "\n",
    "        dat = []\n",
    "        if classification_threshold is not None:\n",
    "            print(\n",
    "                \"Using \",\n",
    "                classification_threshold,\n",
    "                \" for classifying \",\n",
    "                target,\n",
    "                \" data.\",\n",
    "            )\n",
    "            print(\"Converting target data into 1 and 0.\")\n",
    "        all_targets = []\n",
    "\n",
    "        # TODO:make an all key in qm9_dgl\n",
    "        if dataset == \"qm9_dgl\" and target == \"all\":\n",
    "            print(\"Making all qm9_dgl\")\n",
    "            tmp = []\n",
    "            for ii in d:\n",
    "                ii[\"all\"] = [\n",
    "                    ii[\"mu\"],\n",
    "                    ii[\"alpha\"],\n",
    "                    ii[\"homo\"],\n",
    "                    ii[\"lumo\"],\n",
    "                    ii[\"gap\"],\n",
    "                    ii[\"r2\"],\n",
    "                    ii[\"zpve\"],\n",
    "                    ii[\"U0\"],\n",
    "                    ii[\"U\"],\n",
    "                    ii[\"H\"],\n",
    "                    ii[\"G\"],\n",
    "                    ii[\"Cv\"],\n",
    "                ]\n",
    "                tmp.append(ii)\n",
    "            print(\"Made all qm9_dgl\")\n",
    "            d = tmp\n",
    "        for i in d:\n",
    "            if isinstance(i[target], list):  # multioutput target\n",
    "                all_targets.append(torch.tensor(i[target]))\n",
    "                dat.append(i)\n",
    "\n",
    "            elif (\n",
    "                i[target] is not None\n",
    "                and i[target] != \"na\"\n",
    "                and not math.isnan(i[target])\n",
    "            ):\n",
    "                if target_multiplication_factor is not None:\n",
    "                    i[target] = i[target] * target_multiplication_factor\n",
    "                if classification_threshold is not None:\n",
    "                    if i[target] <= classification_threshold:\n",
    "                        i[target] = 0\n",
    "                    elif i[target] > classification_threshold:\n",
    "                        i[target] = 1\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"Check classification data type.\",\n",
    "                            i[target],\n",
    "                            type(i[target]),\n",
    "                        )\n",
    "                dat.append(i)\n",
    "                all_targets.append(i[target])\n",
    "    \n",
    "    if mp_id_list is not None:\n",
    "        if mp_id_list == 'bulk':\n",
    "            print('using mp bulk dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/bulk_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "        \n",
    "        if mp_id_list == 'shear':\n",
    "            print('using mp shear dataset')\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_train.pkl', 'rb') as f:\n",
    "                dataset_train = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_val.pkl', 'rb') as f:\n",
    "                dataset_val = pk.load(f)\n",
    "            with open('/data/keqiangyan/bulk_shear/shear_megnet_test.pkl', 'rb') as f:\n",
    "                dataset_test = pk.load(f)\n",
    "\n",
    "    else:\n",
    "        id_train, id_val, id_test = get_id_train_val_test(\n",
    "            total_size=len(dat),\n",
    "            split_seed=split_seed,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "            n_train=n_train,\n",
    "            n_test=n_test,\n",
    "            n_val=n_val,\n",
    "            keep_data_order=keep_data_order,\n",
    "        )\n",
    "        ids_train_val_test = {}\n",
    "        ids_train_val_test[\"id_train\"] = [dat[i][id_tag] for i in id_train]\n",
    "        ids_train_val_test[\"id_val\"] = [dat[i][id_tag] for i in id_val]\n",
    "        ids_train_val_test[\"id_test\"] = [dat[i][id_tag] for i in id_test]\n",
    "        dumpjson(\n",
    "            data=ids_train_val_test,\n",
    "            filename=os.path.join(output_dir, \"ids_train_val_test.json\"),\n",
    "        )\n",
    "        dataset_train = [dat[x] for x in id_train]\n",
    "        dataset_val = [dat[x] for x in id_val]\n",
    "        dataset_test = [dat[x] for x in id_test]\n",
    "    \n",
    "    train_data, mean_train, std_train = get_pyg_dataset(\n",
    "        dataset=dataset_train,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"train_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "    )\n",
    "    val_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_val,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"val_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "    test_data,_,_ = get_pyg_dataset(\n",
    "        dataset=dataset_test,\n",
    "        id_tag=id_tag,\n",
    "        atom_features=atom_features,\n",
    "        target=target,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        use_canonize=use_canonize,\n",
    "        name=dataset,\n",
    "        line_graph=line_graph,\n",
    "        cutoff=cutoff,\n",
    "        max_neighbors=max_neighbors,\n",
    "        classification=classification_threshold is not None,\n",
    "        output_dir=output_dir,\n",
    "        tmp_name=\"test_data\",\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "        use_save=False,\n",
    "        mean_train=mean_train,\n",
    "        std_train=std_train,\n",
    "    )\n",
    "\n",
    "    \n",
    "    collate_fn = train_data.collate\n",
    "    if line_graph:\n",
    "        collate_fn = train_data.collate_line_graph\n",
    "\n",
    "    # use a regular pytorch dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "        num_workers=workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    if save_dataloader:\n",
    "        torch.save(train_loader, train_sample)\n",
    "        torch.save(val_loader, val_sample)\n",
    "        torch.save(test_loader, test_sample)\n",
    "    \n",
    "    print(\"n_train:\", len(train_loader.dataset))\n",
    "    print(\"n_val:\", len(val_loader.dataset))\n",
    "    print(\"n_test:\", len(test_loader.dataset))\n",
    "    return (\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_loader.dataset.prepare_batch,\n",
    "        mean_train,\n",
    "        std_train,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
