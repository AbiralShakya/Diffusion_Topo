{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first attempt at multi task learning and integration with topological quantum chemistry database\n",
    "import e3nn.util\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "import torch_scatter\n",
    "\n",
    "import e3nn\n",
    "from e3nn import o3\n",
    "#from e3nn.util.datatypes import DataPeriodicNeighbors\n",
    "#from e3nn.nn._gate import GatedConvParityNetwork\n",
    "#from e3nn.math._linalg import Kernel\n",
    "\n",
    "import pymatgen as mg\n",
    "import pymatgen.io\n",
    "from pymatgen.core.structure import Structure\n",
    "import pymatgen.analysis.magnetism.analyzer as pg\n",
    "from mp_api.client import MPRester\n",
    "import numpy as np\n",
    "import pickle\n",
    "from mendeleev import element\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import time, os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "load_dotenv(Path(\"/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/matprojectapi.env\"))\n",
    "api_key = os.getenv(\"MP_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pymatgen as pg\n",
    "from pymatgen.ext.matproj import MPRester\n",
    "from pymatgen.analysis.magnetism import CollinearMagneticStructureAnalyzer\n",
    "\n",
    "order_list_mp = []\n",
    "structures_list_mp = []\n",
    "formula_list_mp = []\n",
    "sites_list = []\n",
    "id_list_mp = []\n",
    "y_values_mp = []\n",
    "\n",
    "order_encode = {\"NM\": 0, \"AFM\": 1, \"FM\": 2, \"FiM\": 2}\n",
    "topo_encode = {False: 0, True: 1}\n",
    "\n",
    "# Load data\n",
    "mp_structures_dict = torch.load('/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/Integrated_Magnetic_Topological/magnetic_order/preload_data/mp_structures_2025-04-07_12-52.pt', \n",
    "                                weights_only=False)\n",
    "\n",
    "structures = mp_structures_dict['structures']\n",
    "materials = mp_structures_dict['materials_id']\n",
    "formulas = mp_structures_dict['formulas']\n",
    "orders = mp_structures_dict['order']\n",
    "nsites = mp_structures_dict['nsites']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list = []\n",
    "\n",
    "for struct in structures:\n",
    "    analyzer = CollinearMagneticStructureAnalyzer(struct)\n",
    "    order_list.append(analyzer.ordering.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_NM = [i for i, order in enumerate(order_list) if order == 'NM']\n",
    "id_AFM = [i for i, order in enumerate(order_list) if order == 'AFM']\n",
    "id_FM = [i for i, order in enumerate(order_list) if order in ['FM', 'FiM']]\n",
    "\n",
    "# Shuffle\n",
    "np.random.shuffle(id_NM)\n",
    "np.random.shuffle(id_FM)\n",
    "np.random.shuffle(id_AFM)\n",
    "\n",
    "# Balance dataset (keeping AFM as reference size)\n",
    "id_AFM, id_AFM_to_delete = np.split(id_AFM, [int(len(id_AFM))])\n",
    "id_NM, id_NM_to_delete = np.split(id_NM, [int(1.2 * len(id_AFM))])\n",
    "id_FM, id_FM_to_delete = np.split(id_FM, [int(1.2 * len(id_AFM))])\n",
    "\n",
    "# Final index list\n",
    "selected_ids = np.concatenate((id_NM, id_FM, id_AFM))\n",
    "np.random.shuffle(selected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in selected_ids:\n",
    "    structure = structures[idx]\n",
    "    material_id = materials[idx]\n",
    "    formula = formulas[idx]\n",
    "    nsite = nsites[idx]\n",
    "\n",
    "    analyzer = CollinearMagneticStructureAnalyzer(structure)\n",
    "    ordering = analyzer.ordering\n",
    "\n",
    "    structures_list_mp.append(structure)\n",
    "    id_list_mp.append(material_id)\n",
    "    formula_list_mp.append(formula)\n",
    "    sites_list.append(nsite)\n",
    "    order_list_mp.append(ordering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 28728.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 21732.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-1184113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 26214.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 20460.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-1076920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 18893.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-1192789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 24966.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-1014111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 22192.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-10753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving SummaryDoc documents: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topological info for mp-1184067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topo_encode = {False: 0, True: 1}\n",
    "topo_labels = []\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "m = MPRester(api_key=api_key)\n",
    "\n",
    "for material_id in id_list_mp:\n",
    "    try:\n",
    "        result = m.materials.summary.search(material_ids=[material_id])\n",
    "        if result and hasattr(result[0], \"is_topological\"):\n",
    "            label = result[0].is_topological\n",
    "            topo_labels.append(topo_encode[label])\n",
    "        else:\n",
    "            print(f\"No topological info for {material_id}\")\n",
    "            topo_labels.append(topo_encode[False])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving TI label for {material_id}: {e}\")\n",
    "        topo_labels.append(topo_encode[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymatgen.core import Structure\n",
    "\n",
    "def fetch_tqc_magnetic_data(bcs_id=\"3.7\"):\n",
    "    \"\"\"\n",
    "    Fetch magnetic topological materials data from the TQC database\n",
    "    \"\"\"\n",
    "    # Simulated example structure — replace with API call in production\n",
    "    return {\n",
    "        \"magnetic_materials\": [\n",
    "            {\n",
    "                \"id\": \"mp-123\",\n",
    "                \"formula\": \"Fe2O3\",\n",
    "                \"spacegroup\": 167,\n",
    "                \"magnetic_ordering\": \"AFM\",\n",
    "                \"topological_class\": \"Strong TI\",\n",
    "                \"band_gap\": 0.8,\n",
    "                \"magnetic_moment\": 4.2\n",
    "            },\n",
    "            # Add more entries as needed\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_combined_dataset(mp_structures, tqc_data):\n",
    "    \"\"\"\n",
    "    Combine Materials Project structures (as Structure objects or dicts) with TQC magnetic data\n",
    "    \"\"\"\n",
    "    tqc_map = {item[\"id\"]: item for item in tqc_data[\"magnetic_materials\"]}\n",
    "    combined_data = []\n",
    "\n",
    "    for struct in mp_structures:\n",
    "        # Handle both dict and Structure input\n",
    "        if isinstance(struct, dict):\n",
    "            mp_id = struct.get(\"material_id\")\n",
    "            structure = struct.get(\"structure\")\n",
    "            formula = struct.get(\"pretty_formula\")\n",
    "            nsites = struct.get(\"nsites\")\n",
    "        elif isinstance(struct, Structure):\n",
    "            mp_id = getattr(struct, \"material_id\", None)\n",
    "            structure = struct\n",
    "            formula = structure.formula\n",
    "            nsites = structure.num_sites\n",
    "        else:\n",
    "            continue  # skip unknown formats\n",
    "\n",
    "        if mp_id in tqc_map:\n",
    "            tqc_info = tqc_map[mp_id]\n",
    "            struct_data = {\n",
    "                \"structure\": structure,\n",
    "                \"material_id\": mp_id,\n",
    "                \"formula\": formula,\n",
    "                \"nsites\": nsites,\n",
    "                \"magnetic_ordering\": tqc_info[\"magnetic_ordering\"],\n",
    "                \"topological_class\": tqc_info[\"topological_class\"],\n",
    "                \"band_gap\": tqc_info.get(\"band_gap\", None),\n",
    "                \"magnetic_moment\": tqc_info.get(\"magnetic_moment\", None),\n",
    "                \"symmetry_operations\": tqc_info.get(\"symmetry_operations\", None)\n",
    "            }\n",
    "            combined_data.append(struct_data)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Example usage:\n",
    "tqc_data = fetch_tqc_magnetic_data(bcs_id=\"3.7\")\n",
    "combined_dataset = create_combined_dataset(structures, tqc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.analysis.local_env import CrystalNN\n",
    "from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n",
    "import torch\n",
    "\n",
    "def extract_magnetic_features(structure):\n",
    "    \"\"\"\n",
    "    Extract features relevant for magnetic ordering classification\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # 1. Magnetic elements\n",
    "    magnetic_elements = ['Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Ce', 'Pr', 'Nd',\n",
    "                         'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb']\n",
    "    element_counts = {}\n",
    "    total_atoms = len(structure)\n",
    "\n",
    "    for site in structure:\n",
    "        symbol = str(site.specie.symbol)\n",
    "        element_counts[symbol] = element_counts.get(symbol, 0) + 1\n",
    "\n",
    "    magnetic_fraction = sum(element_counts.get(el, 0) for el in magnetic_elements) / total_atoms\n",
    "    features['magnetic_fraction'] = magnetic_fraction\n",
    "\n",
    "    # 2. Magnetic exchange pathways\n",
    "    magnetic_sites = [i for i, site in enumerate(structure) if str(site.specie.symbol) in magnetic_elements]\n",
    "    exchange_distances = []\n",
    "\n",
    "    for i in magnetic_sites:\n",
    "        for j in magnetic_sites:\n",
    "            if i < j:\n",
    "                distance = structure.get_distance(i, j)\n",
    "                if distance < 4.0:\n",
    "                    exchange_distances.append(distance)\n",
    "\n",
    "    if exchange_distances:\n",
    "        features['avg_exchange_distance'] = sum(exchange_distances) / len(exchange_distances)\n",
    "        features['min_exchange_distance'] = min(exchange_distances)\n",
    "    else:\n",
    "        features['avg_exchange_distance'] = 0.0\n",
    "        features['min_exchange_distance'] = 0.0\n",
    "\n",
    "    # 3. Crystal field distortion (optional, just log one example distortion)\n",
    "    # NOTE: This can produce many features, we log just one for simplicity\n",
    "    distortion_list = []\n",
    "    for i in magnetic_sites:\n",
    "        neighbors = structure.get_neighbors(structure[i], 3.0)\n",
    "        if neighbors:\n",
    "            distances = [n[1] for n in neighbors]\n",
    "            avg_distance = sum(distances) / len(distances)\n",
    "            distortion = sum((d - avg_distance)**2 for d in distances) / len(distances)\n",
    "            distortion_list.append(distortion)\n",
    "\n",
    "    features['avg_coordination_distortion'] = (\n",
    "        sum(distortion_list) / len(distortion_list) if distortion_list else 0.0\n",
    "    )\n",
    "\n",
    "    # 4. Symmetry features\n",
    "    try:\n",
    "        analyzer = SpacegroupAnalyzer(structure)\n",
    "        spacegroup = analyzer.get_space_group_number()\n",
    "    except Exception:\n",
    "        spacegroup = 0  # fallback if symmetry detection fails\n",
    "\n",
    "    features['spacegroup'] = spacegroup\n",
    "\n",
    "    # 5. Time-reversal breaking (heuristic)\n",
    "    features['potential_time_reversal_breaking'] = 1 if magnetic_fraction > 0.1 else 0\n",
    "\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to all structures\n",
    "magnetic_features = []\n",
    "for struct in structures_list_mp:\n",
    "    magnetic_features.append(extract_magnetic_features(struct))\n",
    "\n",
    "# Convert to tensor (ensure values are numeric)\n",
    "magnetic_feature_tensor = torch.tensor([\n",
    "    [\n",
    "        float(f['magnetic_fraction']),\n",
    "        float(f['avg_exchange_distance']),\n",
    "        float(f['min_exchange_distance']),\n",
    "        int(f['spacegroup']),\n",
    "        int(f['potential_time_reversal_breaking'])\n",
    "    ]\n",
    "    for f in magnetic_features\n",
    "], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import e3nn.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter import scatter_add\n",
    "\n",
    "class MagneticTopologicalClassifier(torch.nn.Module):\n",
    "    def __init__(self, atom_type_in, hidden_dim=128, model_kwargs=None):\n",
    "        super().__init__()\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        # Atom embedding\n",
    "        self.atom_embedding = torch.nn.Linear(atom_type_in, hidden_dim)\n",
    "        \n",
    "        # Magnetic attention module\n",
    "        self.magnetic_attention = MagneticAttention(hidden_dim)\n",
    "        \n",
    "        # E3NN convolution layers (you must define GatedConvParityNetwork elsewhere)\n",
    "        self.model = e3nn.nn.Gate(**model_kwargs)\n",
    "       # self.model = GatedConvParityNetwork(**model_kwargs)\n",
    "        \n",
    "        # Magnetic ordering head\n",
    "        self.magnetic_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 3)  # NM, AFM, FM/FiM\n",
    "        )\n",
    "        \n",
    "        # Topological classification head\n",
    "        self.topological_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 4)  # None, Weak TI, Strong TI, HOTI\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, n_norm=35):\n",
    "        # Initial embedding\n",
    "        x = self.atom_embedding(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Magnetic attention layer\n",
    "        x = self.magnetic_attention(x, edge_index, edge_attr)\n",
    "        \n",
    "        # E3NN-based processing\n",
    "        x = self.model(x, edge_index, edge_attr, n_norm=n_norm)\n",
    "        \n",
    "        # Global pooling\n",
    "        if batch is None:\n",
    "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        x_global = scatter_add(x, batch, dim=0)\n",
    "        \n",
    "        # Classification heads\n",
    "        magnetic_pred = self.magnetic_head(x_global)\n",
    "        topological_pred = self.topological_head(x_global)\n",
    "        \n",
    "        return magnetic_pred, topological_pred\n",
    "\n",
    "\n",
    "class MagneticAttention(MessagePassing):\n",
    "    \"\"\"\n",
    "    Attention mechanism for magnetic interactions\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__(aggr='add')  # Aggregation function\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.att_proj = nn.Linear(2 * hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        return self.propagate(edge_index, q=q, k=k, v=v, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, q_i, k_j, v_j, edge_attr):\n",
    "        attention_input = torch.cat([q_i, k_j], dim=-1)\n",
    "        alpha = F.leaky_relu(self.att_proj(attention_input).squeeze(-1))\n",
    "        alpha = torch.softmax(alpha, dim=0)\n",
    "        return alpha.unsqueeze(-1) * v_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def evaluate_multi_task(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    magnetic_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    topological_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    magnetic_loss_cumulative = 0.0\n",
    "    topological_loss_cumulative = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in dataloader:\n",
    "            d.to(device)\n",
    "            # Forward pass\n",
    "            magnetic_pred, topological_pred = model(d.x, d.edge_index, d.edge_attr, \n",
    "                                                  n_norm=True, batch=d.batch)\n",
    "            # Compute losses\n",
    "            magnetic_loss = magnetic_loss_fn(magnetic_pred, d.magnetic_y)\n",
    "            topological_loss = topological_loss_fn(topological_pred, d.topological_y)\n",
    "            \n",
    "            # Update cumulative losses\n",
    "            magnetic_loss_cumulative += magnetic_loss.detach().item()\n",
    "            topological_loss_cumulative += topological_loss.detach().item()\n",
    "    \n",
    "    # Compute average losses\n",
    "    magnetic_valid_loss = magnetic_loss_cumulative / len(dataloader)\n",
    "    topological_valid_loss = topological_loss_cumulative / len(dataloader)\n",
    "    \n",
    "    return magnetic_valid_loss, topological_valid_loss\n",
    "\n",
    "def multi_task_training(model, dataloader, dataloader_valid, max_iter=101, device=\"cpu\"):\n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'adamw_lr': 0.001,  # Learning rate for AdamW optimizer\n",
    "        'adamw_wd': 0.01    # Weight decay for AdamW optimizer\n",
    "    }\n",
    "    \n",
    "    model.to(device)\n",
    "    # Loss functions\n",
    "    magnetic_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    topological_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['adamw_lr'], weight_decay=params['adamw_wd'])\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.78)\n",
    "    \n",
    "    # Training metrics\n",
    "    valid_loss_min = np.inf\n",
    "    \n",
    "    for step in range(max_iter):\n",
    "        model.train()\n",
    "        magnetic_loss_cumulative = 0.0\n",
    "        topological_loss_cumulative = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for j, d in enumerate(dataloader):\n",
    "            d.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            magnetic_pred, topological_pred = model(d.x, d.edge_index, d.edge_attr, \n",
    "                                                  n_norm=True, batch=d.batch)\n",
    "            \n",
    "            # Compute losses\n",
    "            magnetic_loss = magnetic_loss_fn(magnetic_pred, d.magnetic_y)\n",
    "            topological_loss = topological_loss_fn(topological_pred, d.topological_y)\n",
    "            \n",
    "            # Apply class weighting for imbalanced classes\n",
    "            #TODO: can change this\n",
    "            cost_multiplier = 1\n",
    "            if d.magnetic_y.item() == 2: # FM/FiM classes\n",
    "                magnetic_loss = cost_multiplier * magnetic_loss\n",
    "            \n",
    "            # Combine losses\n",
    "            combined_loss = magnetic_loss + topological_loss\n",
    "            \n",
    "            # Update cumulative losses\n",
    "            magnetic_loss_cumulative += magnetic_loss.detach().item()\n",
    "            topological_loss_cumulative += topological_loss.detach().item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Compute average losses\n",
    "        magnetic_train_loss = magnetic_loss_cumulative / len(dataloader)\n",
    "        topological_train_loss = topological_loss_cumulative / len(dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        magnetic_valid_loss, topological_valid_loss = evaluate_multi_task(model, dataloader_valid, device)\n",
    "        \n",
    "        # Log progress\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step:4d}/{max_iter - 1:4d} \"\n",
    "                  f\"Magnetic Loss: {magnetic_train_loss:7.4f} \"\n",
    "                  f\"Topological Loss: {topological_train_loss:7.4f} \"\n",
    "                  f\"Valid Magnetic: {magnetic_valid_loss:7.4f} \"\n",
    "                  f\"Valid Topological: {topological_valid_loss:7.4f} \"\n",
    "                  f\"Time: {time.time() - start_time:.4f}\")\n",
    "        \n",
    "        # Save model if validation loss improves\n",
    "        valid_loss = magnetic_valid_loss + topological_valid_loss\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(\n",
    "                valid_loss_min, valid_loss))\n",
    "            run_name = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "            torch.save(model.state_dict(), run_name + 'multi_task_model.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symmetry_indicators(structure):\n",
    "    \"\"\"\n",
    "    Extract symmetry indicators relevant for topological classification\n",
    "    Based on Topological Quantum Chemistry principles\n",
    "    \"\"\"\n",
    "    indicators = {}\n",
    "    \n",
    "    # Get space group information\n",
    "    analyzer = SpacegroupAnalyzer(structure)\n",
    "    spacegroup_number = analyzer.get_space_group_number()\n",
    "    point_group = analyzer.get_point_group_symbol()\n",
    "    \n",
    "    indicators['spacegroup_number'] = spacegroup_number\n",
    "    \n",
    "    # Check for inversion symmetry (important for many topological materials)\n",
    "    indicators['has_inversion'] = 1 if analyzer.has_inversion() else 0\n",
    "    \n",
    "    # Time-reversal symmetry is crucial for topological classification\n",
    "    # In a real implementation, this would be more sophisticated\n",
    "    magnetic_elements = ['Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Gd', 'Dy', 'Ho', 'Er', 'Tm', 'Yb']\n",
    "    has_magnetic_elements = any(element in str(structure.composition) for element in magnetic_elements)\n",
    "    indicators['potential_time_reversal_breaking'] = 1 if has_magnetic_elements else 0\n",
    "    \n",
    "    # BCS symmetry indicators based on BCS 3.7 from the TQC database\n",
    "    # This is a simplified implementation\n",
    "    if spacegroup_number in [2, 10, 47, 83, 87, 199, 216, 227]:  # These are examples\n",
    "        indicators['compatible_with_bcs_3_7'] = 1\n",
    "    else:\n",
    "        indicators['compatible_with_bcs_3_7'] = 0\n",
    "    \n",
    "    # Add indicators for nonsymmorphic symmetries\n",
    "    indicators['has_nonsymmorphic'] = 1 if analyzer.is_nonsymmorphic() else 0\n",
    "    \n",
    "    # Add band connectivity indicators\n",
    "    # In real implementation, this would require electronic structure calculation\n",
    "    indicators['estimated_band_inversion'] = 0  # Placeholder\n",
    "    \n",
    "    return indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bcs_compatibility(structure, bcs_id=\"3.7\"):\n",
    "    \"\"\"\n",
    "    Check if a structure is compatible with a specific BCS classification\n",
    "    from the Topological Quantum Chemistry database\n",
    "    \"\"\"\n",
    "    # This would typically require calling the TQC API or using their methods\n",
    "    # For this example, we'll use a simplified approach\n",
    "    \n",
    "    analyzer = SpacegroupAnalyzer(structure)\n",
    "    spacegroup = analyzer.get_space_group_number()\n",
    "    \n",
    "    # BCS 3.7 compatibility rules (simplified)\n",
    "    # In reality, this would involve more detailed symmetry analysis\n",
    "    if bcs_id == \"3.7\":\n",
    "        # Example conditions for BCS 3.7 (these would be replaced with actual conditions)\n",
    "        if spacegroup in [2, 10, 47, 83, 87, 199, 216, 227]:\n",
    "            # Check additional conditions like orbital character, band inversion, etc.\n",
    "            composition = structure.composition\n",
    "            \n",
    "            # Check for elements commonly found in TIs with this BCS\n",
    "            has_heavy_elements = any(element in str(composition) for element in ['Bi', 'Sb', 'Te', 'Se'])\n",
    "            \n",
    "            # Check for inversion symmetry (important for many TIs)\n",
    "            has_inversion = analyzer.has_inversion()\n",
    "            \n",
    "            return has_heavy_elements and has_inversion\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change this lol\n",
    "\n",
    "def predict_topological_class(struct, symmetry_indicators, is_bcs_compatible):\n",
    "    if not is_bcs_compatible:\n",
    "        return \"None\"\n",
    "    if symmetry_indicators.get(\"z4\", 0) == 1:\n",
    "        return \"Strong TI\"\n",
    "    return \"Weak TI\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mendeleev import element\n",
    "def get_en_pauling(symbol):\n",
    "    elem = element(str(symbol))\n",
    "    return elem.electronegativity('pauling')\n",
    "\n",
    "#print(get_en_pauling('O'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pymatgen.core import Element\n",
    "from pymatgen.analysis.magnetism.analyzer import CollinearMagneticStructureAnalyzer\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Dummy helper functions and dictionaries for demonstration.\n",
    "def get_en_pauling(symbol):\n",
    "    en_dict = {'Fe': 1.83, 'O': 3.44, 'Ru': 2.2, 'Rh': 2.28}  # Extend as needed\n",
    "    return en_dict.get(symbol, 0.0)\n",
    "\n",
    "def extract_magnetic_features(struct):\n",
    "    return {'dummy_magnetic': 1.0}\n",
    "\n",
    "def extract_symmetry_indicators(struct):\n",
    "    return {'dummy_symmetry': 1.0}\n",
    "\n",
    "def check_bcs_compatibility(struct, bcs_id):\n",
    "    return True\n",
    "\n",
    "def predict_topological_class(struct, symmetry_indicators, is_bcs_compatible):\n",
    "    return 'TI'\n",
    "\n",
    "order_encode = {'NM': 0, 'AFM': 1, 'FM': 2}  # Adjust as needed\n",
    "topo_encode = {'None': 0, 'TI': 1}            # Adjust as needed\n",
    "params = {'max_radius': 10.0}\n",
    "n_norm = 35\n",
    "\n",
    "\n",
    "class DataPeriodicNeighbors(Data):\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'edge_index':\n",
    "            # Make sure edge indices are incremented by number of nodes during batching\n",
    "            return self.num_nodes\n",
    "        if key == 'cell_index':\n",
    "            # If you use periodic image information per atom\n",
    "            return self.num_nodes\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "    def __init__(self, x=None, pos=None, lattice=None, edge_index=None, r_max=None, \n",
    "                 magnetic_y=None, topological_y=None, magnetic_features=None, \n",
    "                 symmetry_features=None, bcs_compatible=None, n_norm=None, **kwargs):\n",
    "        super().__init__()            \n",
    "        self.x = x\n",
    "        self.pos = pos\n",
    "        self.lattice = lattice\n",
    "        self.edge_index = edge_index\n",
    "        self.r_max = r_max\n",
    "        self.magnetic_y = magnetic_y\n",
    "        self.topological_y = topological_y\n",
    "        self.magnetic_features = magnetic_features\n",
    "        self.symmetry_features = symmetry_features\n",
    "        self.bcs_compatible = bcs_compatible\n",
    "        self.n_norm = n_norm\n",
    "        \n",
    "        # Handle any additional kwargs that might be passed during batch creation\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def preprocess_structures_with_tqc(structures, bcs_id=\"3.7\"):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing pipeline incorporating TQC data and BCS classification.\n",
    "    Returns a list of DataPeriodicNeighbors objects.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    len_element = 100  # Maximum number of element indices we support (e.g., Z < 100)\n",
    "    \n",
    "    for i, struct in enumerate(structures):\n",
    "        print(f\"Processing structure {i+1}/{len(structures)}\", end=\"\\r\", flush=True)\n",
    "        try:\n",
    "            num_sites = len(struct)\n",
    "            # Allocate features: 3 properties per element index\n",
    "            input_features = torch.zeros(num_sites, 3 * len_element)\n",
    "            \n",
    "            for j, site in enumerate(struct):\n",
    "                elem = str(site.specie)\n",
    "                atomic_num = Element(elem).Z\n",
    "                # Clip atomic number if it exceeds our fixed size\n",
    "                if atomic_num >= len_element:\n",
    "                    atomic_num = len_element - 1\n",
    "                \n",
    "                # Retrieve properties with defaults if not available.\n",
    "                atomic_radius = getattr(Element(elem), 'atomic_radius', 0.0) or 0.0\n",
    "                en_pauling = get_en_pauling(elem)\n",
    "                if en_pauling is None:\n",
    "                    en_pauling = 0.0\n",
    "                dipole_polarizability = getattr(Element(elem), 'dipole_polarizability', 0.0) or 0.0\n",
    "                \n",
    "                # Place properties in the feature tensor at positions based on atomic_num\n",
    "                input_features[j, atomic_num] = atomic_radius\n",
    "                input_features[j, len_element + atomic_num] = en_pauling\n",
    "                input_features[j, 2 * len_element + atomic_num] = dipole_polarizability\n",
    "\n",
    "                # Get atom positions\n",
    "            positions = torch.tensor(struct.cart_coords, dtype=torch.float)\n",
    "            \n",
    "            # Create edges based on distance cutoff - consider periodic boundaries\n",
    "            # This is simplified - you may need to use a library like PyMatGen for proper periodic distances\n",
    "            src_list = []\n",
    "            dst_list = []\n",
    "            edge_attr_list = []\n",
    "\n",
    "            for src_idx in range(num_sites):\n",
    "                for dst_idx in range(num_sites):\n",
    "                    # Skip self-loops if you don't want them\n",
    "                    if src_idx == dst_idx:\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate distance (simplified - doesn't account for periodicity)\n",
    "                    dist = torch.norm(positions[src_idx] - positions[dst_idx])\n",
    "                    \n",
    "                    if dist <= params['max_radius']:\n",
    "                        src_list.append(src_idx)\n",
    "                        dst_list.append(dst_idx)\n",
    "                        # Add distance or other edge features\n",
    "                        edge_attr_list.append([dist.item(), 0, 0])  # Example: [distance, 0, 0]\n",
    "\n",
    "            if src_list:  # If there are edges\n",
    "                edge_index = torch.tensor([src_list, dst_list], dtype=torch.long)\n",
    "                edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "            else:\n",
    "                # Fallback to self-loops if no edges found\n",
    "                edge_index = torch.stack([torch.arange(num_sites), torch.arange(num_sites)], dim=0)\n",
    "                edge_attr = torch.zeros((num_sites, 3), dtype=torch.float)\n",
    "        \n",
    "        \n",
    "            \n",
    "            # Create a self-loop edge_index: each node connected to itself.\n",
    "            # This yields an edge_index of shape [2, num_sites] with indices 0...num_sites-1.\n",
    "            #edge_index = torch.stack([torch.arange(num_sites), torch.arange(num_sites)], dim=0)\n",
    "            \n",
    "            # Extract additional features\n",
    "            magnetic_feats = extract_magnetic_features(struct)\n",
    "            symmetry_indicators = extract_symmetry_indicators(struct)\n",
    "            is_bcs_compatible = check_bcs_compatibility(struct, bcs_id=bcs_id)\n",
    "            \n",
    "            # Get magnetic ordering using pymatgen analyzer\n",
    "            analyzer = CollinearMagneticStructureAnalyzer(struct)\n",
    "            magnetic_order = analyzer.ordering.name  # e.g. 'AFM', 'NM', 'FM'\n",
    "            \n",
    "            # Predict topological class\n",
    "            topo_class = predict_topological_class(struct, symmetry_indicators, is_bcs_compatible)\n",
    "            \n",
    "            # Create the DataPeriodicNeighbors object with a valid edge_index\n",
    "            data_point = DataPeriodicNeighbors(\n",
    "                x=input_features,\n",
    "                pos=torch.tensor(struct.cart_coords, dtype=torch.float),\n",
    "                lattice=torch.tensor(struct.lattice.matrix, dtype=torch.float),\n",
    "                edge_index=edge_index,\n",
    "                edge_attr = edge_attr,\n",
    "                r_max=params['max_radius'],\n",
    "                magnetic_y=torch.tensor([order_encode[magnetic_order]], dtype=torch.long),\n",
    "                topological_y=torch.tensor([topo_encode[topo_class]], dtype=torch.long),\n",
    "                magnetic_features=torch.tensor(list(magnetic_feats.values()), dtype=torch.float),\n",
    "                symmetry_features=torch.tensor(list(symmetry_indicators.values()), dtype=torch.float),\n",
    "                bcs_compatible=torch.tensor([int(is_bcs_compatible)], dtype=torch.float),\n",
    "                n_norm=n_norm,\n",
    "            )\n",
    "            \n",
    "            processed_data.append(data_point)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing structure {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nProcessed {len(processed_data)} structures successfully.\")\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_magnetic_space_group(structure):\n",
    "    \"\"\"\n",
    "    Analyze the magnetic space group of a structure\n",
    "    Important for classifying magnetic topological materials\n",
    "    \"\"\"\n",
    "    # This is a placeholder for more sophisticated analysis\n",
    "    # In practice, you would use a library like ISOTROPY or Bilbao Crystallographic Server\n",
    "    \n",
    "    # Get standard space group\n",
    "    analyzer = SpacegroupAnalyzer(structure)\n",
    "    space_group = analyzer.get_space_group_number()\n",
    "    \n",
    "    # Analyze magnetic ordering\n",
    "    mag_analyzer = CollinearMagneticStructureAnalyzer(structure)\n",
    "    ordering = mag_analyzer.ordering.name\n",
    "    \n",
    "    # Simplified magnetic space group determination\n",
    "    # In reality, this requires detailed analysis of symmetry and magnetic moments\n",
    "    if ordering == \"NM\":\n",
    "        # Non-magnetic: equivalent to standard space group\n",
    "        mag_space_group = f\"{space_group}.0\"\n",
    "    elif ordering == \"FM\":\n",
    "        # Ferromagnetic: typically type III or IV MSG\n",
    "        if analyzer.has_inversion():\n",
    "            mag_space_group = f\"{space_group}.10\"  # Example type IV\n",
    "        else:\n",
    "            mag_space_group = f\"{space_group}.8\"   # Example type III\n",
    "    elif ordering == \"AFM\":\n",
    "        # Antiferromagnetic: typically type II or III MSG\n",
    "        if space_group % 2 == 0:  # Even space groups often become type II\n",
    "            mag_space_group = f\"{space_group}.7\"   # Example type II\n",
    "        else:\n",
    "            mag_space_group = f\"{space_group}.9\"   # Example type III\n",
    "    elif ordering in [\"FM\", \"FiM\"]:\n",
    "        # Ferromagnetic or ferrimagnetic: typically type III or IV MSG\n",
    "        if analyzer.has_inversion():\n",
    "            mag_space_group = f\"{space_group}.10\"  # Example type IV\n",
    "        else:\n",
    "            mag_space_group = f\"{space_group}.8\"   # Example type III\n",
    "    else:\n",
    "        mag_space_group = \"unknown\"\n",
    "    \n",
    "    # For BCS 3.7 compatibility\n",
    "    compatible_with_bcs37 = False\n",
    "    if mag_space_group in [\"2.4\", \"10.42\", \"47.252\", \"83.43\", \"87.78\", \"199.13\", \"216.77\", \"227.131\"]:\n",
    "        compatible_with_bcs37 = True\n",
    "    \n",
    "    return {\n",
    "        \"magnetic_space_group\": mag_space_group,\n",
    "        \"compatible_with_bcs37\": compatible_with_bcs37\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagneticTopologicalTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, edge_attr_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.attention_layers = nn.ModuleList([\n",
    "            GraphMultiHeadAttention(hidden_dim, num_heads, edge_attr_dim)\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        self.ffn_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "            ) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        self.layer_norms1 = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim) for _ in range(3)\n",
    "        ])\n",
    "        self.layer_norms2 = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_dim) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        self.magnetic_head = nn.Linear(hidden_dim, 3)      # NM, AFM, FM/FiM\n",
    "        self.topological_head = nn.Linear(hidden_dim, 2)   # Not TI, TI\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for i in range(3):\n",
    "            attention_output = self.attention_layers[i](x, edge_index, edge_attr)\n",
    "            x = self.layer_norms1[i](x + attention_output)\n",
    "            ffn_output = self.ffn_layers[i](x)\n",
    "            x = self.layer_norms2[i](x + ffn_output)\n",
    "        \n",
    "        x = torch_scatter.scatter_mean(x, batch, dim=0)\n",
    "        \n",
    "        magnetic_pred = self.magnetic_head(x)\n",
    "        topological_pred = self.topological_head(x)\n",
    "        \n",
    "        return magnetic_pred, torch.sigmoid(topological_pred)\n",
    "\n",
    "\n",
    "class GraphMultiHeadAttention(MessagePassing):\n",
    "    def __init__(self, hidden_dim, num_heads, edge_attr_dim):\n",
    "        super().__init__(aggr='add')\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.edge_proj = nn.Linear(edge_attr_dim, num_heads)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    # def forward(self, x, edge_index, edge_attr):\n",
    "    #     # Project inputs\n",
    "    #     q = self.q_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "    #     k = self.k_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "    #     v = self.v_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "        \n",
    "    #     # Process edge attributes\n",
    "    #     edge_weights = self.edge_proj(edge_attr).unsqueeze(-1)  # [E, num_heads, 1]\n",
    "        \n",
    "    #     # Propagate through edges\n",
    "    #     out = self.propagate(edge_index, q=q, k=k, v=v, edge_weights=edge_weights)\n",
    "        \n",
    "    #     # Project output\n",
    "    #     return self.output_proj(out.view(-1, self.hidden_dim))\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # If edge_index is None, create a self-loop edge index for each node.\n",
    "        if edge_index is None:\n",
    "            N = x.size(0)\n",
    "            # Create self-loops: each node connected to itself.\n",
    "            edge_index = torch.stack([torch.arange(N, device=x.device),\n",
    "                                    torch.arange(N, device=x.device)], dim=0)\n",
    "        \n",
    "        # If edge_attr is None, create a default tensor with zeros.\n",
    "        if edge_attr is None:\n",
    "            E = edge_index.size(1)  # number of edges\n",
    "            edge_attr = torch.zeros(E, self.edge_proj.in_features, device=x.device)\n",
    "        \n",
    "        # Project inputs\n",
    "        q = self.q_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(-1, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Process edge attributes\n",
    "        edge_weights = self.edge_proj(edge_attr).unsqueeze(-1)  # [E, num_heads, 1]\n",
    "        \n",
    "        # Propagate through edges\n",
    "        out = self.propagate(edge_index, q=q, k=k, v=v, edge_weights=edge_weights)\n",
    "        \n",
    "        # Project output\n",
    "        return self.output_proj(out.view(-1, self.hidden_dim))\n",
    "\n",
    "\n",
    "    \n",
    "    def message(self, q_i, k_j, v_j, edge_weights):\n",
    "        attention = (q_i * k_j).sum(dim=-1) / math.sqrt(self.head_dim)  # [E, num_heads]\n",
    "        attention = attention.unsqueeze(-1) * edge_weights             # Apply edge weighting\n",
    "        attention = F.softmax(attention, dim=0)                        # Normalize over neighbors\n",
    "        return attention * v_j                                         # [E, num_heads, head_dim]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCS37Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Specialized classifier for BCS 3.7 magnetic topological materials\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Symmetry analysis module\n",
    "        self.symmetry_module = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        # Final classification heads\n",
    "        self.magnetic_head = nn.Linear(16, 3)  # NM, AFM, FM/FiM\n",
    "        self.bcs_compatibility_head = nn.Linear(16, 1)  # Sigmoid will be applied\n",
    "        \n",
    "    def forward(self, x, structure_features):\n",
    "        # Extract features\n",
    "        features = self.feature_extraction(x)\n",
    "        \n",
    "        # Add structure-level features like symmetry indicators\n",
    "        combined_features = features + structure_features\n",
    "        \n",
    "        # Analyze symmetry\n",
    "        symmetry_features = self.symmetry_module(combined_features)\n",
    "        \n",
    "        # Get predictions\n",
    "        magnetic_pred = self.magnetic_head(symmetry_features)\n",
    "        bcs_compatibility = torch.sigmoid(self.bcs_compatibility_head(symmetry_features))\n",
    "        \n",
    "        return magnetic_pred, bcs_compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#TODO: change this to somenthing more suitable lol\n",
    "\n",
    "def compute_loss(magnetic_pred, topological_pred, batch):\n",
    "    \"\"\"\n",
    "    Compute the loss for both magnetic and topological predictions.\n",
    "\n",
    "    Arguments:\n",
    "    - magnetic_pred: Predictions for the magnetic ordering (output from the magnetic head)\n",
    "    - topological_pred: Predictions for the topological class (output from the topological head)\n",
    "    - batch: Batch of data containing the true labels for magnetic and topological classes\n",
    "\n",
    "    Returns:\n",
    "    - loss: Total loss (sum of magnetic and topological losses)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Magnetic prediction loss (Cross-Entropy Loss)\n",
    "    magnetic_true = batch.magnetic_y\n",
    "    magnetic_loss = F.cross_entropy(magnetic_pred, magnetic_true)\n",
    "    \n",
    "    # Topological prediction loss (Cross-Entropy Loss)\n",
    "    topological_true = batch.topological_y\n",
    "    topological_loss = F.cross_entropy(topological_pred, topological_true)\n",
    "    \n",
    "    # Total loss is the sum of both losses\n",
    "    total_loss = magnetic_loss + topological_loss\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.44\n"
     ]
    }
   ],
   "source": [
    "# from mendeleev import element\n",
    "\n",
    "# #TODO: think of this logic more\n",
    "\n",
    "# def get_en_pauling(symbol):\n",
    "#     try:\n",
    "#         elem = element(symbol)\n",
    "#         return elem.electronegativityl\n",
    "#     except KeyError:\n",
    "#         return None\n",
    "\n",
    "# symbol = 'Fe'  # Example element\n",
    "# en_pauling = get_en_pauling(symbol)\n",
    "# print(en_pauling)  # Should print the electronegativity value or None if not found\n",
    "\n",
    "\n",
    "from mendeleev import element\n",
    "# Access the element you want (e.g., Oxygen)\n",
    "# element = element('O')\n",
    "\n",
    "# # Get Paulling electronegativity\n",
    "# pauling_electronegativity = element.electronegativity('pauling')\n",
    "# print(f\"Pauling electronegativity of Oxygen: {pauling_electronegativity}\")\n",
    "\n",
    "# # Get Mulliken electronegativity\n",
    "# mulliken_electronegativity = element.electronegativity('mulliken')\n",
    "# print(f\"Mulliken electronegativity of Oxygen: {mulliken_electronegativity}\")\n",
    "\n",
    "def get_en_pauling(symbol):\n",
    "    elem = element(str(symbol))\n",
    "    return elem.electronegativity('pauling')\n",
    "\n",
    "print(get_en_pauling('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing structure 1/9\n",
      "Error processing structure 0: 'FiM'\n",
      "Processing structure 6/9\n",
      "Error processing structure 5: 'FiM'\n",
      "Processing structure 9/9\n",
      "Processed 7 structures successfully.\n",
      "Length of enhanced_data: 7\n",
      "[<class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>, <class '__main__.DataPeriodicNeighbors'>]\n",
      "Graph 0: x.shape = torch.Size([4, 300]), num_nodes = 4\n",
      "Graph 1: x.shape = torch.Size([1, 300]), num_nodes = 1\n",
      "Graph 2: x.shape = torch.Size([3, 300]), num_nodes = 3\n",
      "Graph 3: x.shape = torch.Size([2, 300]), num_nodes = 2\n",
      "Graph 4: x.shape = torch.Size([2, 300]), num_nodes = 2\n",
      "Graph 5: x.shape = torch.Size([1, 300]), num_nodes = 1\n",
      "Graph 6: x.shape = torch.Size([4, 300]), num_nodes = 4\n",
      "Batched x shape: torch.Size([11, 300])\n",
      "Batched edge_index max: tensor(10)\n",
      "Batched num_nodes: 11\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Found indices in 'edge_index' that are larger than 3 (got 7). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 4) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:271\u001b[39m, in \u001b[36mMessagePassing._index_select_safe\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mRuntimeError\u001b[39m: INDICES element is out of DATA bounds, id=4 axis_dim=4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mtrain_mag_topo_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mtrain_mag_topo_model\u001b[39m\u001b[34m(model, optimizer, scheduler, dataloader, dataloader_valid, max_epochs, device)\u001b[39m\n\u001b[32m     76\u001b[39m batch = batch.to(device)  \u001b[38;5;66;03m# Ensure the batch is on the correct device\u001b[39;00m\n\u001b[32m     77\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m magnetic_pred, topological_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Define loss functions and backpropagate here\u001b[39;00m\n\u001b[32m     80\u001b[39m loss = compute_loss(magnetic_pred, topological_pred, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mMagneticTopologicalTransformer.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr, batch)\u001b[39m\n\u001b[32m     30\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layer_norms1[i](x + attention_output)\n\u001b[32m     35\u001b[39m     ffn_output = \u001b[38;5;28mself\u001b[39m.ffn_layers[i](x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mGraphMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, edge_index, edge_attr)\u001b[39m\n\u001b[32m     92\u001b[39m edge_weights = \u001b[38;5;28mself\u001b[39m.edge_proj(edge_attr).unsqueeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [E, num_heads, 1]\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Propagate through edges\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43medge_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Project output\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_proj(out.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.hidden_dim))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:514\u001b[39m, in \u001b[36mMessagePassing.propagate\u001b[39m\u001b[34m(self, edge_index, size, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[32m    512\u001b[39m         kwargs[arg] = decomp_kwargs[arg][i]\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m coll_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mmutable_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m msg_kwargs = \u001b[38;5;28mself\u001b[39m.inspector.collect_param_data(\n\u001b[32m    518\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m, coll_dict)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._message_forward_pre_hooks.values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:361\u001b[39m, in \u001b[36mMessagePassing._collect\u001b[39m\u001b[34m(self, args, edge_index, size, kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[32m    360\u001b[39m             \u001b[38;5;28mself\u001b[39m._set_size(size, dim, data)\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m             data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m         out[arg] = data\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:321\u001b[39m, in \u001b[36mMessagePassing._lift\u001b[39m\u001b[34m(self, src, edge_index, dim)\u001b[39m\n\u001b[32m    319\u001b[39m         index = edge_index[dim]\n\u001b[32m    320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m src.index_select(\u001b[38;5;28mself\u001b[39m.node_dim, index)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[32m    324\u001b[39m     row, col, _ = edge_index.coo()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:267\u001b[39m, in \u001b[36mMessagePassing._index_select\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m src.index_select(\u001b[38;5;28mself\u001b[39m.node_dim, index)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_index_select_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:282\u001b[39m, in \u001b[36mMessagePassing._index_select_safe\u001b[39m\u001b[34m(self, src, index)\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[32m    275\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound negative indices in \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    276\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex.min().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Please ensure that all \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    277\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindices in \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m\u001b[33m point to valid indices \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.size(\u001b[38;5;28mself\u001b[39m.node_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    279\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour node feature matrix and try again.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (index.numel() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index.max() >= src.size(\u001b[38;5;28mself\u001b[39m.node_dim)):\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[32m    283\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound indices in \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m\u001b[33m that are larger \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.size(\u001b[38;5;28mself\u001b[39m.node_dim)\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (got \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex.max().item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Please ensure that all \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mindices in \u001b[39m\u001b[33m'\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m'\u001b[39m\u001b[33m point to valid indices \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    287\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc.size(\u001b[38;5;28mself\u001b[39m.node_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour node feature matrix and try again.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mIndexError\u001b[39m: Found indices in 'edge_index' that are larger than 3 (got 7). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 4) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the device (use CUDA if available, otherwise use CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define atom_types_dim based on your feature encoding\n",
    "# For example, if you use atomic radius, electronegativity, and dipole moment as features for each atom type\n",
    "atom_types_dim = 300  # For this case, you would need 3 features per atom type\n",
    "\n",
    "# Initialize the model\n",
    "model = MagneticTopologicalTransformer(\n",
    "    input_dim=atom_types_dim, \n",
    "    hidden_dim=128,\n",
    "    num_heads=4, \n",
    "    edge_attr_dim= 3\n",
    ").to(device)  # Make sure to move the model to the correct device\n",
    "\n",
    "# Preprocess data with TQC insights\n",
    "enhanced_data = preprocess_structures_with_tqc(structures_list_mp, bcs_id=\"3.7\")\n",
    "\n",
    "# Split data\n",
    "indices = np.arange(len(enhanced_data))\n",
    "np.random.shuffle(indices)\n",
    "index_tr, index_va, index_te = np.split(indices, [int(.8 * len(indices)), int(.9 * len(indices))])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4 # Increased batch size for transformer\n",
    "print(f\"Length of enhanced_data: {len(enhanced_data)}\")\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "print([type(g) for g in enhanced_data])\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     [enhanced_data[i] for i in index_tr], \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=True\n",
    "# )\n",
    "# dataloader_valid = DataLoader(\n",
    "#     [enhanced_data[i] for i in index_va], \n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# Create DataLoader instances using the batched data\n",
    "dataloader = DataLoader(enhanced_data, batch_size=4, shuffle=True)\n",
    "dataloader_valid = DataLoader(enhanced_data, batch_size=4)\n",
    "\n",
    "# Initialize optimizer with learning rate warmup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler (Cosine annealing with warmup)\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer, \n",
    "    T_0=100,  # First restart period\n",
    "    T_mult=2,  # Multiply restart period after each cycle\n",
    "    eta_min=0,  # Minimum learning rate\n",
    "    last_epoch=-1  # Start from epoch 0\n",
    ")\n",
    "\n",
    "# Training function placeholder (assuming you have defined it previously)\n",
    "def train_mag_topo_model(model, optimizer, scheduler, dataloader, dataloader_valid, max_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(max_epochs):\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            # Debug info: print shapes and check edge_index validity\n",
    "            print(\"Batch x.shape:\", batch.x.shape)\n",
    "            print(\"Batch edge_index.shape:\", batch.edge_index.shape)\n",
    "            print(\"Batch edge_index max:\", batch.edge_index.max().item())\n",
    "            print(\"Batch node assignment shape:\", batch.batch.shape)\n",
    "            # Ensure edge indices are within [0, total_nodes)\n",
    "            assert batch.edge_index.max().item() < batch.x.shape[0], (\n",
    "                f\"edge_index out of bounds! Max index: {batch.edge_index.max().item()}, \"\n",
    "                f\"num_nodes: {batch.x.shape[0]}\"\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            magnetic_pred, topological_pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            loss = compute_loss(magnetic_pred, topological_pred, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Optionally, evaluate on the validation set here.\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} completed.\")\n",
    "        \n",
    "# def train_mag_topo_model(model, optimizer, scheduler, dataloader, dataloader_valid, max_epochs, device):\n",
    "#     model.train()\n",
    "#     for epoch in range(max_epochs):\n",
    "#         for batch in dataloader:\n",
    "            \n",
    "#             nx_graph = to_networkx(batch, to_undirected=True)\n",
    "#             plt.figure(figsize=(8, 6))\n",
    "#             nx.draw(nx_graph, with_labels=True, node_size=300, font_size=10)\n",
    "#             plt.title(\"Batch Graph Visualization\")\n",
    "#             plt.show()\n",
    "\n",
    "#             batch = batch.to(device)  # Ensure the batch is on the correct device\n",
    "#             optimizer.zero_grad()\n",
    "#             magnetic_pred, topological_pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "#             # Define loss functions and backpropagate here\n",
    "#             loss = compute_loss(magnetic_pred, topological_pred, batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for batch in dataloader_valid:\n",
    "             \n",
    "               \n",
    "#                 batch = batch.to(device)  # Ensure validation batch is on the correct device\n",
    "#                 magnetic_pred, topological_pred = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "#                 # Compute validation loss or any evaluation metric here\n",
    "#                 val_loss = compute_loss(magnetic_pred, topological_pred, batch)\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "\n",
    "for i, data in enumerate(enhanced_data):\n",
    "    assert isinstance(data, DataPeriodicNeighbors)\n",
    "    # print(f\"Graph {i}:\")\n",
    "    # print(f\"  num_nodes: {data.num_nodes}\")\n",
    "    # print(f\"  edge_index max: {data.edge_index.max().item()}\")\n",
    "    # print(f\"  edge_index shape: {data.edge_index.shape}\")\n",
    "\n",
    "for i, g in enumerate(enhanced_data):\n",
    "    print(f\"Graph {i}: x.shape = {g.x.shape}, num_nodes = {g.num_nodes}\")\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(\"Batched x shape:\", batch.x.shape)\n",
    "    print(\"Batched edge_index max:\", batch.edge_index.max())\n",
    "    print(\"Batched num_nodes:\", batch.num_nodes)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_mag_topo_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    dataloader, \n",
    "    dataloader_valid, \n",
    "    max_epochs=1, \n",
    "    device=device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the model\n",
    "# atom_types_dim = 3\n",
    "# model = MagneticTopologicalTransformer(\n",
    "#     input_dim=atom_types_dim, \n",
    "#     hidden_dim=128,\n",
    "#     num_heads=4\n",
    "# )\n",
    "\n",
    "# # Preprocess data with TQC insights\n",
    "# enhanced_data = preprocess_structures_with_tqc(structures_list_mp, bcs_id=\"3.7\")\n",
    "\n",
    "# # Split data\n",
    "# indices = np.arange(len(enhanced_data))\n",
    "# np.random.shuffle(indices)\n",
    "# index_tr, index_va, index_te = np.split(indices, [int(.8 * len(indices)), int(.9 * len(indices))])\n",
    "\n",
    "# # Create dataloaders\n",
    "# batch_size = 16  # Increased batch size for transformer\n",
    "# dataloader = torch_geometric.data.DataLoader(\n",
    "#     [enhanced_data[i] for i in index_tr], \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=True\n",
    "# )\n",
    "# dataloader_valid = torch_geometric.data.DataLoader(\n",
    "#     [enhanced_data[i] for i in index_va], \n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "\n",
    "# # Initialize optimizer with learning rate warmup\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "# scheduler = get_cosine_schedule_with_warmup(\n",
    "#     optimizer, \n",
    "#     num_warmup_steps=100, \n",
    "#     num_training_steps=100*len(dataloader)\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# train_mag_topo_model(\n",
    "#     model, \n",
    "#     optimizer, \n",
    "#     scheduler,\n",
    "#     dataloader, \n",
    "#     dataloader_valid, \n",
    "#     max_epochs=100, \n",
    "#     device= 'cpu'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
