{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "import logging\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/cdvae')\n",
    "import cdvae\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CDVAE_TI_Generator:\n",
    "    \"\"\"\n",
    "    Crystal Diffusion Variational Autoencoder (CDVAE) with Reinforcement Learning\n",
    "    for targeted generation of Topological Insulator materials.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.float32 #set default tensor type to float32\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize CDVAE model components\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Set up optimizers\n",
    "        self.setup_optimizers()\n",
    "        \n",
    "        # Initialize results tracking\n",
    "        self.results = {\n",
    "            'rewards': [],\n",
    "            'z_gap': [],\n",
    "            'topological_indices': [],\n",
    "            'formation_energies': [],\n",
    "            'best_structures': [],\n",
    "            'best_rewards': [],\n",
    "        }\n",
    "        \n",
    "        # Initialize replay buffer for experience replay\n",
    "        self.replay_buffer = ReplayBuffer(config['buffer_size'])\n",
    "        \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize CDVAE encoder, decoder and policy networks.\"\"\"\n",
    "        # Import specific model classes\n",
    "        try:\n",
    "            from cdvae.pl_modules.decoder import GemNetTDecoder\n",
    "            from cdvae.common.data_utils import StandardScalerTorch\n",
    "            from cdvae.common.data_utils import ATOM_TYPES\n",
    "     \n",
    "        except ImportError:\n",
    "            logger.error(\"Failed to import CDVAE modules. Please ensure CDVAE is installed correctly.\")\n",
    "            raise\n",
    "            \n",
    "        # Get dimensions and parameters from config\n",
    "        self.latent_dim = self.config['latent_dim']\n",
    "        self.n_elements = len(self.config['elements']) if 'elements' in self.config else len(ATOM_TYPES.get())\n",
    "        \n",
    "        #TODO: possibly write an encoder file addition to cdvae\n",
    "        # Initialize encoder (if using pre-trained weights)\n",
    "        # if self.config.get('use_encoder', False):\n",
    "        #     self.encoder = GraphEncoder(\n",
    "        #         hidden_dim=self.config['hidden_dim'],\n",
    "        #         latent_dim=self.latent_dim,\n",
    "        #         use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        #     ).to(self.device)\n",
    "            \n",
    "        #     if self.config.get('encoder_checkpoint'):\n",
    "        #         self._load_model(self.encoder, self.config['encoder_checkpoint'])\n",
    "        # else:\n",
    "        #     self.encoder = None\n",
    "\n",
    "        self.encoder = None\n",
    "            \n",
    "        # Initialize decoder\n",
    "        self.decoder = GemNetTDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            #cutoff=self.config.get('cutoff', 6.0),\n",
    "            max_neighbors=self.config.get('max_neighbors', 20),\n",
    "            #use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('decoder_checkpoint'):\n",
    "            self._load_model(self.decoder, self.config['decoder_checkpoint'])\n",
    "            \n",
    "        # Initialize policy network for RL\n",
    "        self.policy_net = PolicyNetwork(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('policy_hidden_dims', [256, 256]),\n",
    "            activation=self.config.get('policy_activation', 'relu')\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize critic network for actor-critic methods\n",
    "        if self.config.get('use_critic', True):\n",
    "            self.critic = CriticNetwork(\n",
    "                latent_dim=self.latent_dim,\n",
    "                hidden_dims=self.config.get('critic_hidden_dims', [256, 128]),\n",
    "                activation=self.config.get('critic_activation', 'relu')\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.critic = None\n",
    "            \n",
    "        # DFT surrogate models - predict quantum properties directly from latent space\n",
    "        self.energy_predictor = EnergyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('energy_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.topological_predictor = TopologicalPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('topo_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('surrogate_checkpoint'):\n",
    "            self._load_surrogate_models(self.config['surrogate_checkpoint'])\n",
    "        \n",
    "    def setup_optimizers(self):\n",
    "        \"\"\"Set up optimizers for different components.\"\"\"\n",
    "        # Policy optimizer\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config.get('policy_lr', 1e-4),\n",
    "            weight_decay=self.config.get('policy_weight_decay', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Critic optimizer (if using actor-critic)\n",
    "        if self.critic is not None:\n",
    "            self.critic_optimizer = torch.optim.Adam(\n",
    "                self.critic.parameters(),\n",
    "                lr=self.config.get('critic_lr', 3e-4),\n",
    "                weight_decay=self.config.get('critic_weight_decay', 1e-6)\n",
    "            )\n",
    "        \n",
    "        # Surrogate model optimizers for fine-tuning\n",
    "        if self.config.get('train_surrogates', False):\n",
    "            self.energy_optimizer = torch.optim.Adam(\n",
    "                self.energy_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "            self.topo_optimizer = torch.optim.Adam(\n",
    "                self.topological_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "    def _load_model(self, model, checkpoint_path):\n",
    "        \"\"\"Load model weights from checkpoint.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            if 'state_dict' in checkpoint:\n",
    "                # Handle pytorch-lightning checkpoints\n",
    "                state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items() \n",
    "                              if k.startswith('model.')}\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "            else:\n",
    "                # Handle regular torch checkpoints\n",
    "                model.load_state_dict(checkpoint, strict=False)\n",
    "            logger.info(f\"Loaded weights from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load weights: {e}\")\n",
    "            \n",
    "    def _load_surrogate_models(self, checkpoint_path):\n",
    "        \"\"\"Load surrogate model weights.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            self.energy_predictor.load_state_dict(checkpoint['energy_predictor'])\n",
    "            self.topological_predictor.load_state_dict(checkpoint['topo_predictor'])\n",
    "            logger.info(f\"Loaded surrogate models from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load surrogate models: {e}\")\n",
    "\n",
    "    \n",
    "    def generate_structures(self, batch_size=None):\n",
    "        \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.get('batch_size', 32)\n",
    "        \n",
    "        # Sample latent vectors from the policy network\n",
    "        z_noise = torch.randn(batch_size, self.latent_dim, device=self.device, dtype=self.dtype)\n",
    "        z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "        # For testing, let's use a smaller batch size and fewer atoms per crystal\n",
    "        \n",
    "        batch_size = z_sampled.shape[0]\n",
    "        max_atoms  = self.config.get('max_atoms', 10)\n",
    "\n",
    "        # Start with zeros (or any padding value)\n",
    "        pred_frac_coords = torch.zeros(batch_size, max_atoms, 3,\n",
    "                                    device=self.device, dtype=self.dtype)\n",
    "        pred_atom_types  = torch.zeros(batch_size, max_atoms,\n",
    "                                    device=self.device, dtype=torch.long)\n",
    "\n",
    "        \n",
    "        # Create a batch where each structure has a different number of atoms\n",
    "        num_atoms = torch.randint(2, max_atoms+1, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Lists to store fractional coordinates and atom types for each structure\n",
    "        frac_coords_list = []\n",
    "        atom_types_list = []\n",
    "\n",
    "        for i, n_i in enumerate(num_atoms):\n",
    "            # Random fractional coordinates for this structure\n",
    "            frac_coords = torch.rand(n_i, 3, device=self.device, dtype=self.dtype)\n",
    "            frac_coords_list.append(frac_coords)\n",
    "\n",
    "            # Random atom types for this structure\n",
    "            atom_types = torch.randint(0, self.n_elements, (n_i,), device=self.device)\n",
    "            atom_types_list.append(atom_types)\n",
    "\n",
    "        # Concatenate the lists to create the final tensors\n",
    "        frac_coords = torch.cat(frac_coords_list, dim=0)\n",
    "        atom_types = torch.cat(atom_types_list, dim=0)\n",
    "        \n",
    "        # Random unit cell parameters\n",
    "        lengths = torch.rand(batch_size, 3, device=self.device, dtype=self.dtype) * 5 + 5  # Between 5-10 Å\n",
    "        angles = torch.rand(batch_size, 3, device=self.device, dtype=self.dtype) * 30 + 90  # Between 90-120°\n",
    "\n",
    "        for i, n_i in enumerate(num_atoms):\n",
    "            # random coords for this sample\n",
    "            pred_frac_coords[i, :n_i] = torch.rand(n_i, 3,\n",
    "                                                device=self.device,\n",
    "                                                dtype=self.dtype)\n",
    "            # random atom types\n",
    "            pred_atom_types[i, :n_i] = torch.randint(\n",
    "                0, self.n_elements, (n_i,),\n",
    "                device=self.device\n",
    "            )\n",
    "    \n",
    "        \n",
    "        # Generate structures using the decoder\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                pred_cart_coord_diff, pred_atom_types = self.decoder(\n",
    "                    z_sampled,\n",
    "                    frac_coords,\n",
    "                    atom_types,\n",
    "                    num_atoms,\n",
    "                    lengths,\n",
    "                    angles\n",
    "                )\n",
    "                \n",
    "                # Combine the results\n",
    "                generated_structures = {\n",
    "                    'frac_coords': frac_coords,\n",
    "                    'atom_types': atom_types,\n",
    "                    'num_atoms': num_atoms,\n",
    "                    'lengths': lengths,\n",
    "                    'angles': angles,\n",
    "                    'pred_cart_coord_diff': pred_cart_coord_diff,\n",
    "                    'pred_atom_types': pred_atom_types\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder: {e}\")\n",
    "                generated_structures = {\n",
    "                    'frac_coords': frac_coords,\n",
    "                    'atom_types': atom_types,\n",
    "                    'num_atoms': num_atoms,\n",
    "                    'lengths': lengths,\n",
    "                    'angles': angles\n",
    "                }\n",
    "    \n",
    "        return generated_structures, z_sampled, log_probs\n",
    "\n",
    "    def evaluate_structures(self, structures, z_vectors):\n",
    "        \"\"\"Evaluate generated structures using surrogate models.\"\"\"\n",
    "        # Predict formation energies\n",
    "        with torch.no_grad():\n",
    "            energies = self.energy_predictor(z_vectors)\n",
    "            \n",
    "            # Predict topological indices (Z2 invariants, Chern numbers, etc.)\n",
    "            topo_indices = self.topological_predictor(z_vectors)\n",
    "            \n",
    "            # Calculate band gaps (can be part of the topological predictor or separate)\n",
    "            band_gaps = self.estimate_band_gap(structures, z_vectors)\n",
    "            \n",
    "        # Combine predictions into a comprehensive evaluation\n",
    "        evaluations = {\n",
    "            'formation_energies': energies.cpu().numpy(),\n",
    "            'topological_indices': topo_indices.cpu().numpy(),\n",
    "            'band_gaps': band_gaps.cpu().numpy() if isinstance(band_gaps, torch.Tensor) else band_gaps\n",
    "        }\n",
    "        \n",
    "        return evaluations\n",
    "\n",
    "    def calculate_rewards(self, evaluations):\n",
    "        \"\"\"Calculate rewards based on desired material properties.\"\"\"\n",
    "        # Extract evaluations\n",
    "        energies = evaluations['formation_energies']\n",
    "        topo_indices = evaluations['topological_indices']\n",
    "        band_gaps = evaluations['band_gaps']\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        if isinstance(energies, torch.Tensor):\n",
    "            energies = energies.cpu().numpy().squeeze()\n",
    "        if isinstance(topo_indices, torch.Tensor):\n",
    "            topo_indices = topo_indices.cpu().numpy()\n",
    "        if isinstance(band_gaps, torch.Tensor):\n",
    "            band_gaps = band_gaps.cpu().numpy().squeeze()\n",
    "            \n",
    "        # Calculate stability reward component\n",
    "        # Lower formation energy is better, but must be below threshold to be stable\n",
    "        stability_threshold = self.config.get('stability_threshold', 0.1)\n",
    "        stability_rewards = -energies * (energies < stability_threshold)\n",
    "        \n",
    "        # Calculate topological reward component\n",
    "        # For Z2 invariants, we typically want (1;000) for 3D TIs\n",
    "        # This is a simplified example - actual implementation depends on how topo_indices are represented\n",
    "        topo_rewards = np.sum(topo_indices * self.config.get('topo_weights', [2.0, 1.0, 1.0, 1.0]), axis=1)\n",
    "        \n",
    "        # Calculate band gap reward component\n",
    "        # Usually want a moderate band gap (not too small, not too large)\n",
    "        target_gap = self.config.get('target_band_gap', 0.3)  # in eV\n",
    "        gap_tolerance = self.config.get('gap_tolerance', 0.2)  # in eV\n",
    "        gap_rewards = 1.0 - np.minimum(np.abs(band_gaps - target_gap) / gap_tolerance, 1.0)\n",
    "        \n",
    "        # Combine reward components with configurable weights\n",
    "        w_stability = self.config.get('w_stability', 1.0)\n",
    "        w_topological = self.config.get('w_topological', 2.0)\n",
    "        w_gap = self.config.get('w_gap', 1.5)\n",
    "        \n",
    "        combined_rewards = (w_stability * stability_rewards + \n",
    "                           w_topological * topo_rewards +\n",
    "                           w_gap * gap_rewards)\n",
    "        \n",
    "        # Create rewards dictionary\n",
    "        rewards_dict = {\n",
    "            'total': combined_rewards,\n",
    "            'stability': stability_rewards,\n",
    "            'topological': topo_rewards,\n",
    "            'band_gap': gap_rewards\n",
    "        }\n",
    "        \n",
    "        return rewards_dict\n",
    "\n",
    "    def estimate_band_gap(self, structures, z_vectors):\n",
    "        \"\"\"Estimate band gaps of structures using a surrogate model.\"\"\"\n",
    "        # This would typically be a separate model or part of topological_predictor\n",
    "        # For simplicity, we'll use a mock implementation\n",
    "        batch_size = z_vectors.shape[0]\n",
    "        \n",
    "        # Mock band gap estimation (replace with actual model)\n",
    "        # In practice, this would use a trained neural network or other predictor\n",
    "        gaps = 0.2 + 0.3 * torch.sigmoid(z_vectors[:, 0]) + 0.1 * torch.randn(batch_size).to(self.device)\n",
    "        \n",
    "        return gaps\n",
    "\n",
    "    def reinforce_update(self, rewards, log_probs):\n",
    "        \"\"\"Update policy network using REINFORCE algorithm.\"\"\"\n",
    "        # Convert to tensor with the right dtype\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = -(log_probs * rewards_normalized).mean()\n",
    "        \n",
    "        # Update policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Optional gradient clipping\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "    def actor_critic_update(self, z_vectors, rewards, log_probs):\n",
    "        \"\"\"Update policy and critic networks using Actor-Critic algorithm.\"\"\"\n",
    "        if self.critic is None:\n",
    "            return self.reinforce_update(rewards, log_probs)\n",
    "            \n",
    "        # Convert rewards to tensor with proper dtype\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Get critic's value predictions\n",
    "        value_predictions = self.critic(z_vectors).squeeze()\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = rewards_tensor - value_predictions.detach()\n",
    "        \n",
    "        # Calculate policy (actor) loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "        \n",
    "        # Option 1: Combine losses and do a single backward pass\n",
    "        total_loss = policy_loss + critic_loss\n",
    "        \n",
    "        # Zero all gradients\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Single backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping if needed\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.critic.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "        \n",
    "        # Update both networks\n",
    "        self.policy_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item()\n",
    "    \n",
    "  \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform a single training step.\"\"\"\n",
    "        # Generate structures\n",
    "        structures, z_vectors, log_probs = self.generate_structures()\n",
    "        print(len(structures))\n",
    "\n",
    "        if not structures: \n",
    "            logger.error(\"no strucutres generated in this step\")\n",
    "            return  {'mean_reward': 0, 'max_reward': 0, 'mean_energy': 0, 'policy_loss': 0}\n",
    "        \n",
    "        # Evaluate structures\n",
    "        evaluations = self.evaluate_structures(structures, z_vectors)\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards_dict = self.calculate_rewards(evaluations)\n",
    "        total_rewards = rewards_dict['total']\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        for i in range(len(total_rewards)):\n",
    "            self.replay_buffer.add(\n",
    "                z_vectors[i].detach().cpu().numpy(),\n",
    "                total_rewards[i],\n",
    "                log_probs[i].detach().cpu().numpy()\n",
    "            )\n",
    "        \n",
    "        # Update policy using actor-critic or REINFORCE\n",
    "        if self.critic is not None:\n",
    "            loss_info = self.actor_critic_update(z_vectors, total_rewards, log_probs)\n",
    "            policy_loss = loss_info[0]\n",
    "        else:\n",
    "            policy_loss = self.reinforce_update(total_rewards, log_probs)\n",
    "            \n",
    "        # Track best structures\n",
    "        best_idx = np.argmax(total_rewards)\n",
    "\n",
    "        if best_idx >= len(total_rewards):\n",
    "            logger.warning(f\"best_idx {best_idx} out of bounds for total_rewards with length {len(total_rewards)}\")\n",
    "            best_idx = 0  # Fallback to first item\n",
    "\n",
    "        #print(f\"Type of total_rewards: {type(total_rewards)}\")\n",
    "        #print(f\"Shape of total_rewards: {total_rewards.shape if hasattr(total_rewards, 'shape') else 'no shape attribute'}\")\n",
    "        #print(f\"Type of total_rewards[best_idx]: {type(total_rewards[best_idx])}\")\n",
    "            \n",
    "        # Fix: Access the scalar value directly without trying to index further\n",
    "        #best_reward = float(total_rewards[best_idx])  # Remove the [0] indexing\n",
    "        reward_value = total_rewards[best_idx]\n",
    "        if isinstance(reward_value, np.ndarray):\n",
    "            if reward_value.size == 1:\n",
    "                best_reward = float(reward_value.item())\n",
    "            else:\n",
    "                # If it's an array with multiple values, take the first one\n",
    "                best_reward = float(reward_value[0])\n",
    "        else:\n",
    "            # If it's already a scalar type (int, float)\n",
    "            best_reward = float(reward_value)\n",
    "\n",
    "        # Determine if this iteration’s best is a new overall best\n",
    "        if len(self.results['best_rewards']) == 0:\n",
    "            is_new_best = True\n",
    "        else:\n",
    "            prev_best = max(self.results['best_rewards'])\n",
    "            # force a Python bool\n",
    "            is_new_best = bool(best_reward > prev_best)\n",
    "\n",
    "        if is_new_best:\n",
    "            best_structure = {\n",
    "                'frac_coords': structures['frac_coords'][best_idx].tolist(),\n",
    "                'atom_types': structures['atom_types'][best_idx].tolist(),\n",
    "                'lengths': structures['lengths'][best_idx].tolist(),\n",
    "                'angles': structures['angles'][best_idx].tolist()\n",
    "            }\n",
    "            self.results['best_structures'].append(best_structure)\n",
    "            self.results['best_rewards'].append(best_reward)\n",
    "            logger.info(f\"New best structure found with reward: {best_reward:.4f}\")\n",
    "\n",
    "        # Log results\n",
    "        self.results['rewards'].append(total_rewards)\n",
    "        self.results['z_gap'].append(evaluations['band_gaps'])\n",
    "        self.results['topological_indices'].append(evaluations['topological_indices'])\n",
    "        self.results['formation_energies'].append(evaluations['formation_energies'])\n",
    "    # Surrogate model to predict topological invariants (Z2, Chern number) \n",
    "    # from latent space.\n",
    "    \n",
    "\n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64], num_invariants=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - multiple values for topological invariants\n",
    "        # For 3D topological insulators, typically 4 Z2 invariants (ν₀;ν₁ν₂ν₃)\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.invariant_head = nn.Linear(input_dim, num_invariants)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict topological invariants from latent vector.\"\"\"\n",
    "        features = self.feature_extractor(z)\n",
    "        # Apply sigmoid to constrain outputs between 0 and 1\n",
    "        # In practice, these would be discretized to 0 or 1 when interpreting\n",
    "        invariants = torch.sigmoid(self.invariant_head(features))\n",
    "        return invariants\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for more stable training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add(self, z, reward, log_prob):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (z, reward, log_prob)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        z, rewards, log_probs = map(np.array, zip(*batch))\n",
    "        return z, rewards, log_probs\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdvae\n",
    "\n",
    "# Example configuration\n",
    "def get_default_config():\n",
    "    \"\"\"Get default configuration for CDVAE + RL training.\"\"\"\n",
    "    return {\n",
    "        # Model dimensions\n",
    "        \"latent_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \n",
    "        # Elements to consider\n",
    "        \"elements\": [\"Si\", \"Ge\", \"Sn\", \"Pb\", \"Bi\", \"Sb\", \"Te\", \"Se\", \"O\"],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"batch_size\": 32,\n",
    "        \"num_iterations\": 1000,\n",
    "        \"policy_lr\": 1e-4,\n",
    "        \"critic_lr\": 3e-4,\n",
    "        \"surrogate_lr\": 1e-4,\n",
    "        \n",
    "        # RL parameters\n",
    "        \"use_critic\": True,  # Use actor-critic instead of REINFORCE\n",
    "        \"clip_grad\": True,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"buffer_size\": 5000,  # Replay buffer size\n",
    "        \n",
    "        # Reward components\n",
    "        \"stability_threshold\": 0.1,\n",
    "        \"target_band_gap\": 0.3,  # Target band gap in eV\n",
    "        \"gap_tolerance\": 0.2,    # Acceptable deviation from target\n",
    "        \"topo_weights\": [2.0, 1.0, 1.0, 1.0],  # Weights for Z2 invariants\n",
    "        \n",
    "        # Reward weights\n",
    "        \"w_stability\": 1.0,\n",
    "        \"w_topological\": 2.0,\n",
    "        \"w_gap\": 1.5,\n",
    "        \n",
    "        # Logging and checkpoints\n",
    "        \"log_frequency\": 10,\n",
    "        \"save_frequency\": 100,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"results_dir\": \"./results\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got dict\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m config = get_default_config()\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Create training framework\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Set default tensor type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m ti_generator = \u001b[43mCDVAE_TI_Generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     21\u001b[39m ti_generator.train(num_iterations=\u001b[32m500\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mCDVAE_TI_Generator.__init__\u001b[39m\u001b[34m(self, latent_dim, hidden_dims, num_invariants)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# Build hidden layers\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hidden_dim \u001b[38;5;129;01min\u001b[39;00m hidden_dims:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     layers.append(\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    527\u001b[39m     layers.append(nn.ReLU())\n\u001b[32m    528\u001b[39m     input_dim = hidden_dim\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/Topological_Insulators_OnGithub/TIvenv/lib/python3.12/site-packages/torch/nn/modules/linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[31mTypeError\u001b[39m: empty(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got dict\""
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "    # Get default configuration\n",
    "    config = get_default_config()\n",
    "    \n",
    "    # Create training framework\n",
    "    # Set default tensor type\n",
    "    ti_generator = CDVAE_TI_Generator(config)\n",
    "    \n",
    "    # Train the model\n",
    "    ti_generator.train(num_iterations=500)\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(ti_generator.results['rewards'])\n",
    "    plt.title('Average Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    # Plot formation energies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(ti_generator.results['formation_energies'])\n",
    "    plt.title('Average Formation Energy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Energy (eV)')\n",
    "    \n",
    "    # Plot topological indices\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(ti_generator.results['topological_indices'])\n",
    "    plt.title('Average Topological Index')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Index Value')\n",
    "    \n",
    "    # Plot best rewards\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(ti_generator.results['best_rewards'])\n",
    "    plt.title('Best Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate some final structures\n",
    "    structures, _, _ = ti_generator.generate_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
