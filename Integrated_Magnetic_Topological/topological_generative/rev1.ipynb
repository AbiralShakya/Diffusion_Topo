{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdvae.pl_modules\n",
    "import cdvae.pl_modules.decoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cdvae\n",
    "from cdvae.pl_modules.decoder import GemNetTDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentPolicyNet(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(latent_dim))  # Learnable log std\n",
    "\n",
    "    def forward(self, z_noise):\n",
    "        mu = self.fc(z_noise)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        z_sampled = dist.rsample()  # Reparameterized sampling\n",
    "        return z_sampled, dist.log_prob(z_sampled).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce_update(policy_net, optimizer, rewards, log_probs):\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)  # Normalize\n",
    "    loss = -(log_probs * rewards).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_formation_energy(structure):\n",
    "    # Placeholder for formation energy estimation\n",
    "    return torch.randn(1).item()  # Replace with actual implementation\n",
    "\n",
    "def predict_magnetic_ordering(structure):\n",
    "    # Placeholder for magnetic ordering prediction\n",
    "    return torch.sigmoid(torch.randn(1)).item()  # Replace with actual implementation\n",
    "\n",
    "decoder = GemNetTDecoder(latent_dim=latent_dim, n_elements=10)  # Example: Adjust n_elements\n",
    "\n",
    "def decoder(z):\n",
    "    # Use the CDVAE decoder to generate structures\n",
    "    generated_structure = decoder(z) # Assuming decoder takes a latent vector z\n",
    "    return [generated_structure for _ in range(batch_size)]\n",
    "\n",
    "def reward_function(structure):\n",
    "    # Custom logic based on simulated structure\n",
    "    energy = estimate_formation_energy(structure)\n",
    "    magnetic_score = predict_magnetic_ordering(structure)\n",
    "\n",
    "    reward = -energy + 2.0 * magnetic_score  # Tunable trade-off\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_steps = 100\n",
    "batch_size = 32\n",
    "latent_dim = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize policy network and optimizer\n",
    "policy_net = LatentPolicyNet(latent_dim)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    z_noise = torch.randn(batch_size, latent_dim)\n",
    "    z_sampled, log_probs = policy_net(z_noise)\n",
    "\n",
    "    # Decode structure from CDVAE decoder\n",
    "    generated_structures = decoder(z_sampled)\n",
    "\n",
    "    # Score each generated structure\n",
    "    rewards = []\n",
    "    for structure in generated_structures:\n",
    "        reward = reward_function(structure)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Update policy using REINFORCE\n",
    "    loss = reinforce_update(policy_net, optimizer, rewards, log_probs)\n",
    "\n",
    "    print(f\"Step {step} | Avg Reward: {sum(rewards)/len(rewards):.3f} | Policy Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
