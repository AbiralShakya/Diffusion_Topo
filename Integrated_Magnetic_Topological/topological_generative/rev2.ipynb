{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CDVAE_TI_Generator:\n",
    "    \"\"\"\n",
    "    Crystal Diffusion Variational Autoencoder (CDVAE) with Reinforcement Learning\n",
    "    for targeted generation of Topological Insulator materials.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize CDVAE model components\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Set up optimizers\n",
    "        self.setup_optimizers()\n",
    "        \n",
    "        # Initialize results tracking\n",
    "        self.results = {\n",
    "            'rewards': [],\n",
    "            'z_gap': [],\n",
    "            'topological_indices': [],\n",
    "            'formation_energies': [],\n",
    "            'best_structures': [],\n",
    "            'best_rewards': [],\n",
    "        }\n",
    "        \n",
    "        # Initialize replay buffer for experience replay\n",
    "        self.replay_buffer = ReplayBuffer(config['buffer_size'])\n",
    "        \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize CDVAE encoder, decoder and policy networks.\"\"\"\n",
    "        # Import specific model classes\n",
    "        try:\n",
    "            from cdvae.pl_modules.decoder import GemNetTDecoder\n",
    "            from cdvae.common.data_utils import ATOM_TYPES\n",
    "        except ImportError:\n",
    "            logger.error(\"Failed to import CDVAE modules. Please ensure CDVAE is installed correctly.\")\n",
    "            raise\n",
    "            \n",
    "        # Get dimensions and parameters from config\n",
    "        self.latent_dim = self.config['latent_dim']\n",
    "        self.n_elements = len(self.config['elements']) if 'elements' in self.config else len(ATOM_TYPES)\n",
    "        \n",
    "        #TODO: possibly write an encoder file addition to cdvae\n",
    "        # Initialize encoder (if using pre-trained weights)\n",
    "        # if self.config.get('use_encoder', False):\n",
    "        #     self.encoder = GraphEncoder(\n",
    "        #         hidden_dim=self.config['hidden_dim'],\n",
    "        #         latent_dim=self.latent_dim,\n",
    "        #         use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        #     ).to(self.device)\n",
    "            \n",
    "        #     if self.config.get('encoder_checkpoint'):\n",
    "        #         self._load_model(self.encoder, self.config['encoder_checkpoint'])\n",
    "        # else:\n",
    "        #     self.encoder = None\n",
    "\n",
    "        self.encoder = None\n",
    "            \n",
    "        # Initialize decoder\n",
    "        self.decoder = GemNetTDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            n_elements=self.n_elements,\n",
    "            cutoff=self.config.get('cutoff', 6.0),\n",
    "            max_neighbors=self.config.get('max_neighbors', 20),\n",
    "            use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('decoder_checkpoint'):\n",
    "            self._load_model(self.decoder, self.config['decoder_checkpoint'])\n",
    "            \n",
    "        # Initialize policy network for RL\n",
    "        self.policy_net = PolicyNetwork(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('policy_hidden_dims', [256, 256]),\n",
    "            activation=self.config.get('policy_activation', 'relu')\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize critic network for actor-critic methods\n",
    "        if self.config.get('use_critic', True):\n",
    "            self.critic = CriticNetwork(\n",
    "                latent_dim=self.latent_dim,\n",
    "                hidden_dims=self.config.get('critic_hidden_dims', [256, 128]),\n",
    "                activation=self.config.get('critic_activation', 'relu')\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.critic = None\n",
    "            \n",
    "        # DFT surrogate models - predict quantum properties directly from latent space\n",
    "        self.energy_predictor = EnergyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('energy_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.topological_predictor = TopologicalPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('topo_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('surrogate_checkpoint'):\n",
    "            self._load_surrogate_models(self.config['surrogate_checkpoint'])\n",
    "        \n",
    "    def setup_optimizers(self):\n",
    "        \"\"\"Set up optimizers for different components.\"\"\"\n",
    "        # Policy optimizer\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config.get('policy_lr', 1e-4),\n",
    "            weight_decay=self.config.get('policy_weight_decay', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Critic optimizer (if using actor-critic)\n",
    "        if self.critic is not None:\n",
    "            self.critic_optimizer = torch.optim.Adam(\n",
    "                self.critic.parameters(),\n",
    "                lr=self.config.get('critic_lr', 3e-4),\n",
    "                weight_decay=self.config.get('critic_weight_decay', 1e-6)\n",
    "            )\n",
    "        \n",
    "        # Surrogate model optimizers for fine-tuning\n",
    "        if self.config.get('train_surrogates', False):\n",
    "            self.energy_optimizer = torch.optim.Adam(\n",
    "                self.energy_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "            self.topo_optimizer = torch.optim.Adam(\n",
    "                self.topological_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "    def _load_model(self, model, checkpoint_path):\n",
    "        \"\"\"Load model weights from checkpoint.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            if 'state_dict' in checkpoint:\n",
    "                # Handle pytorch-lightning checkpoints\n",
    "                state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items() \n",
    "                              if k.startswith('model.')}\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "            else:\n",
    "                # Handle regular torch checkpoints\n",
    "                model.load_state_dict(checkpoint, strict=False)\n",
    "            logger.info(f\"Loaded weights from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load weights: {e}\")\n",
    "            \n",
    "    def _load_surrogate_models(self, checkpoint_path):\n",
    "        \"\"\"Load surrogate model weights.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            self.energy_predictor.load_state_dict(checkpoint['energy_predictor'])\n",
    "            self.topological_predictor.load_state_dict(checkpoint['topo_predictor'])\n",
    "            logger.info(f\"Loaded surrogate models from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load surrogate models: {e}\")\n",
    "            \n",
    "    def generate_structures(self, batch_size=None):\n",
    "        \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "        # Sample latent vectors from the policy network\n",
    "        z_noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "        z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "        # Generate structures using the decoder\n",
    "        with torch.no_grad():\n",
    "            # Assuming decoder outputs a dictionary with:\n",
    "            # - frac_coords: fractional coordinates of atoms\n",
    "            # - atom_types: types of atoms (one-hot or indices)\n",
    "            # - lattice: lattice parameters for unit cells\n",
    "            generated_structures = self.decoder(z_sampled)\n",
    "            \n",
    "        return generated_structures, z_sampled, log_probs\n",
    "    \n",
    "    def evaluate_structures(self, structures, z_vectors):\n",
    "        \"\"\"Evaluate generated structures using surrogate models.\"\"\"\n",
    "        # Predict formation energies\n",
    "        with torch.no_grad():\n",
    "            energies = self.energy_predictor(z_vectors)\n",
    "            \n",
    "            # Predict topological indices (Z2 invariants, Chern numbers, etc.)\n",
    "            topo_indices = self.topological_predictor(z_vectors)\n",
    "            \n",
    "            # Calculate band gaps (can be part of the topological predictor or separate)\n",
    "            band_gaps = self.estimate_band_gap(structures, z_vectors)\n",
    "            \n",
    "        # Combine predictions into a comprehensive evaluation\n",
    "        evaluations = {\n",
    "            'formation_energies': energies.cpu().numpy(),\n",
    "            'topological_indices': topo_indices.cpu().numpy(),\n",
    "            'band_gaps': band_gaps.cpu().numpy() if isinstance(band_gaps, torch.Tensor) else band_gaps\n",
    "        }\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "    def calculate_rewards(self, evaluations):\n",
    "        \"\"\"Calculate rewards based on desired material properties.\"\"\"\n",
    "        # Extract evaluations\n",
    "        energies = evaluations['formation_energies']\n",
    "        topo_indices = evaluations['topological_indices']\n",
    "        band_gaps = evaluations['band_gaps']\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        if isinstance(energies, torch.Tensor):\n",
    "            energies = energies.cpu().numpy()\n",
    "        if isinstance(topo_indices, torch.Tensor):\n",
    "            topo_indices = topo_indices.cpu().numpy()\n",
    "        if isinstance(band_gaps, torch.Tensor):\n",
    "            band_gaps = band_gaps.cpu().numpy()\n",
    "            \n",
    "        # Calculate stability reward component\n",
    "        # Lower formation energy is better, but must be below threshold to be stable\n",
    "        stability_threshold = self.config.get('stability_threshold', 0.1)\n",
    "        stability_rewards = -energies * (energies < stability_threshold)\n",
    "        \n",
    "        # Calculate topological reward component\n",
    "        # For Z2 invariants, we typically want (1;000) for 3D TIs\n",
    "        # This is a simplified example - actual implementation depends on how topo_indices are represented\n",
    "        topo_rewards = np.sum(topo_indices * self.config.get('topo_weights', [2.0, 1.0, 1.0, 1.0]), axis=1)\n",
    "        \n",
    "        # Calculate band gap reward component\n",
    "        # Usually want a moderate band gap (not too small, not too large)\n",
    "        target_gap = self.config.get('target_band_gap', 0.3)  # in eV\n",
    "        gap_tolerance = self.config.get('gap_tolerance', 0.2)  # in eV\n",
    "        gap_rewards = 1.0 - np.minimum(np.abs(band_gaps - target_gap) / gap_tolerance, 1.0)\n",
    "        \n",
    "        # Combine reward components with configurable weights\n",
    "        w_stability = self.config.get('w_stability', 1.0)\n",
    "        w_topological = self.config.get('w_topological', 2.0)\n",
    "        w_gap = self.config.get('w_gap', 1.5)\n",
    "        \n",
    "        combined_rewards = (w_stability * stability_rewards + \n",
    "                           w_topological * topo_rewards +\n",
    "                           w_gap * gap_rewards)\n",
    "        \n",
    "        # Create rewards dictionary\n",
    "        rewards_dict = {\n",
    "            'total': combined_rewards,\n",
    "            'stability': stability_rewards,\n",
    "            'topological': topo_rewards,\n",
    "            'band_gap': gap_rewards\n",
    "        }\n",
    "        \n",
    "        return rewards_dict\n",
    "    \n",
    "    def estimate_band_gap(self, structures, z_vectors):\n",
    "        \"\"\"Estimate band gaps of structures using a surrogate model.\"\"\"\n",
    "        # This would typically be a separate model or part of topological_predictor\n",
    "        # For simplicity, we'll use a mock implementation\n",
    "        batch_size = z_vectors.shape[0]\n",
    "        \n",
    "        # Mock band gap estimation (replace with actual model)\n",
    "        # In practice, this would use a trained neural network or other predictor\n",
    "        gaps = 0.2 + 0.3 * torch.sigmoid(z_vectors[:, 0]) + 0.1 * torch.randn(batch_size).to(self.device)\n",
    "        \n",
    "        return gaps\n",
    "        \n",
    "    def reinforce_update(self, rewards, log_probs):\n",
    "        \"\"\"Update policy network using REINFORCE algorithm.\"\"\"\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = -(log_probs * rewards_normalized).mean()\n",
    "        \n",
    "        # Update policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Optional gradient clipping\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "        \n",
    "    def actor_critic_update(self, z_vectors, rewards, log_probs):\n",
    "        \"\"\"Update policy and critic networks using Actor-Critic algorithm.\"\"\"\n",
    "        if self.critic is None:\n",
    "            return self.reinforce_update(rewards, log_probs)\n",
    "            \n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "        # Get critic's value predictions\n",
    "        value_predictions = self.critic(z_vectors).squeeze()\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = rewards_tensor - value_predictions.detach()\n",
    "        \n",
    "        # Calculate policy (actor) loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "        \n",
    "        # Update policy network\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.critic.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform a single training step.\"\"\"\n",
    "        # Generate structures\n",
    "        structures, z_vectors, log_probs = self.generate_structures()\n",
    "        \n",
    "        # Evaluate structures\n",
    "        evaluations = self.evaluate_structures(structures, z_vectors)\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards_dict = self.calculate_rewards(evaluations)\n",
    "        total_rewards = rewards_dict['total']\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        for i in range(len(total_rewards)):\n",
    "            self.replay_buffer.add(\n",
    "                z_vectors[i].detach().cpu().numpy(),\n",
    "                total_rewards[i],\n",
    "                log_probs[i].detach().cpu().numpy()\n",
    "            )\n",
    "        \n",
    "        # Update policy using actor-critic or REINFORCE\n",
    "        if self.critic is not None:\n",
    "            loss_info = self.actor_critic_update(z_vectors, total_rewards, log_probs)\n",
    "            policy_loss = loss_info[0]\n",
    "        else:\n",
    "            policy_loss = self.reinforce_update(total_rewards, log_probs)\n",
    "            \n",
    "        # Track best structures\n",
    "        best_idx = np.argmax(total_rewards)\n",
    "        best_reward = total_rewards[best_idx]\n",
    "        \n",
    "        # Update current best if this is better\n",
    "        if not self.results['best_rewards'] or best_reward > max(self.results['best_rewards']):\n",
    "            self.results['best_structures'].append(structures[best_idx])\n",
    "            \n",
    "        # Store results\n",
    "        self.results['rewards'].append(np.mean(total_rewards))\n",
    "        self.results['formation_energies'].append(np.mean(evaluations['formation_energies']))\n",
    "        self.results['topological_indices'].append(np.mean(evaluations['topological_indices']))\n",
    "        self.results['best_rewards'].append(best_reward)\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(total_rewards),\n",
    "            'max_reward': np.max(total_rewards),\n",
    "            'mean_energy': np.mean(evaluations['formation_energies']),\n",
    "            'policy_loss': policy_loss\n",
    "        }\n",
    "    \n",
    "    def train(self, num_iterations=None):\n",
    "        \"\"\"Train the model for the specified number of iterations.\"\"\"\n",
    "        if num_iterations is None:\n",
    "            num_iterations = self.config.get('num_iterations', 500)\n",
    "            \n",
    "        logger.info(f\"Starting training for {num_iterations} iterations\")\n",
    "        \n",
    "        for iteration in tqdm(range(num_iterations)):\n",
    "            # Perform a training step\n",
    "            step_results = self.train_step()\n",
    "            \n",
    "            # Log progress periodically\n",
    "            if iteration % self.config.get('log_frequency', 10) == 0:\n",
    "                logger.info(\n",
    "                    f\"Iteration {iteration} | \"\n",
    "                    f\"Mean Reward: {step_results['mean_reward']:.4f} | \"\n",
    "                    f\"Max Reward: {step_results['max_reward']:.4f} | \"\n",
    "                    f\"Mean Energy: {step_results['mean_energy']:.4f} | \"\n",
    "                    f\"Policy Loss: {step_results['policy_loss']:.4f}\"\n",
    "                )\n",
    "                \n",
    "            # Save checkpoints periodically\n",
    "            if iteration % self.config.get('save_frequency', 100) == 0 and iteration > 0:\n",
    "                self.save_checkpoint(f\"checkpoint_iter_{iteration}.pt\")\n",
    "                \n",
    "        logger.info(\"Training completed\")\n",
    "        self.save_checkpoint(\"final_checkpoint.pt\")\n",
    "        self.save_results(\"training_results.pkl\")\n",
    "        \n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_dir = self.config.get('checkpoint_dir', './checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'iteration': len(self.results['rewards'])\n",
    "        }\n",
    "        \n",
    "        if self.critic is not None:\n",
    "            checkpoint['critic_state_dict'] = self.critic.state_dict()\n",
    "            checkpoint['critic_optimizer'] = self.critic_optimizer.state_dict()\n",
    "            \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "    def save_results(self, filename):\n",
    "        \"\"\"Save training results.\"\"\"\n",
    "        results_dir = self.config.get('results_dir', './results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        results_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "            \n",
    "        logger.info(f\"Saved results to {results_path}\")\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model from checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "        \n",
    "        if self.critic is not None and 'critic_state_dict' in checkpoint:\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "            \n",
    "        logger.info(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        return checkpoint.get('iteration', 0)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for RL-based latent space exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 256], activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Map activation function string to actual function\n",
    "        act_fn = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'silu': nn.SiLU()\n",
    "        }.get(activation.lower(), nn.ReLU())\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(act_fn)\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer for mean\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(input_dim, latent_dim)\n",
    "        \n",
    "        # Learnable log std for exploration\n",
    "        self.log_std = nn.Parameter(torch.zeros(latent_dim))\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z_noise):\n",
    "        \"\"\"\n",
    "        Forward pass through the policy network.\n",
    "        \n",
    "        Args:\n",
    "            z_noise: Random noise tensor of shape [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            z_sampled: Sampled latent vectors\n",
    "            log_probs: Log probabilities of the sampled vectors\n",
    "        \"\"\"\n",
    "        x = self.fc_layers(z_noise)\n",
    "        mu = self.fc_mu(x)\n",
    "        \n",
    "        # Get standard deviation from learnable parameter\n",
    "        std = torch.exp(self.log_std.clamp(-20, 2))  # Clamp for stability\n",
    "        \n",
    "        # Create normal distribution\n",
    "        dist = Normal(mu, std)\n",
    "        \n",
    "        # Sample using reparameterization trick\n",
    "        z_sampled = dist.rsample()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = dist.log_prob(z_sampled).sum(dim=-1)\n",
    "        \n",
    "        return z_sampled, log_probs\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network for actor-critic method.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 128], activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Map activation function string to actual function\n",
    "        act_fn = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'silu': nn.SiLU()\n",
    "        }.get(activation.lower(), nn.ReLU())\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(act_fn)\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - single value output\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass through the critic network.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector of shape [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            value: Predicted value of the state\n",
    "        \"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class EnergyPredictor(nn.Module):\n",
    "    \"\"\"Surrogate model to predict formation energy from latent space.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - single value for formation energy\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict formation energy from latent vector.\"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class TopologicalPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Surrogate model to predict topological invariants (Z2, Chern number) \n",
    "    from latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64], num_invariants=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - multiple values for topological invariants\n",
    "        # For 3D topological insulators, typically 4 Z2 invariants (ν₀;ν₁ν₂ν₃)\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.invariant_head = nn.Linear(input_dim, num_invariants)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict topological invariants from latent vector.\"\"\"\n",
    "        features = self.feature_extractor(z)\n",
    "        # Apply sigmoid to constrain outputs between 0 and 1\n",
    "        # In practice, these would be discretized to 0 or 1 when interpreting\n",
    "        invariants = torch.sigmoid(self.invariant_head(features))\n",
    "        return invariants\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for more stable training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add(self, z, reward, log_prob):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (z, reward, log_prob)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        z, rewards, log_probs = map(np.array, zip(*batch))\n",
    "        return z, rewards, log_probs\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example configuration\n",
    "def get_default_config():\n",
    "    \"\"\"Get default configuration for CDVAE + RL training.\"\"\"\n",
    "    return {\n",
    "        # Model dimensions\n",
    "        \"latent_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \n",
    "        # Elements to consider\n",
    "        \"elements\": [\"Si\", \"Ge\", \"Sn\", \"Pb\", \"Bi\", \"Sb\", \"Te\", \"Se\", \"O\"],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"batch_size\": 32,\n",
    "        \"num_iterations\": 1000,\n",
    "        \"policy_lr\": 1e-4,\n",
    "        \"critic_lr\": 3e-4,\n",
    "        \"surrogate_lr\": 1e-4,\n",
    "        \n",
    "        # RL parameters\n",
    "        \"use_critic\": True,  # Use actor-critic instead of REINFORCE\n",
    "        \"clip_grad\": True,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"buffer_size\": 5000,  # Replay buffer size\n",
    "        \n",
    "        # Reward components\n",
    "        \"stability_threshold\": 0.1,\n",
    "        \"target_band_gap\": 0.3,  # Target band gap in eV\n",
    "        \"gap_tolerance\": 0.2,    # Acceptable deviation from target\n",
    "        \"topo_weights\": [2.0, 1.0, 1.0, 1.0],  # Weights for Z2 invariants\n",
    "        \n",
    "        # Reward weights\n",
    "        \"w_stability\": 1.0,\n",
    "        \"w_topological\": 2.0,\n",
    "        \"w_gap\": 1.5,\n",
    "        \n",
    "        # Logging and checkpoints\n",
    "        \"log_frequency\": 10,\n",
    "        \"save_frequency\": 100,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"results_dir\": \"./results\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 16:41:52,882 - INFO - Using device: cpu\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so, 0x0006): Symbol not found: __ZN2at4_ops16div__Tensor_mode4callERNS_6TensorERKS2_NSt3__18optionalIN3c1017basic_string_viewIcEEEE\n  Referenced from: <4A3195B8-9E71-3AE7-AE80-DBA66ADAC535> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so\n  Expected in:     <DA215AD3-6EAE-3755-B6A5-A8EB4EF952B0> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m config \u001b[38;5;241m=\u001b[39m get_default_config()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create training framework\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m ti_generator \u001b[38;5;241m=\u001b[39m \u001b[43mCDVAE_TI_Generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m ti_generator\u001b[38;5;241m.\u001b[39mtrain(num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mCDVAE_TI_Generator.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize CDVAE model components\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Set up optimizers\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_optimizers()\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mCDVAE_TI_Generator.initialize_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Import specific model classes\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpl_modules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GemNetTDecoder\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ATOM_TYPES\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cdvae/pl_modules/decoder.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpl_modules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAX_ATOMIC_NUM\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpl_modules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GemNetT\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_mlp\u001b[39m(in_dim, hidden_dim, fc_num_layers, out_dim):\n\u001b[1;32m     10\u001b[0m     mods \u001b[38;5;241m=\u001b[39m [nn\u001b[38;5;241m.\u001b[39mLinear(in_dim, hidden_dim), nn\u001b[38;5;241m.\u001b[39mReLU()]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/cdvae/pl_modules/gemnet/gemnet.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_sparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseTensor\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdvae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     get_pbc_distances, radius_graph_pbc, frac_to_cart_coords)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_scatter/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m spec \u001b[38;5;241m=\u001b[39m cuda_spec \u001b[38;5;129;01mor\u001b[39;00m cpu_spec\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBUILD_DOCS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibrary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mosp\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_ops.py:1357\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1352\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so, 0x0006): Symbol not found: __ZN2at4_ops16div__Tensor_mode4callERNS_6TensorERKS2_NSt3__18optionalIN3c1017basic_string_viewIcEEEE\n  Referenced from: <4A3195B8-9E71-3AE7-AE80-DBA66ADAC535> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch_scatter/_scatter_cpu.so\n  Expected in:     <DA215AD3-6EAE-3755-B6A5-A8EB4EF952B0> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Get default configuration\n",
    "    config = get_default_config()\n",
    "    \n",
    "    # Create training framework\n",
    "    ti_generator = CDVAE_TI_Generator(config)\n",
    "    \n",
    "    # Train the model\n",
    "    ti_generator.train(num_iterations=500)\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(ti_generator.results['rewards'])\n",
    "    plt.title('Average Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    # Plot formation energies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(ti_generator.results['formation_energies'])\n",
    "    plt.title('Average Formation Energy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Energy (eV)')\n",
    "    \n",
    "    # Plot topological indices\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(ti_generator.results['topological_indices'])\n",
    "    plt.title('Average Topological Index')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Index Value')\n",
    "    \n",
    "    # Plot best rewards\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(ti_generator.results['best_rewards'])\n",
    "    plt.title('Best Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate some final structures\n",
    "    structures, _, _ = ti_generator.generate_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
