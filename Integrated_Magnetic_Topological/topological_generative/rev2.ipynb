{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3168869884.py, line 587)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 587\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mself.results['rewards'].append(np.mean(total_rewards))\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/Users/abiralshakya/Documents/Research/Topological_Insulators_OnGithub/generative_nmti/cdvae')\n",
    "import cdvae\n",
    "# from cdvae.pl_modules.model import CDVAE\n",
    "# from cdvae.common.data_utils import get_train_val_test_test_loaders\n",
    "# from cdvae.common.data_utils import get_train_val_test_loaders\n",
    "# from cdvae.pl_data.dataset import CDVAEDataset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CDVAE_TI_Generator:\n",
    "    \"\"\"\n",
    "    Crystal Diffusion Variational Autoencoder (CDVAE) with Reinforcement Learning\n",
    "    for targeted generation of Topological Insulator materials.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.float32 #set default tensor type to float32\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize CDVAE model components\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Set up optimizers\n",
    "        self.setup_optimizers()\n",
    "        \n",
    "        # Initialize results tracking\n",
    "        self.results = {\n",
    "            'rewards': [],\n",
    "            'z_gap': [],\n",
    "            'topological_indices': [],\n",
    "            'formation_energies': [],\n",
    "            'best_structures': [],\n",
    "            'best_rewards': [],\n",
    "        }\n",
    "        \n",
    "        # Initialize replay buffer for experience replay\n",
    "        self.replay_buffer = ReplayBuffer(config['buffer_size'])\n",
    "        \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize CDVAE encoder, decoder and policy networks.\"\"\"\n",
    "        # Import specific model classes\n",
    "        try:\n",
    "            from cdvae.pl_modules.decoder import GemNetTDecoder\n",
    "            from cdvae.common.data_utils import StandardScalerTorch\n",
    "            from cdvae.common.data_utils import ATOM_TYPES\n",
    "     \n",
    "        except ImportError:\n",
    "            logger.error(\"Failed to import CDVAE modules. Please ensure CDVAE is installed correctly.\")\n",
    "            raise\n",
    "            \n",
    "        # Get dimensions and parameters from config\n",
    "        self.latent_dim = self.config['latent_dim']\n",
    "        self.n_elements = len(self.config['elements']) if 'elements' in self.config else len(ATOM_TYPES.get())\n",
    "        \n",
    "        #TODO: possibly write an encoder file addition to cdvae\n",
    "        # Initialize encoder (if using pre-trained weights)\n",
    "        # if self.config.get('use_encoder', False):\n",
    "        #     self.encoder = GraphEncoder(\n",
    "        #         hidden_dim=self.config['hidden_dim'],\n",
    "        #         latent_dim=self.latent_dim,\n",
    "        #         use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        #     ).to(self.device)\n",
    "            \n",
    "        #     if self.config.get('encoder_checkpoint'):\n",
    "        #         self._load_model(self.encoder, self.config['encoder_checkpoint'])\n",
    "        # else:\n",
    "        #     self.encoder = None\n",
    "\n",
    "        self.encoder = None\n",
    "            \n",
    "        # Initialize decoder\n",
    "        self.decoder = GemNetTDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=self.config['hidden_dim'],\n",
    "            #cutoff=self.config.get('cutoff', 6.0),\n",
    "            max_neighbors=self.config.get('max_neighbors', 20),\n",
    "            #use_layer_norm=self.config.get('use_layer_norm', True)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('decoder_checkpoint'):\n",
    "            self._load_model(self.decoder, self.config['decoder_checkpoint'])\n",
    "            \n",
    "        # Initialize policy network for RL\n",
    "        self.policy_net = PolicyNetwork(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('policy_hidden_dims', [256, 256]),\n",
    "            activation=self.config.get('policy_activation', 'relu')\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize critic network for actor-critic methods\n",
    "        if self.config.get('use_critic', True):\n",
    "            self.critic = CriticNetwork(\n",
    "                latent_dim=self.latent_dim,\n",
    "                hidden_dims=self.config.get('critic_hidden_dims', [256, 128]),\n",
    "                activation=self.config.get('critic_activation', 'relu')\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.critic = None\n",
    "            \n",
    "        # DFT surrogate models - predict quantum properties directly from latent space\n",
    "        self.energy_predictor = EnergyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('energy_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.topological_predictor = TopologicalPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config.get('topo_predictor_dims', [128, 64])\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if self.config.get('surrogate_checkpoint'):\n",
    "            self._load_surrogate_models(self.config['surrogate_checkpoint'])\n",
    "        \n",
    "    def setup_optimizers(self):\n",
    "        \"\"\"Set up optimizers for different components.\"\"\"\n",
    "        # Policy optimizer\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config.get('policy_lr', 1e-4),\n",
    "            weight_decay=self.config.get('policy_weight_decay', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Critic optimizer (if using actor-critic)\n",
    "        if self.critic is not None:\n",
    "            self.critic_optimizer = torch.optim.Adam(\n",
    "                self.critic.parameters(),\n",
    "                lr=self.config.get('critic_lr', 3e-4),\n",
    "                weight_decay=self.config.get('critic_weight_decay', 1e-6)\n",
    "            )\n",
    "        \n",
    "        # Surrogate model optimizers for fine-tuning\n",
    "        if self.config.get('train_surrogates', False):\n",
    "            self.energy_optimizer = torch.optim.Adam(\n",
    "                self.energy_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "            self.topo_optimizer = torch.optim.Adam(\n",
    "                self.topological_predictor.parameters(),\n",
    "                lr=self.config.get('surrogate_lr', 1e-4)\n",
    "            )\n",
    "            \n",
    "    def _load_model(self, model, checkpoint_path):\n",
    "        \"\"\"Load model weights from checkpoint.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            if 'state_dict' in checkpoint:\n",
    "                # Handle pytorch-lightning checkpoints\n",
    "                state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items() \n",
    "                              if k.startswith('model.')}\n",
    "                model.load_state_dict(state_dict, strict=False)\n",
    "            else:\n",
    "                # Handle regular torch checkpoints\n",
    "                model.load_state_dict(checkpoint, strict=False)\n",
    "            logger.info(f\"Loaded weights from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load weights: {e}\")\n",
    "            \n",
    "    def _load_surrogate_models(self, checkpoint_path):\n",
    "        \"\"\"Load surrogate model weights.\"\"\"\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            self.energy_predictor.load_state_dict(checkpoint['energy_predictor'])\n",
    "            self.topological_predictor.load_state_dict(checkpoint['topo_predictor'])\n",
    "            logger.info(f\"Loaded surrogate models from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load surrogate models: {e}\")\n",
    "            \n",
    "    # def generate_structures(self, batch_size=None):\n",
    "    #     \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "    #     if batch_size is None:\n",
    "    #         batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "    #     # Sample latent vectors from the policy network\n",
    "    #     z_noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "    #     z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "    #     # Generate structures using the decoder\n",
    "    #     with torch.no_grad():\n",
    "    #         # Assuming decoder outputs a dictionary with:\n",
    "    #         # - frac_coords: fractional coordinates of atoms\n",
    "    #         # - atom_types: types of atoms (one-hot or indices)\n",
    "    #         # - lattice: lattice parameters for unit cells\n",
    "    #         generated_structures = self.decoder(z_sampled)\n",
    "            \n",
    "    #     return generated_structures, z_sampled, log_probs\n",
    "    # def generate_structures(self, batch_size=None):\n",
    "    #     \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "    #     if batch_size is None:\n",
    "    #         batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "    #     # Sample latent vectors from the policy network\n",
    "    #     z_noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "    #     z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "    #     # Create dummy/initial structural parameters\n",
    "    #     # These values should be adjusted based on your specific use case\n",
    "    #     pred_frac_coords = torch.rand(batch_size, 10, 3).to(self.device)  # Batch, atoms, 3D coords\n",
    "    #     pred_atom_types = torch.randint(0, self.n_elements, (batch_size, 10)).to(self.device)  # Batch, atoms\n",
    "    #     num_atoms = torch.full((batch_size,), 10, dtype=torch.long).to(self.device)  # Fixed at 10 atoms per structure\n",
    "    #     lengths = torch.rand(batch_size, 3).to(self.device) * 5 + 5  # Random cell lengths between 5-10\n",
    "    #     angles = torch.rand(batch_size, 3).to(self.device) * 30 + 75  # Random angles between 75-105 degrees\n",
    "        \n",
    "    #     # Generate structures using the decoder\n",
    "    #     with torch.no_grad():\n",
    "    #         generated_structures = self.decoder(\n",
    "    #             z_sampled,\n",
    "    #             pred_frac_coords,\n",
    "    #             pred_atom_types,\n",
    "    #             num_atoms,\n",
    "    #             lengths,\n",
    "    #             angles\n",
    "    #         )\n",
    "            \n",
    "    #     return generated_structures, z_sampled, log_probs\n",
    "    \n",
    "    def generate_structures(self, batch_size=None):\n",
    "        \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "        # Sample latent vectors from the policy network\n",
    "        z_noise = torch.randn(batch_size, self.latent_dim, device=self.device, dtype=self.dtype)\n",
    "        z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "        # For testing, let's use a smaller batch size and fewer atoms per crystal\n",
    "        max_atoms = 5  # Small number of atoms per crystal for testing\n",
    "        \n",
    "        # Create a batch where each structure has a different number of atoms\n",
    "        num_atoms = torch.randint(2, max_atoms+1, (batch_size,), device=self.device)\n",
    "        \n",
    "        # Create tensors with proper dimensions\n",
    "        total_atoms = num_atoms.sum().item()\n",
    "        \n",
    "        # Create a batch index\n",
    "        batch_idx = torch.repeat_interleave(\n",
    "            torch.arange(batch_size, device=self.device), \n",
    "            num_atoms\n",
    "        )\n",
    "        \n",
    "        # Random fractional coordinates for each atom (values between 0 and 1)\n",
    "        frac_coords = torch.rand(total_atoms, 3, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Random atom types\n",
    "        atom_types = torch.randint(0, self.n_elements, (total_atoms,), device=self.device)\n",
    "        \n",
    "        # Random unit cell parameters\n",
    "        lengths = torch.rand(batch_size, 3, device=self.device, dtype=self.dtype) * 5 + 5  # Between 5-10 Å\n",
    "        angles = torch.rand(batch_size, 3, device=self.device, dtype=self.dtype) * 30 + 90  # Between 90-120°\n",
    "        \n",
    "        # Generate structures using the decoder\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                pred_cart_coord_diff, pred_atom_types = self.decoder(\n",
    "                    z_sampled,\n",
    "                    frac_coords,\n",
    "                    atom_types,\n",
    "                    num_atoms,\n",
    "                    lengths,\n",
    "                    angles\n",
    "                )\n",
    "                \n",
    "                # Combine the results\n",
    "                generated_structures = {\n",
    "                    'frac_coords': frac_coords,\n",
    "                    'atom_types': atom_types,\n",
    "                    'num_atoms': num_atoms,\n",
    "                    'lengths': lengths,\n",
    "                    'angles': angles,\n",
    "                    'pred_cart_coord_diff': pred_cart_coord_diff,\n",
    "                    'pred_atom_types': pred_atom_types\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder: {e}\")\n",
    "                generated_structures = {\n",
    "                    'frac_coords': frac_coords,\n",
    "                    'atom_types': atom_types,\n",
    "                    'num_atoms': num_atoms,\n",
    "                    'lengths': lengths,\n",
    "                    'angles': angles\n",
    "                }\n",
    "        \n",
    "        return generated_structures, z_sampled, log_probs\n",
    "    \n",
    "    def evaluate_structures(self, structures, z_vectors):\n",
    "        \"\"\"Evaluate generated structures using surrogate models.\"\"\"\n",
    "        # Predict formation energies\n",
    "        with torch.no_grad():\n",
    "            energies = self.energy_predictor(z_vectors)\n",
    "            \n",
    "            # Predict topological indices (Z2 invariants, Chern numbers, etc.)\n",
    "            topo_indices = self.topological_predictor(z_vectors)\n",
    "            \n",
    "            # Calculate band gaps (can be part of the topological predictor or separate)\n",
    "            band_gaps = self.estimate_band_gap(structures, z_vectors)\n",
    "            \n",
    "        # Combine predictions into a comprehensive evaluation\n",
    "        evaluations = {\n",
    "            'formation_energies': energies.cpu().numpy(),\n",
    "            'topological_indices': topo_indices.cpu().numpy(),\n",
    "            'band_gaps': band_gaps.cpu().numpy() if isinstance(band_gaps, torch.Tensor) else band_gaps\n",
    "        }\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "    def calculate_rewards(self, evaluations):\n",
    "        \"\"\"Calculate rewards based on desired material properties.\"\"\"\n",
    "        # Extract evaluations\n",
    "        energies = evaluations['formation_energies']\n",
    "        topo_indices = evaluations['topological_indices']\n",
    "        band_gaps = evaluations['band_gaps']\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        if isinstance(energies, torch.Tensor):\n",
    "            energies = energies.cpu().numpy()\n",
    "        if isinstance(topo_indices, torch.Tensor):\n",
    "            topo_indices = topo_indices.cpu().numpy()\n",
    "        if isinstance(band_gaps, torch.Tensor):\n",
    "            band_gaps = band_gaps.cpu().numpy()\n",
    "            \n",
    "        # Calculate stability reward component\n",
    "        # Lower formation energy is better, but must be below threshold to be stable\n",
    "        stability_threshold = self.config.get('stability_threshold', 0.1)\n",
    "        stability_rewards = -energies * (energies < stability_threshold)\n",
    "        \n",
    "        # Calculate topological reward component\n",
    "        # For Z2 invariants, we typically want (1;000) for 3D TIs\n",
    "        # This is a simplified example - actual implementation depends on how topo_indices are represented\n",
    "        topo_rewards = np.sum(topo_indices * self.config.get('topo_weights', [2.0, 1.0, 1.0, 1.0]), axis=1)\n",
    "        \n",
    "        # Calculate band gap reward component\n",
    "        # Usually want a moderate band gap (not too small, not too large)\n",
    "        target_gap = self.config.get('target_band_gap', 0.3)  # in eV\n",
    "        gap_tolerance = self.config.get('gap_tolerance', 0.2)  # in eV\n",
    "        gap_rewards = 1.0 - np.minimum(np.abs(band_gaps - target_gap) / gap_tolerance, 1.0)\n",
    "        \n",
    "        # Combine reward components with configurable weights\n",
    "        w_stability = self.config.get('w_stability', 1.0)\n",
    "        w_topological = self.config.get('w_topological', 2.0)\n",
    "        w_gap = self.config.get('w_gap', 1.5)\n",
    "        \n",
    "        combined_rewards = (w_stability * stability_rewards + \n",
    "                           w_topological * topo_rewards +\n",
    "                           w_gap * gap_rewards)\n",
    "        \n",
    "        # Create rewards dictionary\n",
    "        rewards_dict = {\n",
    "            'total': combined_rewards,\n",
    "            'stability': stability_rewards,\n",
    "            'topological': topo_rewards,\n",
    "            'band_gap': gap_rewards\n",
    "        }\n",
    "        \n",
    "        return rewards_dict\n",
    "    \n",
    "    def estimate_band_gap(self, structures, z_vectors):\n",
    "        \"\"\"Estimate band gaps of structures using a surrogate model.\"\"\"\n",
    "        # This would typically be a separate model or part of topological_predictor\n",
    "        # For simplicity, we'll use a mock implementation\n",
    "        batch_size = z_vectors.shape[0]\n",
    "        \n",
    "        # Mock band gap estimation (replace with actual model)\n",
    "        # In practice, this would use a trained neural network or other predictor\n",
    "        gaps = 0.2 + 0.3 * torch.sigmoid(z_vectors[:, 0]) + 0.1 * torch.randn(batch_size).to(self.device)\n",
    "        \n",
    "        return gaps\n",
    "    \n",
    "    def reinforce_update(self, rewards, log_probs):\n",
    "        \"\"\"Update policy network using REINFORCE algorithm.\"\"\"\n",
    "        # Convert to tensor with the right dtype\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = -(log_probs * rewards_normalized).mean()\n",
    "        \n",
    "        # Update policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Optional gradient clipping\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "        \n",
    "    # def reinforce_update(self, rewards, log_probs):\n",
    "    #     \"\"\"Update policy network using REINFORCE algorithm.\"\"\"\n",
    "    #     rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "    #     # Normalize rewards\n",
    "    #     rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        \n",
    "    #     # Calculate policy loss\n",
    "    #     policy_loss = -(log_probs * rewards_normalized).mean()\n",
    "        \n",
    "    #     # Update policy\n",
    "    #     self.policy_optimizer.zero_grad()\n",
    "    #     policy_loss.backward()\n",
    "        \n",
    "    #     # Optional gradient clipping\n",
    "    #     if self.config.get('clip_grad', False):\n",
    "    #         torch.nn.utils.clip_grad_norm_(\n",
    "    #             self.policy_net.parameters(), \n",
    "    #             self.config.get('max_grad_norm', 1.0)\n",
    "    #         )\n",
    "            \n",
    "    #     self.policy_optimizer.step()\n",
    "        \n",
    "    #     return policy_loss.item()\n",
    "    \n",
    "    def actor_critic_update(self, z_vectors, rewards, log_probs):\n",
    "        \"\"\"Update policy and critic networks using Actor-Critic algorithm.\"\"\"\n",
    "        if self.critic is None:\n",
    "            return self.reinforce_update(rewards, log_probs)\n",
    "            \n",
    "        # Convert rewards to tensor with proper dtype\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Get critic's value predictions\n",
    "        value_predictions = self.critic(z_vectors).squeeze()\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = rewards_tensor - value_predictions.detach()\n",
    "        \n",
    "        # Calculate policy (actor) loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "        \n",
    "        # Option 1: Combine losses and do a single backward pass\n",
    "        total_loss = policy_loss + critic_loss\n",
    "        \n",
    "        # Zero all gradients\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        \n",
    "        # Single backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping if needed\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.critic.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "        \n",
    "        # Update both networks\n",
    "        self.policy_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item()\n",
    "    \n",
    "    # def actor_critic_update(self, z_vectors, rewards, log_probs):\n",
    "    #     \"\"\"Update policy and critic networks using Actor-Critic algorithm.\"\"\"\n",
    "    #     if self.critic is None:\n",
    "    #         return self.reinforce_update(rewards, log_probs)\n",
    "            \n",
    "    #     rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "    #     # Get critic's value predictions\n",
    "    #     value_predictions = self.critic(z_vectors).squeeze()\n",
    "        \n",
    "    #     # Calculate advantages\n",
    "    #     advantages = rewards_tensor - value_predictions.detach()\n",
    "        \n",
    "    #     # Calculate policy (actor) loss\n",
    "    #     policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "    #     # Calculate value (critic) loss\n",
    "    #     critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "        \n",
    "    #     # Update policy network\n",
    "    #     self.policy_optimizer.zero_grad()\n",
    "    #     policy_loss.backward()\n",
    "    #     if self.config.get('clip_grad', False):\n",
    "    #         torch.nn.utils.clip_grad_norm_(\n",
    "    #             self.policy_net.parameters(), \n",
    "    #             self.config.get('max_grad_norm', 1.0)\n",
    "    #         )\n",
    "    #     self.policy_optimizer.step()\n",
    "        \n",
    "    #     # Update critic network\n",
    "    #     self.critic_optimizer.zero_grad()\n",
    "    #     critic_loss.backward()\n",
    "    #     if self.config.get('clip_grad', False):\n",
    "    #         torch.nn.utils.clip_grad_norm_(\n",
    "    #             self.critic.parameters(), \n",
    "    #             self.config.get('max_grad_norm', 1.0)\n",
    "    #         )\n",
    "    #     self.critic_optimizer.step()\n",
    "        \n",
    "    #     return policy_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform a single training step.\"\"\"\n",
    "        # Generate structures\n",
    "        structures, z_vectors, log_probs = self.generate_structures()\n",
    "        print(len(structures))\n",
    "\n",
    "        if not structures: \n",
    "            logger.error(\"no strucutres generated in this step\")\n",
    "            return  {'mean_reward': 0, 'max_reward': 0, 'mean_energy': 0, 'policy_loss': 0}\n",
    "        \n",
    "        # Evaluate structures\n",
    "        evaluations = self.evaluate_structures(structures, z_vectors)\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards_dict = self.calculate_rewards(evaluations)\n",
    "        total_rewards = rewards_dict['total']\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        for i in range(len(total_rewards)):\n",
    "            self.replay_buffer.add(\n",
    "                z_vectors[i].detach().cpu().numpy(),\n",
    "                total_rewards[i],\n",
    "                log_probs[i].detach().cpu().numpy()\n",
    "            )\n",
    "        \n",
    "        # Update policy using actor-critic or REINFORCE\n",
    "        if self.critic is not None:\n",
    "            loss_info = self.actor_critic_update(z_vectors, total_rewards, log_probs)\n",
    "            policy_loss = loss_info[0]\n",
    "        else:\n",
    "            policy_loss = self.reinforce_update(total_rewards, log_probs)\n",
    "            \n",
    "        # Track best structures\n",
    "        best_idx = np.argmax(total_rewards)\n",
    "        best_reward = total_rewards[best_idx]\n",
    "        \n",
    "        # # Update current best if this is better\n",
    "        # if not self.results['best_rewards'] or best_reward > max(self.results['best_rewards']):\n",
    "        #     self.results['best_structures'].append(structures[best_idx])\n",
    "\n",
    "        if not self.results['best_rewards'] or best_reward > max(self.results['best_rewards']):\n",
    "            # Core fields (always present)\n",
    "            best_struct = {\n",
    "                'frac_coords': structures['frac_coords'][best_idx],\n",
    "                'atom_types':  structures['atom_types'][best_idx],\n",
    "                'num_atoms':   structures['num_atoms'][best_idx],\n",
    "                'lengths':     structures['lengths'][best_idx],\n",
    "                'angles':      structures['angles'][best_idx],\n",
    "            }\n",
    "            # Optional fields (guard against None)\n",
    "            pccd = structures.get('pred_cart_coord_diff', None)\n",
    "            if pccd is not None:\n",
    "                best_struct['pred_cart_coord_diff'] = pccd[best_idx]\n",
    "            pact = structures.get('pred_atom_types', None)\n",
    "            if pact is not None:\n",
    "                best_struct['pred_atom_types'] = pact[best_idx]\n",
    "\n",
    "        self.results['best_structures'].append(best_struct)\n",
    "\n",
    "                    \n",
    "        # Store results\n",
    "        self.results['rewards'].append(np.mean(total_rewards))\n",
    "        self.results['formation_energies'].append(np.mean(evaluations['formation_energies']))\n",
    "        self.results['topological_indices'].append(np.mean(evaluations['topological_indices']))\n",
    "        self.results['best_rewards'].append(best_reward)\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(total_rewards),\n",
    "            'max_reward': np.max(total_rewards),\n",
    "            'mean_energy': np.mean(evaluations['formation_energies']),\n",
    "            'policy_loss': policy_loss\n",
    "        }\n",
    "    \n",
    "    def train(self, num_iterations=None):\n",
    "        \"\"\"Train the model for the specified number of iterations.\"\"\"\n",
    "        if num_iterations is None:\n",
    "            num_iterations = self.config.get('num_iterations', 500)\n",
    "            \n",
    "        logger.info(f\"Starting training for {num_iterations} iterations\")\n",
    "        \n",
    "        for iteration in tqdm(range(num_iterations)):\n",
    "            # Perform a training step\n",
    "            step_results = self.train_step()\n",
    "            \n",
    "            # Log progress periodically\n",
    "            if iteration % self.config.get('log_frequency', 10) == 0:\n",
    "                logger.info(\n",
    "                    f\"Iteration {iteration} | \"\n",
    "                    f\"Mean Reward: {step_results['mean_reward']:.4f} | \"\n",
    "                    f\"Max Reward: {step_results['max_reward']:.4f} | \"\n",
    "                    f\"Mean Energy: {step_results['mean_energy']:.4f} | \"\n",
    "                    f\"Policy Loss: {step_results['policy_loss']:.4f}\"\n",
    "                )\n",
    "                \n",
    "            # Save checkpoints periodically\n",
    "            if iteration % self.config.get('save_frequency', 100) == 0 and iteration > 0:\n",
    "                self.save_checkpoint(f\"checkpoint_iter_{iteration}.pt\")\n",
    "                \n",
    "        logger.info(\"Training completed\")\n",
    "        self.save_checkpoint(\"final_checkpoint.pt\")\n",
    "        self.save_results(\"training_results.pkl\")\n",
    "        \n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_dir = self.config.get('checkpoint_dir', './checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'policy_optimizer': self.policy_optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'iteration': len(self.results['rewards'])\n",
    "        }\n",
    "        \n",
    "        if self.critic is not None:\n",
    "            checkpoint['critic_state_dict'] = self.critic.state_dict()\n",
    "            checkpoint['critic_optimizer'] = self.critic_optimizer.state_dict()\n",
    "            \n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "    def save_results(self, filename):\n",
    "        \"\"\"Save training results.\"\"\"\n",
    "        results_dir = self.config.get('results_dir', './results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        results_path = os.path.join(results_dir, filename)\n",
    "        \n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "            \n",
    "        logger.info(f\"Saved results to {results_path}\")\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Load model from checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        self.policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        self.policy_optimizer.load_state_dict(checkpoint['policy_optimizer'])\n",
    "        \n",
    "        if self.critic is not None and 'critic_state_dict' in checkpoint:\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "            \n",
    "        logger.info(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "        return checkpoint.get('iteration', 0)\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for RL-based latent space exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 256], activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Map activation function string to actual function\n",
    "        act_fn = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'silu': nn.SiLU()\n",
    "        }.get(activation.lower(), nn.ReLU())\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(act_fn)\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer for mean\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(input_dim, latent_dim)\n",
    "        \n",
    "        # Learnable log std for exploration\n",
    "        self.log_std = nn.Parameter(torch.zeros(latent_dim))\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    # def _initialize_weights(self):\n",
    "    #     \"\"\"Initialize network weights.\"\"\"\n",
    "    #     for m in self.modules():\n",
    "    #         if isinstance(m, nn.Linear):\n",
    "    #             nn.init.xavier_normal_(m.weight)\n",
    "    #             if m.bias is not None:\n",
    "    #                 nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "                \n",
    "                # Ensure dtype is correct\n",
    "                m.weight.data = m.weight.data.to(dtype=torch.float32)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = m.bias.data.to(dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, z_noise):\n",
    "        \"\"\"\n",
    "        Forward pass through the policy network.\n",
    "        \n",
    "        Args:\n",
    "            z_noise: Random noise tensor of shape [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            z_sampled: Sampled latent vectors\n",
    "            log_probs: Log probabilities of the sampled vectors\n",
    "        \"\"\"\n",
    "        x = self.fc_layers(z_noise)\n",
    "        mu = self.fc_mu(x)\n",
    "        \n",
    "        # Get standard deviation from learnable parameter\n",
    "        std = torch.exp(self.log_std.clamp(-20, 2))  # Clamp for stability\n",
    "        \n",
    "        # Create normal distribution\n",
    "        dist = Normal(mu, std)\n",
    "        \n",
    "        # Sample using reparameterization trick\n",
    "        z_sampled = dist.rsample()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = dist.log_prob(z_sampled).sum(dim=-1)\n",
    "        \n",
    "        return z_sampled, log_probs\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network for actor-critic method.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 128], activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Map activation function string to actual function\n",
    "        act_fn = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.2),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'silu': nn.SiLU()\n",
    "        }.get(activation.lower(), nn.ReLU())\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(act_fn)\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - single value output\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass through the critic network.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector of shape [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            value: Predicted value of the state\n",
    "        \"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class EnergyPredictor(nn.Module):\n",
    "    \"\"\"Surrogate model to predict formation energy from latent space.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - single value for formation energy\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict formation energy from latent vector.\"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class TopologicalPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Surrogate model to predict topological invariants (Z2, Chern number) \n",
    "    from latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64], num_invariants=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - multiple values for topological invariants\n",
    "        # For 3D topological insulators, typically 4 Z2 invariants (ν₀;ν₁ν₂ν₃)\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        self.invariant_head = nn.Linear(input_dim, num_invariants)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict topological invariants from latent vector.\"\"\"\n",
    "        features = self.feature_extractor(z)\n",
    "        # Apply sigmoid to constrain outputs between 0 and 1\n",
    "        # In practice, these would be discretized to 0 or 1 when interpreting\n",
    "        invariants = torch.sigmoid(self.invariant_head(features))\n",
    "        return invariants\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for more stable training.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add(self, z, reward, log_prob):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (z, reward, log_prob)\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        z, rewards, log_probs = map(np.array, zip(*batch))\n",
    "        return z, rewards, log_probs\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size.\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdvae\n",
    "\n",
    "# Example configuration\n",
    "def get_default_config():\n",
    "    \"\"\"Get default configuration for CDVAE + RL training.\"\"\"\n",
    "    return {\n",
    "        # Model dimensions\n",
    "        \"latent_dim\": 64,\n",
    "        \"hidden_dim\": 128,\n",
    "        \n",
    "        # Elements to consider\n",
    "        \"elements\": [\"Si\", \"Ge\", \"Sn\", \"Pb\", \"Bi\", \"Sb\", \"Te\", \"Se\", \"O\"],\n",
    "        \n",
    "        # Training parameters\n",
    "        \"batch_size\": 32,\n",
    "        \"num_iterations\": 1000,\n",
    "        \"policy_lr\": 1e-4,\n",
    "        \"critic_lr\": 3e-4,\n",
    "        \"surrogate_lr\": 1e-4,\n",
    "        \n",
    "        # RL parameters\n",
    "        \"use_critic\": True,  # Use actor-critic instead of REINFORCE\n",
    "        \"clip_grad\": True,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"buffer_size\": 5000,  # Replay buffer size\n",
    "        \n",
    "        # Reward components\n",
    "        \"stability_threshold\": 0.1,\n",
    "        \"target_band_gap\": 0.3,  # Target band gap in eV\n",
    "        \"gap_tolerance\": 0.2,    # Acceptable deviation from target\n",
    "        \"topo_weights\": [2.0, 1.0, 1.0, 1.0],  # Weights for Z2 invariants\n",
    "        \n",
    "        # Reward weights\n",
    "        \"w_stability\": 1.0,\n",
    "        \"w_topological\": 2.0,\n",
    "        \"w_gap\": 1.5,\n",
    "        \n",
    "        # Logging and checkpoints\n",
    "        \"log_frequency\": 10,\n",
    "        \"save_frequency\": 100,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"results_dir\": \"./results\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 12:26:25,295 - INFO - Using device: cpu\n",
      "2025-04-06 12:26:25,456 - INFO - Starting training for 500 iterations\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]/var/folders/sb/srkfc7tj3319314qk68v5cch0000gn/T/ipykernel_58467/442450879.py:456: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in decoder: index out of range in self\n",
      "5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m ti_generator = CDVAE_TI_Generator(config)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mti_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Plot training results\u001b[39;00m\n\u001b[32m     24\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m8\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 601\u001b[39m, in \u001b[36mCDVAE_TI_Generator.train\u001b[39m\u001b[34m(self, num_iterations)\u001b[39m\n\u001b[32m    597\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_iterations)):\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     step_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m     \u001b[38;5;66;03m# Log progress periodically\u001b[39;00m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m iteration % \u001b[38;5;28mself\u001b[39m.config.get(\u001b[33m'\u001b[39m\u001b[33mlog_frequency\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m10\u001b[39m) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 573\u001b[39m, in \u001b[36mCDVAE_TI_Generator.train_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# # Update current best if this is better\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# if not self.results['best_rewards'] or best_reward > max(self.results['best_rewards']):\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m#     self.results['best_structures'].append(structures[best_idx])\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results[\u001b[33m'\u001b[39m\u001b[33mbest_rewards\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m best_reward > \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.results[\u001b[33m'\u001b[39m\u001b[33mbest_rewards\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m    567\u001b[39m     best_struct = {\n\u001b[32m    568\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfrac_coords\u001b[39m\u001b[33m'\u001b[39m:   structures[\u001b[33m'\u001b[39m\u001b[33mfrac_coords\u001b[39m\u001b[33m'\u001b[39m][best_idx],\n\u001b[32m    569\u001b[39m         \u001b[33m'\u001b[39m\u001b[33matom_types\u001b[39m\u001b[33m'\u001b[39m:    structures[\u001b[33m'\u001b[39m\u001b[33matom_types\u001b[39m\u001b[33m'\u001b[39m][best_idx],\n\u001b[32m    570\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnum_atoms\u001b[39m\u001b[33m'\u001b[39m:     structures[\u001b[33m'\u001b[39m\u001b[33mnum_atoms\u001b[39m\u001b[33m'\u001b[39m][best_idx],\n\u001b[32m    571\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlengths\u001b[39m\u001b[33m'\u001b[39m:       structures[\u001b[33m'\u001b[39m\u001b[33mlengths\u001b[39m\u001b[33m'\u001b[39m][best_idx],\n\u001b[32m    572\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mangles\u001b[39m\u001b[33m'\u001b[39m:        structures[\u001b[33m'\u001b[39m\u001b[33mangles\u001b[39m\u001b[33m'\u001b[39m][best_idx],\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpred_cart_coord_diff\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mstructures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpred_cart_coord_diff\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_idx\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m    574\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpred_atom_types\u001b[39m\u001b[33m'\u001b[39m:      structures.get(\u001b[33m'\u001b[39m\u001b[33mpred_atom_types\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)[best_idx],\n\u001b[32m    575\u001b[39m     }\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m.results[\u001b[33m'\u001b[39m\u001b[33mbest_structures\u001b[39m\u001b[33m'\u001b[39m].append(best_struct)\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "    # Get default configuration\n",
    "    config = get_default_config()\n",
    "    \n",
    "    # Create training framework\n",
    "    # Set default tensor type\n",
    "    ti_generator = CDVAE_TI_Generator(config)\n",
    "    \n",
    "    # Train the model\n",
    "    ti_generator.train(num_iterations=500)\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot rewards\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(ti_generator.results['rewards'])\n",
    "    plt.title('Average Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    # Plot formation energies\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(ti_generator.results['formation_energies'])\n",
    "    plt.title('Average Formation Energy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Energy (eV)')\n",
    "    \n",
    "    # Plot topological indices\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(ti_generator.results['topological_indices'])\n",
    "    plt.title('Average Topological Index')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Index Value')\n",
    "    \n",
    "    # Plot best rewards\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(ti_generator.results['best_rewards'])\n",
    "    plt.title('Best Reward')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate some final structures\n",
    "    structures, _, _ = ti_generator.generate_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/abiralshakya/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/sb/srkfc7tj3319314qk68v5cch0000gn/T/ipykernel_4835/2440548420.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 12:07:28,664 - INFO - Using device: cpu\n",
      "2025-04-05 12:07:29,484 - INFO - Starting training for 500 iterations\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 690\u001b[0m\n\u001b[1;32m    687\u001b[0m ti_generator \u001b[38;5;241m=\u001b[39m TopologicalInsulatorGenerator(config)\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mti_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# Generate and evaluate final structures\u001b[39;00m\n\u001b[1;32m    693\u001b[0m final_structures, z_vectors, _ \u001b[38;5;241m=\u001b[39m ti_generator\u001b[38;5;241m.\u001b[39mgenerate_structures(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 399\u001b[0m, in \u001b[0;36mTopologicalInsulatorGenerator.train\u001b[0;34m(self, num_iterations)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_iterations)):\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Perform training step\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Log progress\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iter_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_freq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 358\u001b[0m, in \u001b[0;36mTopologicalInsulatorGenerator.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Update policy\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 358\u001b[0m     actor_loss, critic_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy_actor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_rewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     loss_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: actor_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: critic_loss}\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 339\u001b[0m, in \u001b[0;36mTopologicalInsulatorGenerator.update_policy_actor_critic\u001b[0;34m(self, z_vectors, rewards)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Update critic network\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 339\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy_loss\u001b[38;5;241m.\u001b[39mitem(), critic_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Normal\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SimplifiedDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified decoder that generates crystal structures directly from latent vectors.\n",
    "    Instead of relying on complex GemNetT architecture, we use a simpler MLP-based model.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim=128, max_atoms=32, n_elements=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_atoms = max_atoms\n",
    "        self.n_elements = n_elements\n",
    "        \n",
    "        # MLP for decoding\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        # - Fractional coordinates: 3 values per atom (x,y,z)\n",
    "        # - Atom types: one-hot encoding for each atom\n",
    "        # - Existence flags: binary value indicating if atom exists\n",
    "        # - Lattice parameters: 6 values (a, b, c, alpha, beta, gamma)\n",
    "        self.coords_head = nn.Linear(hidden_dim * 2, max_atoms * 3)\n",
    "        self.atom_types_head = nn.Linear(hidden_dim * 2, max_atoms * n_elements)\n",
    "        self.exists_head = nn.Linear(hidden_dim * 2, max_atoms)\n",
    "        self.lattice_head = nn.Linear(hidden_dim * 2, 6)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vectors into crystal structures.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vectors [batch_size, latent_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing crystal structure components:\n",
    "            - frac_coords: fractional coordinates [batch_size, max_atoms, 3]\n",
    "            - atom_types: one-hot encoded atom types [batch_size, max_atoms, n_elements]\n",
    "            - atom_mask: existence mask [batch_size, max_atoms]\n",
    "            - lattice: lattice parameters [batch_size, 6]\n",
    "        \"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "        \n",
    "        # Pass through MLP\n",
    "        h = self.mlp(z)\n",
    "        \n",
    "        # Decode coordinates\n",
    "        coords_flat = self.coords_head(h)\n",
    "        frac_coords = coords_flat.view(batch_size, self.max_atoms, 3)\n",
    "        # Constrain to unit cell (0,1)\n",
    "        frac_coords = torch.sigmoid(frac_coords)\n",
    "        \n",
    "        # Decode atom types\n",
    "        atom_types_logits = self.atom_types_head(h)\n",
    "        atom_types = atom_types_logits.view(batch_size, self.max_atoms, self.n_elements)\n",
    "        atom_types = F.softmax(atom_types, dim=-1)\n",
    "        \n",
    "        # Decode existence flags\n",
    "        exists_logits = self.exists_head(h)\n",
    "        atom_mask = torch.sigmoid(exists_logits)\n",
    "        \n",
    "        # Decode lattice parameters\n",
    "        # a, b, c: positive values in Angstrom\n",
    "        # alpha, beta, gamma: angles in degrees, constrained to reasonable ranges\n",
    "        lattice_params = self.lattice_head(h)\n",
    "        \n",
    "        # Split into cell lengths and angles\n",
    "        cell_lengths = torch.abs(lattice_params[:, :3]) + 3.0  # Min 3 Angstrom\n",
    "        cell_angles = 60 + 60 * torch.sigmoid(lattice_params[:, 3:])  # Range: 60-120 degrees\n",
    "        \n",
    "        # Combine into final lattice parameters\n",
    "        lattice = torch.cat([cell_lengths, cell_angles], dim=-1)\n",
    "        \n",
    "        return {\n",
    "            'frac_coords': frac_coords,\n",
    "            'atom_types': atom_types,\n",
    "            'atom_mask': atom_mask,\n",
    "            'lattice': lattice\n",
    "        }\n",
    "\n",
    "\n",
    "class TopologicalInsulatorGenerator:\n",
    "    \"\"\"\n",
    "    Simplified framework for generating topological insulator materials\n",
    "    using a VAE with reinforcement learning for optimization.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Setup optimizers\n",
    "        self.setup_optimizers()\n",
    "        \n",
    "        # Results tracking\n",
    "        self.results = defaultdict(list)\n",
    "        \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize all model components.\"\"\"\n",
    "        # Get dimensions\n",
    "        self.latent_dim = self.config['latent_dim']\n",
    "        self.hidden_dim = self.config['hidden_dim']\n",
    "        self.n_elements = len(self.config['elements'])\n",
    "        self.max_atoms = self.config['max_atoms']\n",
    "        \n",
    "        # Initialize decoder\n",
    "        self.decoder = SimplifiedDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            max_atoms=self.max_atoms,\n",
    "            n_elements=self.n_elements\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize policy network\n",
    "        self.policy_net = PolicyNetwork(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config['policy_hidden_dims']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize surrogate property predictors\n",
    "        self.formation_energy_net = PropertyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config['property_hidden_dims'],\n",
    "            output_dim=1,\n",
    "            name=\"Formation Energy\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.band_structure_net = PropertyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config['property_hidden_dims'],\n",
    "            output_dim=self.config['band_structure_dim'],\n",
    "            name=\"Band Structure\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.topological_net = PropertyPredictor(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dims=self.config['property_hidden_dims'],\n",
    "            output_dim=self.config['topo_invariant_dim'],\n",
    "            name=\"Topological Invariants\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Optional: initialize critic for actor-critic method\n",
    "        if self.config.get('use_critic', True):\n",
    "            self.critic = CriticNetwork(\n",
    "                latent_dim=self.latent_dim,\n",
    "                hidden_dims=self.config['critic_hidden_dims']\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.critic = None\n",
    "    \n",
    "    def setup_optimizers(self):\n",
    "        \"\"\"Setup optimizers for all trainable components.\"\"\"\n",
    "        # Policy optimizer\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=self.config['policy_lr'],\n",
    "            weight_decay=self.config.get('weight_decay', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Optional critic optimizer\n",
    "        if self.critic is not None:\n",
    "            self.critic_optimizer = torch.optim.Adam(\n",
    "                self.critic.parameters(),\n",
    "                lr=self.config['critic_lr'],\n",
    "                weight_decay=self.config.get('weight_decay', 1e-6)\n",
    "            )\n",
    "    \n",
    "    def generate_structures(self, batch_size=None):\n",
    "        \"\"\"Generate crystal structures from latent space samples.\"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.config['batch_size']\n",
    "        \n",
    "        # Sample from policy network\n",
    "        z_noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "        z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "        # Generate structures using the decoder\n",
    "        with torch.no_grad():\n",
    "            structures = self.decoder(z_sampled)\n",
    "        \n",
    "        return structures, z_sampled, log_probs\n",
    "\n",
    "    # def generate_structures(self, batch_size = None):\n",
    "    # \"\"\"Generate crystal structures using the policy network and decoder.\"\"\"\n",
    "    #     if batch_size is None:\n",
    "    #         batch_size = self.config.get('batch_size', 32)\n",
    "            \n",
    "    #     # Sample latent vectors from the policy network\n",
    "    #     z_noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "    #     z_sampled, log_probs = self.policy_net(z_noise)\n",
    "        \n",
    "    #     # Create dummy/initial structural parameters\n",
    "    #     # These values should be adjusted based on your specific use case\n",
    "    #     pred_frac_coords = torch.rand(batch_size, 10, 3).to(self.device)  # Batch, atoms, 3D coords\n",
    "    #     pred_atom_types = torch.randint(0, self.n_elements, (batch_size, 10)).to(self.device)  # Batch, atoms\n",
    "    #     num_atoms = torch.full((batch_size,), 10, dtype=torch.long).to(self.device)  # Fixed at 10 atoms per structure\n",
    "    #     lengths = torch.rand(batch_size, 3).to(self.device) * 5 + 5  # Random cell lengths between 5-10\n",
    "    #     angles = torch.rand(batch_size, 3).to(self.device) * 30 + 75  # Random angles between 75-105 degrees\n",
    "        \n",
    "    #     # Generate structures using the decoder\n",
    "    #     with torch.no_grad():\n",
    "    #         generated_structures = self.decoder(\n",
    "    #             z_sampled,\n",
    "    #             pred_frac_coords,\n",
    "    #             pred_atom_types,\n",
    "    #             num_atoms,\n",
    "    #             lengths,\n",
    "    #             angles\n",
    "    #         )\n",
    "            \n",
    "    #     return generated_structures, z_sampled, log_probs\n",
    "    \n",
    "    def predict_properties(self, z_vectors):\n",
    "        \"\"\"Predict material properties from latent vectors.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Predict formation energy (lower is better for stability)\n",
    "            energy = self.formation_energy_net(z_vectors)\n",
    "            \n",
    "            # Predict band structure features\n",
    "            band_features = self.band_structure_net(z_vectors)\n",
    "            \n",
    "            # Extract band gap from band features\n",
    "            # Assuming the first element is the band gap\n",
    "            band_gap = band_features[:, 0:1]\n",
    "            \n",
    "            # Predict topological invariants (Z2, Chern numbers)\n",
    "            topo_invariants = self.topological_net(z_vectors)\n",
    "            # Apply sigmoid to constrain between 0 and 1\n",
    "            topo_invariants = torch.sigmoid(topo_invariants)\n",
    "        \n",
    "        return {\n",
    "            'formation_energy': energy.squeeze(),\n",
    "            'band_gap': band_gap.squeeze(),\n",
    "            'topo_invariants': topo_invariants\n",
    "        }\n",
    "    \n",
    "    def calculate_rewards(self, properties):\n",
    "        \"\"\"Calculate rewards based on material properties.\"\"\"\n",
    "        # Extract properties\n",
    "        energy = properties['formation_energy']\n",
    "        band_gap = properties['band_gap']\n",
    "        topo_invariants = properties['topo_invariants']\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        if isinstance(energy, torch.Tensor):\n",
    "            energy = energy.cpu().numpy()\n",
    "        if isinstance(band_gap, torch.Tensor):\n",
    "            band_gap = band_gap.cpu().numpy()\n",
    "        if isinstance(topo_invariants, torch.Tensor):\n",
    "            topo_invariants = topo_invariants.cpu().numpy()\n",
    "        \n",
    "        # 1. Stability reward - negative formation energy with threshold\n",
    "        stability_threshold = self.config.get('stability_threshold', 0.2)\n",
    "        stability_reward = -np.clip(energy, -1.0, 1.0) * (energy < stability_threshold)\n",
    "        \n",
    "        # 2. Band gap reward - aim for target band gap\n",
    "        target_gap = self.config.get('target_band_gap', 0.3)  # in eV\n",
    "        gap_tolerance = self.config.get('gap_tolerance', 0.2)  # in eV\n",
    "        gap_reward = 1.0 - np.minimum(np.abs(band_gap - target_gap) / gap_tolerance, 1.0)\n",
    "        \n",
    "        # 3. Topological reward - prefer non-trivial topological insulators\n",
    "        # For Z2 invariants, we typically want (1;000) for strong 3D TIs\n",
    "        # Assuming first invariant is the strong Z2 index (ν₀)\n",
    "        strong_z2_idx = 0\n",
    "        topo_reward = topo_invariants[:, strong_z2_idx]\n",
    "        \n",
    "        # Add additional weight to other invariants if desired\n",
    "        if topo_invariants.shape[1] > 1:\n",
    "            weak_indices = np.mean(topo_invariants[:, 1:], axis=1) \n",
    "            # Typically want (1;000) so penalize non-zero weak indices slightly\n",
    "            topo_reward = topo_reward * (1.0 - 0.2 * weak_indices)\n",
    "        \n",
    "        # Combine rewards with configurable weights\n",
    "        w_stability = self.config.get('w_stability', 1.0)\n",
    "        w_gap = self.config.get('w_gap', 1.5)\n",
    "        w_topo = self.config.get('w_topo', 2.0)\n",
    "        \n",
    "        total_reward = (\n",
    "            w_stability * stability_reward +\n",
    "            w_gap * gap_reward +\n",
    "            w_topo * topo_reward\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total': total_reward,\n",
    "            'stability': stability_reward,\n",
    "            'band_gap': gap_reward,\n",
    "            'topological': topo_reward\n",
    "        }\n",
    "    \n",
    "    def update_policy_reinforce(self, rewards, log_probs):\n",
    "        \"\"\"Update policy using REINFORCE algorithm.\"\"\"\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = -(log_probs * rewards_normalized).mean()\n",
    "        \n",
    "        # Update policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Optional gradient clipping\n",
    "        if self.config.get('clip_grad', False):\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.policy_net.parameters(), \n",
    "                self.config.get('max_grad_norm', 1.0)\n",
    "            )\n",
    "            \n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "    \n",
    "    def update_policy_actor_critic(self, z_vectors, rewards):\n",
    "        \"\"\"Update policy using Actor-Critic algorithm.\"\"\"\n",
    "        if self.critic is None:\n",
    "            return None, None\n",
    "            \n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        \n",
    "        # Get critic's value predictions\n",
    "        value_predictions = self.critic(z_vectors).squeeze()\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = rewards_tensor - value_predictions.detach()\n",
    "        \n",
    "        # Get latest policy log probabilities\n",
    "        _, log_probs = self.policy_net(z_vectors)\n",
    "        \n",
    "        # Calculate policy (actor) loss\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        # Calculate value (critic) loss\n",
    "        critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "        \n",
    "        # Update policy network\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform a single training step.\"\"\"\n",
    "        # Generate structures\n",
    "        structures, z_vectors, log_probs = self.generate_structures()\n",
    "        \n",
    "        # Predict properties\n",
    "        properties = self.predict_properties(z_vectors)\n",
    "        \n",
    "        # Calculate rewards\n",
    "        rewards_dict = self.calculate_rewards(properties)\n",
    "        total_rewards = rewards_dict['total']\n",
    "        \n",
    "        # Update policy\n",
    "        if self.critic is not None:\n",
    "            actor_loss, critic_loss = self.update_policy_actor_critic(z_vectors, total_rewards)\n",
    "            loss_info = {'actor_loss': actor_loss, 'critic_loss': critic_loss}\n",
    "        else:\n",
    "            policy_loss = self.update_policy_reinforce(total_rewards, log_probs)\n",
    "            loss_info = {'policy_loss': policy_loss}\n",
    "        \n",
    "        # Track best structures\n",
    "        best_idx = np.argmax(total_rewards)\n",
    "        best_reward = total_rewards[best_idx]\n",
    "        best_structure = {k: v[best_idx].cpu().detach().numpy() if isinstance(v, torch.Tensor) else v[best_idx] \n",
    "                         for k, v in structures.items()}\n",
    "        \n",
    "        # Update results\n",
    "        self.results['rewards'].append(np.mean(total_rewards))\n",
    "        self.results['max_rewards'].append(best_reward)\n",
    "        self.results['formation_energy'].append(np.mean(properties['formation_energy'].cpu().numpy()))\n",
    "        self.results['band_gap'].append(np.mean(properties['band_gap'].cpu().numpy()))\n",
    "        self.results['topo_score'].append(np.mean(properties['topo_invariants'].cpu().numpy()[:, 0]))\n",
    "        \n",
    "        if not self.results.get('best_reward', []) or best_reward > max(self.results.get('best_reward', [0])):\n",
    "            self.results['best_structure'] = best_structure\n",
    "            self.results['best_reward'] = best_reward\n",
    "            self.results['best_iteration'] = len(self.results['rewards']) - 1\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(total_rewards),\n",
    "            'max_reward': best_reward,\n",
    "            'mean_energy': np.mean(properties['formation_energy'].cpu().numpy()),\n",
    "            'loss_info': loss_info\n",
    "        }\n",
    "    \n",
    "    def train(self, num_iterations=None):\n",
    "        \"\"\"Train the model for specified iterations.\"\"\"\n",
    "        if num_iterations is None:\n",
    "            num_iterations = self.config.get('num_iterations', 1000)\n",
    "        \n",
    "        logger.info(f\"Starting training for {num_iterations} iterations\")\n",
    "        \n",
    "        # Training loop\n",
    "        for iter_idx in tqdm(range(num_iterations)):\n",
    "            # Perform training step\n",
    "            step_info = self.train_step()\n",
    "            \n",
    "            # Log progress\n",
    "            if iter_idx % self.config.get('log_freq', 10) == 0:\n",
    "                loss_str = \"\"\n",
    "                if 'policy_loss' in step_info['loss_info']:\n",
    "                    loss_str = f\"Policy Loss: {step_info['loss_info']['policy_loss']:.4f}\"\n",
    "                elif 'actor_loss' in step_info['loss_info']:\n",
    "                    loss_str = (f\"Actor Loss: {step_info['loss_info']['actor_loss']:.4f}, \"\n",
    "                               f\"Critic Loss: {step_info['loss_info']['critic_loss']:.4f}\")\n",
    "                \n",
    "                logger.info(\n",
    "                    f\"Iter {iter_idx}/{num_iterations} | \"\n",
    "                    f\"Mean Reward: {step_info['mean_reward']:.4f} | \"\n",
    "                    f\"Max Reward: {step_info['max_reward']:.4f} | \"\n",
    "                    f\"Mean Energy: {step_info['mean_energy']:.4f} | \"\n",
    "                    f\"{loss_str}\"\n",
    "                )\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if iter_idx > 0 and iter_idx % self.config.get('save_freq', 100) == 0:\n",
    "                self.save_checkpoint(f\"checkpoint_iter_{iter_idx}.pt\")\n",
    "        \n",
    "        # Save final model\n",
    "        logger.info(\"Training completed\")\n",
    "        self.save_checkpoint(\"final_model.pt\")\n",
    "        self.save_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint_dir = self.config.get('checkpoint_dir', './checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'policy_state_dict': self.policy_net.state_dict(),\n",
    "            'decoder_state_dict': self.decoder.state_dict(),\n",
    "            'config': self.config,\n",
    "            'iteration': len(self.results['rewards']),\n",
    "        }\n",
    "        \n",
    "        if self.critic is not None:\n",
    "            checkpoint['critic_state_dict'] = self.critic.state_dict()\n",
    "        \n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, filename))\n",
    "        logger.info(f\"Saved checkpoint to {os.path.join(checkpoint_dir, filename)}\")\n",
    "    \n",
    "    def save_results(self):\n",
    "        \"\"\"Save training results and plots.\"\"\"\n",
    "        results_dir = self.config.get('results_dir', './results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Save metrics history\n",
    "        np.save(os.path.join(results_dir, 'training_metrics.npy'), dict(self.results))\n",
    "        \n",
    "        # Create and save plots\n",
    "        self.plot_training_results(os.path.join(results_dir, 'training_plots.png'))\n",
    "        \n",
    "        logger.info(f\"Saved results to {results_dir}\")\n",
    "    \n",
    "    def plot_training_results(self, filename):\n",
    "        \"\"\"Create plots of training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Plot average reward\n",
    "        axes[0, 0].plot(self.results['rewards'])\n",
    "        axes[0, 0].set_title('Average Reward')\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        \n",
    "        # Plot formation energy\n",
    "        axes[0, 1].plot(self.results['formation_energy'])\n",
    "        axes[0, 1].set_title('Average Formation Energy')\n",
    "        axes[0, 1].set_xlabel('Iteration')\n",
    "        axes[0, 1].set_ylabel('Energy (eV)')\n",
    "        \n",
    "        # Plot band gap\n",
    "        axes[1, 0].plot(self.results['band_gap'])\n",
    "        axes[1, 0].set_title('Average Band Gap')\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Band Gap (eV)')\n",
    "        \n",
    "        # Plot topological score\n",
    "        axes[1, 1].plot(self.results['topo_score'])\n",
    "        axes[1, 1].set_title('Average Topological Score')\n",
    "        axes[1, 1].set_xlabel('Iteration')\n",
    "        axes[1, 1].set_ylabel('Z2 Index (ν₀)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for RL-based latent space exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 256]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer for mean\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(input_dim, latent_dim)\n",
    "        \n",
    "        # Learnable log std for exploration\n",
    "        self.log_std = nn.Parameter(torch.zeros(latent_dim))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z_noise):\n",
    "        \"\"\"Forward pass through the policy network.\"\"\"\n",
    "        x = self.fc_layers(z_noise)\n",
    "        mu = self.fc_mu(x)\n",
    "        \n",
    "        # Get standard deviation from learnable parameter\n",
    "        std = torch.exp(self.log_std.clamp(-20, 2))  # Clamp for stability\n",
    "        \n",
    "        # Create normal distribution\n",
    "        dist = Normal(mu, std)\n",
    "        \n",
    "        # Sample using reparameterization trick\n",
    "        z_sampled = dist.rsample()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = dist.log_prob(z_sampled).sum(dim=-1)\n",
    "        \n",
    "        return z_sampled, log_probs\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network for actor-critic method.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[256, 128]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer - single value output\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"Forward pass through the critic network.\"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "class PropertyPredictor(nn.Module):\n",
    "    \"\"\"Neural network for predicting material properties from latent space.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, hidden_dims=[128, 64], output_dim=1, name=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = latent_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"Predict property from latent vector.\"\"\"\n",
    "        return self.model(z)\n",
    "\n",
    "\n",
    "# Default configuration\n",
    "def get_default_config():\n",
    "    return {\n",
    "        # Model dimensions\n",
    "        'latent_dim': 32,\n",
    "        'hidden_dim': 128,\n",
    "        'max_atoms': 24,\n",
    "        \n",
    "        # Elements to consider - common in topological insulators\n",
    "        'elements': ['Bi', 'Sb', 'Te', 'Se', 'Sn', 'Ge', 'Pb', 'O', 'S'],\n",
    "        \n",
    "        # Network architectures\n",
    "        'policy_hidden_dims': [256, 256],\n",
    "        'critic_hidden_dims': [256, 128],\n",
    "        'property_hidden_dims': [128, 64],\n",
    "        \n",
    "        # Property prediction dimensions\n",
    "        'band_structure_dim': 5,  # Band gap + other band structure features\n",
    "        'topo_invariant_dim': 4,  # Z2 invariants (ν₀;ν₁ν₂ν₃)\n",
    "        \n",
    "        # Training parameters\n",
    "        'batch_size': 32,\n",
    "        'num_iterations': 500,\n",
    "        'policy_lr': 1e-4,\n",
    "        'critic_lr': 3e-4,\n",
    "        'weight_decay': 1e-6,\n",
    "        'use_critic': True,\n",
    "        'clip_grad': True,\n",
    "        'max_grad_norm': 1.0,\n",
    "        \n",
    "        # Reward components\n",
    "        'stability_threshold': 0.2,\n",
    "        'target_band_gap': 0.3,  # Target band gap in eV\n",
    "        'gap_tolerance': 0.2,    # Acceptable deviation from target\n",
    "        \n",
    "        # Reward weights\n",
    "        'w_stability': 1.0,\n",
    "        'w_gap': 1.5,\n",
    "        'w_topo': 2.0,\n",
    "        \n",
    "        # Logging and checkpoints\n",
    "        'log_freq': 10,\n",
    "        'save_freq': 100,\n",
    "        'checkpoint_dir': './checkpoints',\n",
    "        'results_dir': './results'\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Get configuration\n",
    "    config = get_default_config()\n",
    "    \n",
    "    # Create generator\n",
    "    ti_generator = TopologicalInsulatorGenerator(config)\n",
    "    \n",
    "    # Train model\n",
    "    results = ti_generator.train(num_iterations=500)\n",
    "    \n",
    "    # Generate and evaluate final structures\n",
    "    final_structures, z_vectors, _ = ti_generator.generate_structures(batch_size=10)\n",
    "    properties = ti_generator.predict_properties(z_vectors)\n",
    "    rewards = ti_generator.calculate_rewards(properties)\n",
    "    \n",
    "    # Print best structure information\n",
    "    best_idx = np.argmax(rewards['total'])\n",
    "    print(\"\\nBest Generated Structure:\")\n",
    "    print(f\"- Formation Energy: {properties['formation_energy'][best_idx].item():.4f} eV\")\n",
    "    print(f\"- Band Gap: {properties['band_gap'][best_idx].item():.4f} eV\")\n",
    "    print(f\"- Z2 Invariants: {properties['topo_invariants'][best_idx].cpu().numpy()}\")\n",
    "    print(f\"- Total Reward: {rewards['total'][best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.fake_quantize_per_tensor_affine>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
