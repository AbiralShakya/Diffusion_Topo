{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_transformer_ti_generator.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- Import your new modules ---\n",
    "# These would be new files you create\n",
    "# from crystal_tokenizer import CrystalTokenizer \n",
    "# from ctg_model import CrystalTransformerGeneratorModel # Your Transformer for generation\n",
    "# from mcc_model import MultitaskCrystalClassifierModel # Your GNN/Transformer for classification\n",
    "\n",
    "# --- Placeholder for new modules (define these in separate files) ---\n",
    "class CrystalTokenizer:\n",
    "    def __init__(self, vocab_path, config):\n",
    "        self.vocab = self.load_vocab(vocab_path)\n",
    "        self.config = config\n",
    "        # <SOS>, <EOS>, <PAD>, <UNK> tokens, etc.\n",
    "        self.sos_token = \"<SOS>\"\n",
    "        self.eos_token = \"<EOS>\"\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        # TODO: Load actual vocabulary and define token-to-id and id-to-token mappings\n",
    "        print(\"Mock CrystalTokenizer initialized.\")\n",
    "\n",
    "    def load_vocab(self, vocab_path):\n",
    "        # In a real implementation, load from a file (e.g., JSON)\n",
    "        # For this skeleton, it's a mock\n",
    "        mock_vocab = {\n",
    "            \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3,\n",
    "            \"<sg_1>\": 4, \"<sg_225>\": 5, # ... all 230 space groups\n",
    "            \"<Fe>\": 234, \"<O>\": 235, \"<Si>\":236, # ... all elements\n",
    "            \"<w_4a>\": 500, \"<w_8b>\": 501, # ... all Wyckoff symbols\n",
    "            \"<length_bin_001>\": 3000, \"<length_bin_300>\": 3299, # ... discretized lengths\n",
    "            \"<angle_bin_001>\": 4000, \"<angle_bin_080>\": 4079, # ... discretized angles\n",
    "            \"<fcoord_bin_001>\": 5000, \"<fcoord_bin_100>\": 5099, # ... discretized frac coords\n",
    "        }\n",
    "        # Add more mock tokens as needed for the skeleton to run\n",
    "        for i in range(2, 225): # Add more sg tokens\n",
    "            mock_vocab[f\"<sg_{i}>\"] = 5 + (i-1)\n",
    "        return mock_vocab\n",
    "\n",
    "    def sequence_to_structure(self, token_id_sequence):\n",
    "        # Convert a sequence of token IDs back to a pymatgen Structure object\n",
    "        # This is a complex process involving parsing space group, lattice, Wyckoff sites, elements, coords\n",
    "        # For the skeleton, we'll return a mock structure or None\n",
    "        print(f\"Mock decoding sequence: {token_id_sequence[:10]}...\")\n",
    "        # In a real scenario, you'd use pymatgen here\n",
    "        try:\n",
    "            # Mock logic: if it sees common tokens, pretend it's a valid structure\n",
    "            if 1 in token_id_sequence and 2 in token_id_sequence and 5 in token_id_sequence: # SOS, EOS, SG_225\n",
    "                # This is where you'd use pymatgen to construct the structure\n",
    "                # from pymatgen.core import Structure, Lattice, Element\n",
    "                # lattice = Lattice.cubic(5.0)\n",
    "                # species = [Element(\"Fe\"), Element(\"O\")]\n",
    "                # coords = [[0,0,0], [0.5,0.5,0.5]]\n",
    "                # return Structure(lattice, species, coords)\n",
    "                return \"mock_pymatgen_structure_object\" # Placeholder\n",
    "            else: \n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to decode sequence: {e}\")\n",
    "            return None\n",
    "\n",
    "    def structure_to_sequence_ids(self, structure_obj):\n",
    "        # Convert a pymatgen Structure object to a sequence of token IDs\n",
    "        # For the skeleton, return a mock sequence\n",
    "        # Example: [<SOS_id>, <sg_225_id>, <len_bin_X_id>, ..., <Fe_id>, <w_4a_id>, ..., <EOS_id>]\n",
    "        mock_sequence_ids = [\n",
    "            self.vocab[self.sos_token], self.vocab[\"<sg_225>\"], self.vocab[\"<length_bin_001>\"],\n",
    "            self.vocab[\"<length_bin_001>\"], self.vocab[\"<length_bin_001>\"],\n",
    "            self.vocab[\"<angle_bin_001>\"], self.vocab[\"<angle_bin_001>\"], self.vocab[\"<angle_bin_001>\"],\n",
    "            self.vocab[\"<Fe>\"], self.vocab[\"<w_4a>\"],\n",
    "            self.vocab[\"<fcoord_bin_001>\"], self.vocab[\"<fcoord_bin_001>\"], self.vocab[\"<fcoord_bin_001>\"],\n",
    "            self.vocab[self.eos_token]\n",
    "        ]\n",
    "        return mock_sequence_ids\n",
    "\n",
    "class CrystalTransformerGeneratorModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "        # Mock transformer layers - in reality, use nn.TransformerDecoder or similar\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, config.get('max_seq_len', 256), embed_dim))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        print(\"Mock CTGModel initialized.\")\n",
    "\n",
    "    def forward(self, tgt_token_ids, memory=None, tgt_mask=None, tgt_key_padding_mask=None):\n",
    "        # tgt_token_ids: (batch_size, seq_len)\n",
    "        tgt_embed = self.token_embedding(tgt_token_ids) + self.positional_encoding[:, :tgt_token_ids.size(1), :]\n",
    "        \n",
    "        # For decoder-only, memory is not typically used from an encoder.\n",
    "        # If it's a true nn.TransformerDecoder, it expects memory.\n",
    "        # For a GPT-like model, you'd implement causal attention directly or use a pre-built one.\n",
    "        # This is a simplified skeleton.\n",
    "        # A true GPT-style model wouldn't use nn.TransformerDecoder in this way without an encoder's memory.\n",
    "        # It would use multiple nn.TransformerDecoderLayer with causal masking.\n",
    "        # For simplicity, let's assume memory is not used or is self-referential for generation.\n",
    "        # This part needs careful implementation based on chosen transformer type (e.g. decoder-only GPT style)\n",
    "        \n",
    "        # Simplified pass for skeleton:\n",
    "        # This is NOT a correct way to use nn.TransformerDecoder for autoregressive generation\n",
    "        # without proper causal masking and memory handling.\n",
    "        # A proper implementation would involve generating a causal mask.\n",
    "        if memory is None: # For a decoder-only model, memory would be the target itself for self-attention\n",
    "             memory = tgt_embed \n",
    "        \n",
    "        # Create a causal mask if not provided\n",
    "        if tgt_mask is None:\n",
    "            seq_len = tgt_token_ids.size(1)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt_token_ids.device)\n",
    "\n",
    "        output = self.transformer_decoder(tgt_embed, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        logits = self.fc_out(output)\n",
    "        return logits # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_sequences(self, batch_size, start_token_id, eos_token_id, max_len, device):\n",
    "        # Autoregressive generation\n",
    "        sequences = torch.full((batch_size, 1), start_token_id, dtype=torch.long, device=device)\n",
    "        log_probs_list = [[] for _ in range(batch_size)]\n",
    "        ended = [False] * batch_size\n",
    "        \n",
    "        for _ in range(max_len - 1):\n",
    "            current_seq_len = sequences.size(1)\n",
    "            # Create padding mask for current sequences\n",
    "            # All current tokens are valid, so no padding mask needed for this step if all sequences are same length\n",
    "            \n",
    "            logits = self(sequences) # Get logits for the whole sequence so far\n",
    "            next_token_logits = logits[:, -1, :] # Get logits for the next token: (batch_size, vocab_size)\n",
    "            \n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token_ids = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            \n",
    "            # Store log_probs for RL\n",
    "            for i in range(batch_size):\n",
    "                if not ended[i]:\n",
    "                    log_prob = torch.log(probs[i, next_token_ids[i, 0]])\n",
    "                    log_probs_list[i].append(log_prob)\n",
    "\n",
    "            sequences = torch.cat([sequences, next_token_ids], dim=1)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if not ended[i] and next_token_ids[i,0] == eos_token_id:\n",
    "                    ended[i] = True\n",
    "            \n",
    "            if all(ended):\n",
    "                break\n",
    "        \n",
    "        # Sum log_probs for each sequence\n",
    "        final_log_probs = torch.tensor([sum(lp).item() if lp else 0.0 for lp in log_probs_list], device=device)\n",
    "        return sequences, final_log_probs\n",
    "\n",
    "\n",
    "class MultitaskCrystalClassifierModel(nn.Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim, num_layers, mcc_config):\n",
    "        super().__init__()\n",
    "        self.mcc_config = mcc_config\n",
    "        # Mock GNN layers - in reality, use something like GCNConv, GATConv, or a graph transformer\n",
    "        self.gnn_layers = nn.ModuleList([nn.Linear(input_feat_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Output heads\n",
    "        self.topology_head = nn.Linear(hidden_dim, mcc_config.get('num_topo_classes', 2)) # e.g., TI / not TI\n",
    "        self.magnetism_head = nn.Linear(hidden_dim, mcc_config.get('num_mag_classes', 3)) # e.g., FM / AFM / Para\n",
    "        self.energy_head = nn.Linear(hidden_dim, 1) # Formation energy\n",
    "        print(\"Mock MCCModel initialized.\")\n",
    "\n",
    "    def forward(self, graph_batch): # graph_batch would be a Batch object from torch_geometric\n",
    "        # x: node features (batch_num_nodes, node_feat_dim)\n",
    "        # edge_index: graph connectivity (2, batch_num_edges)\n",
    "        # batch: assigns each node to its graph (batch_num_nodes)\n",
    "        # This is a highly simplified GNN\n",
    "        \n",
    "        # In a real GNN, you'd use message passing layers.\n",
    "        # For skeleton: pass dummy features through linear layers.\n",
    "        # Assume graph_batch is a tensor of global graph features for simplicity here.\n",
    "        # x = graph_batch # (batch_size, input_feat_dim)\n",
    "        \n",
    "        # A more realistic GNN forward pass (conceptual)\n",
    "        # x, edge_index, batch_idx = graph_batch.x, graph_batch.edge_index, graph_batch.batch\n",
    "        # for layer in self.gnn_layers:\n",
    "        #     x = self.relu(layer(x, edge_index)) # Pass edge_index to GNN layers\n",
    "        # # Global pooling (e.g., mean pooling per graph)\n",
    "        # from torch_geometric.nn import global_mean_pool\n",
    "        # x_pooled = global_mean_pool(x, batch_idx) # (batch_size, hidden_dim)\n",
    "\n",
    "        # SIMPLIFIED SKELETON: Assume graph_batch is already pooled features\n",
    "        x_pooled = graph_batch # (batch_size, hidden_dim) - this is a placeholder!\n",
    "        for layer in self.gnn_layers:\n",
    "             x_pooled = self.relu(layer(x_pooled))\n",
    "\n",
    "\n",
    "        topo_logits = self.topology_head(x_pooled)\n",
    "        mag_logits = self.magnetism_head(x_pooled)\n",
    "        energy_pred = self.energy_head(x_pooled)\n",
    "        \n",
    "        return {\n",
    "            'topology_logits': topo_logits,    # (batch_size, num_topo_classes)\n",
    "            'magnetism_logits': mag_logits,  # (batch_size, num_mag_classes)\n",
    "            'formation_energy': energy_pred # (batch_size, 1)\n",
    "        }\n",
    "\n",
    "# --- Replay Buffer (can be largely reused) ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.position = 0   \n",
    "        \n",
    "    def add(self, sequence, reward, log_prob): # Store sequence instead of z\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(None)\n",
    "        # Store sequence as list of IDs, reward, and log_prob\n",
    "        self.buffer[self.position] = (sequence.cpu().numpy().tolist(), reward, log_prob.cpu().item())\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        actual_batch_size = min(batch_size, len(self.buffer))\n",
    "        if actual_batch_size == 0:\n",
    "            return [], [], []\n",
    "        batch = random.sample(self.buffer, actual_batch_size)\n",
    "        sequences, rewards, log_probs = zip(*batch)\n",
    "        # Sequences are lists of IDs, rewards and log_probs are scalars\n",
    "        return list(sequences), np.array(rewards), np.array(log_probs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- Main Generator Class ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TransformerTIGenerator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.float32 \n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize Tokenizer\n",
    "        self.tokenizer = CrystalTokenizer(\n",
    "            vocab_path=config.get('ctg_vocab_path', 'mock_vocab.json'), \n",
    "            config=config\n",
    "        )\n",
    "        self.vocab_size = len(self.tokenizer.vocab)\n",
    "        self.sos_token_id = self.tokenizer.vocab[self.tokenizer.sos_token]\n",
    "        self.eos_token_id = self.tokenizer.vocab[self.tokenizer.eos_token]\n",
    "        self.pad_token_id = self.tokenizer.vocab[self.tokenizer.pad_token]\n",
    "\n",
    "\n",
    "        # Initialize CTG (Crystal Transformer Generator)\n",
    "        # These would be loaded from pre-trained checkpoints in a real scenario\n",
    "        self.ctg = CrystalTransformerGeneratorModel(\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=config.get('ctg_embed_dim', 256),\n",
    "            num_heads=config.get('ctg_num_heads', 8),\n",
    "            num_layers=config.get('ctg_num_layers', 6),\n",
    "            config=config\n",
    "        ).to(self.device)\n",
    "        # self._load_model(self.ctg, config.get('ctg_checkpoint_path')) # Implement this\n",
    "\n",
    "        # Initialize MCC (Multitask Crystal Classifier)\n",
    "        self.mcc = MultitaskCrystalClassifierModel(\n",
    "            input_feat_dim=config.get('mcc_input_feat_dim', 128), # Example: features from a graph node\n",
    "            hidden_dim=config.get('mcc_hidden_dim', 256),\n",
    "            num_layers=config.get('mcc_num_layers', 4),\n",
    "            mcc_config=config.get('mcc_specific_config', {})\n",
    "        ).to(self.device)\n",
    "        # self._load_model(self.mcc, config.get('mcc_checkpoint_path')) # Implement this\n",
    "\n",
    "        # Optimizers (primarily for RL fine-tuning of CTG)\n",
    "        # The CTG itself is the policy network in this new setup\n",
    "        self.ctg_optimizer = torch.optim.Adam(\n",
    "            self.ctg.parameters(),\n",
    "            lr=config.get('ctg_rl_lr', 5e-5), # Potentially smaller LR for fine-tuning\n",
    "            weight_decay=config.get('ctg_weight_decay', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Critic Network (optional, if using Actor-Critic for RL fine-tuning CTG)\n",
    "        # Input to critic needs to be decided: embedding of sequence? features from MCC?\n",
    "        self.critic_input_dim = config.get('ctg_embed_dim', 256) # Example: use CTG's output embedding\n",
    "        if self.config.get('use_critic_rl', True):\n",
    "            # Reusing your CriticNetwork definition, but input might change\n",
    "            self.critic = CriticNetwork( \n",
    "                input_dim=self.critic_input_dim, # This needs to match what you feed the critic\n",
    "                hidden_dims=config.get('critic_hidden_dims', [256, 128])\n",
    "            ).to(self.device)\n",
    "            self.critic_optimizer = torch.optim.Adam(\n",
    "                self.critic.parameters(),\n",
    "                lr=config.get('critic_lr', 1e-4)\n",
    "            )\n",
    "        else:\n",
    "            self.critic = None\n",
    "            \n",
    "        self.results = {\n",
    "            'rewards': [], 'best_structures_info': [], 'best_rewards': [],\n",
    "            'mcc_evals': {'topology': [], 'magnetism': [], 'energy': []} # Store MCC outputs\n",
    "        }\n",
    "        self.replay_buffer = ReplayBuffer(config.get('buffer_size', 1000))\n",
    "\n",
    "    def _load_model(self, model, checkpoint_path):\n",
    "        if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(checkpoint_path, map_location=self.device))\n",
    "                logger.info(f\"Loaded weights from {checkpoint_path} for {model.__class__.__name__}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load weights for {model.__class__.__name__} from {checkpoint_path}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"No checkpoint path provided or path does not exist for {model.__class__.__name__}. Using initialized weights.\")\n",
    "\n",
    "    def generate_and_decode_structures(self, batch_size):\n",
    "        \"\"\"Generate sequences with CTG and decode them into pymatgen structures.\"\"\"\n",
    "        self.ctg.eval() # Set CTG to evaluation mode for generation\n",
    "        \n",
    "        token_sequences_ids, log_probs = self.ctg.generate_sequences(\n",
    "            batch_size=batch_size,\n",
    "            start_token_id=self.sos_token_id,\n",
    "            eos_token_id=self.eos_token_id,\n",
    "            max_len=self.config.get('max_seq_len', 256),\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        decoded_structures = []\n",
    "        valid_indices = [] # Keep track of sequences that decoded successfully\n",
    "        \n",
    "        for i, seq_ids in enumerate(token_sequences_ids):\n",
    "            # Pass the tensor directly, or convert to list of python ints if tokenizer expects that\n",
    "            struct_obj = self.tokenizer.sequence_to_structure(seq_ids.cpu().tolist())\n",
    "            if struct_obj: # If decoding is successful\n",
    "                decoded_structures.append(struct_obj)\n",
    "                valid_indices.append(i)\n",
    "            else:\n",
    "                logger.debug(f\"Sequence {i} failed to decode.\")\n",
    "\n",
    "        # Filter log_probs for successfully decoded structures\n",
    "        valid_log_probs = log_probs[torch.tensor(valid_indices, device=self.device)] if valid_indices else torch.tensor([], device=self.device)\n",
    "        \n",
    "        return decoded_structures, token_sequences_ids[valid_indices], valid_log_probs # Return pymatgen objects, their sequences, and log_probs\n",
    "\n",
    "    def evaluate_structures_with_mcc(self, pymatgen_structures_list):\n",
    "        \"\"\"Evaluate decoded structures using the MCC.\"\"\"\n",
    "        if not pymatgen_structures_list:\n",
    "            return {'topology_probs': [], 'magnetism_probs': [], 'formation_energy': []} # Empty results\n",
    "\n",
    "        self.mcc.eval() # Set MCC to evaluation mode\n",
    "        batch_mcc_inputs = []\n",
    "        for struct_obj in pymatgen_structures_list:\n",
    "            # --- CRITICAL STEP: Convert pymatgen_structure to MCC input format ---\n",
    "            # This depends heavily on your MCC's architecture (e.g., graph features for a GNN)\n",
    "            # For skeleton: create dummy features\n",
    "            # In a real GNN, this would involve creating a graph object (e.g., torch_geometric.data.Data)\n",
    "            # For simplicity, assume MCC takes a fixed-size tensor representing the structure\n",
    "            mock_mcc_input = torch.randn(1, self.config.get('mcc_input_feat_dim', 128)).to(self.device)\n",
    "            batch_mcc_inputs.append(mock_mcc_input)\n",
    "        \n",
    "        if not batch_mcc_inputs:\n",
    "             return {'topology_probs': [], 'magnetism_probs': [], 'formation_energy': []}\n",
    "\n",
    "        # This assumes MCC can process a batch of these representations\n",
    "        # If MCC takes torch_geometric Batch objects, you'd use `torch_geometric.data.Batch.from_data_list()`\n",
    "        mcc_input_tensor = torch.cat(batch_mcc_inputs, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.mcc(mcc_input_tensor) # Get dict of predictions\n",
    "\n",
    "        # Process predictions (e.g., apply softmax for classification)\n",
    "        evaluations = {\n",
    "            'topology_probs': F.softmax(predictions['topology_logits'], dim=-1).cpu().numpy() if 'topology_logits' in predictions else [],\n",
    "            'magnetism_probs': F.softmax(predictions['magnetism_logits'], dim=-1).cpu().numpy() if 'magnetism_logits' in predictions else [],\n",
    "            'formation_energy': predictions['formation_energy'].cpu().numpy().squeeze() if 'formation_energy' in predictions else []\n",
    "        }\n",
    "        return evaluations\n",
    "\n",
    "    def calculate_rewards(self, evaluations):\n",
    "        \"\"\"Calculate rewards based on MCC evaluations (adapt your existing logic).\"\"\"\n",
    "        # Example: Reward for being a TI and having low energy\n",
    "        # This is highly dependent on your goals and MCC output.\n",
    "        \n",
    "        topo_probs = evaluations.get('topology_probs', np.array([]))\n",
    "        formation_energies = evaluations.get('formation_energy', np.array([]))\n",
    "\n",
    "        if topo_probs.size == 0 or formation_energies.size == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        # Assuming topo_probs is [prob_not_TI, prob_TI]\n",
    "        ti_probability = topo_probs[:, 1] if topo_probs.ndim > 1 and topo_probs.shape[1] > 1 else topo_probs\n",
    "\n",
    "        # Reward for being TI (higher prob is better)\n",
    "        reward_ti = ti_probability \n",
    "        \n",
    "        # Reward for stability (lower energy is better, normalize or scale)\n",
    "        # Simple example: -energy, maybe clip or scale\n",
    "        reward_stability = -formation_energies \n",
    "        # Normalize stability reward if energies vary a lot\n",
    "        if len(reward_stability) > 1 and np.std(reward_stability) > 1e-6:\n",
    "             reward_stability = (reward_stability - np.mean(reward_stability)) / (np.std(reward_stability) + 1e-6)\n",
    "\n",
    "        # Combine rewards (example weights)\n",
    "        w_ti = self.config.get('w_ti_reward', 2.0)\n",
    "        w_stability = self.config.get('w_stability_reward', 1.0)\n",
    "        \n",
    "        total_rewards = w_ti * reward_ti + w_stability * reward_stability\n",
    "        return total_rewards\n",
    "\n",
    "\n",
    "    def rl_update_step(self, sequences, rewards, log_probs):\n",
    "        \"\"\"Perform RL update for the CTG (policy) and critic.\"\"\"\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards_tensor = torch.tensor(rewards, device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            rewards_tensor = rewards.to(self.device).to(self.dtype)\n",
    "\n",
    "        if not isinstance(log_probs, torch.Tensor):\n",
    "            log_probs_tensor = torch.tensor(log_probs, device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            log_probs_tensor = log_probs.to(self.device).to(self.dtype)\n",
    "\n",
    "\n",
    "        if log_probs_tensor.numel() == 0 or rewards_tensor.numel() == 0:\n",
    "            logger.warning(\"Empty log_probs or rewards, skipping RL update.\")\n",
    "            return 0.0, 0.0 if self.critic else 0.0\n",
    "        \n",
    "        # --- Policy (CTG) Update ---\n",
    "        self.ctg.train()\n",
    "        if self.critic:\n",
    "            self.critic.train()\n",
    "            # For critic input, we need a representation of the state (sequence)\n",
    "            # This is a placeholder: assumes critic can take the raw sequence tensor (padded)\n",
    "            # Or, use embeddings from CTG. For now, let's assume a method to get state features.\n",
    "            # This part needs careful design: what represents the \"state\" for the critic?\n",
    "            # Let's use a dummy input for the critic for now.\n",
    "            # A better approach might be to use the hidden states of the CTG at <EOS> or an average.\n",
    "            \n",
    "            # Pad sequences for critic input if they are of variable length\n",
    "            # This requires knowing the max_len used for generation or a config param\n",
    "            padded_sequences = nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(s, device=self.device) for s in sequences], \n",
    "                batch_first=True, \n",
    "                padding_value=self.pad_token_id\n",
    "            )\n",
    "            # This is still problematic as critic expects fixed size input unless it's also a sequence model\n",
    "            # For skeleton: let's assume critic takes a fixed size input (e.g. mean embedding)\n",
    "            # This is a placeholder for getting critic input features from sequences\n",
    "            critic_input_features = torch.randn(len(sequences), self.critic_input_dim, device=self.device)\n",
    "\n",
    "\n",
    "            value_predictions = self.critic(critic_input_features).squeeze()\n",
    "            advantages = rewards_tensor - value_predictions.detach()\n",
    "            policy_loss = -(log_probs_tensor * advantages).mean()\n",
    "            \n",
    "            critic_loss = F.mse_loss(value_predictions, rewards_tensor)\n",
    "            \n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            if self.config.get('clip_grad', True):\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.get('max_grad_norm', 1.0))\n",
    "            self.critic_optimizer.step()\n",
    "        else: # REINFORCE\n",
    "            # Normalize rewards\n",
    "            rewards_normalized = (rewards_tensor - rewards_tensor.mean()) / (rewards_tensor.std() + 1e-8)\n",
    "            policy_loss = -(log_probs_tensor * rewards_normalized).mean()\n",
    "            critic_loss = torch.tensor(0.0)\n",
    "\n",
    "        self.ctg_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        if self.config.get('clip_grad', True):\n",
    "            torch.nn.utils.clip_grad_norm_(self.ctg.parameters(), self.config.get('max_grad_norm', 1.0))\n",
    "        self.ctg_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), critic_loss.item() if self.critic else 0.0\n",
    "\n",
    "\n",
    "    def train_rl_step(self):\n",
    "        \"\"\"Perform a single RL training step for fine-tuning CTG.\"\"\"\n",
    "        batch_size = self.config.get('rl_batch_size', 16)\n",
    "\n",
    "        # 1. Generate structures (sequences) and get log_probs from CTG\n",
    "        # decoded_structures_pmg is a list of pymatgen Structure objects (or your mock string)\n",
    "        # generated_sequences_ids is a tensor of token IDs (batch, seq_len) for valid structures\n",
    "        # current_log_probs is a tensor of summed log_probs for these valid sequences (batch,)\n",
    "        decoded_structures_pmg, generated_sequences_ids, current_log_probs = self.generate_and_decode_structures(batch_size)\n",
    "\n",
    "        if not decoded_structures_pmg:\n",
    "            logger.warning(\"No structures were successfully generated or decoded in this step.\")\n",
    "            return {'mean_reward': 0, 'policy_loss': 0, 'critic_loss': 0}\n",
    "\n",
    "        # 2. Evaluate structures with MCC\n",
    "        evaluations = self.evaluate_structures_with_mcc(decoded_structures_pmg)\n",
    "\n",
    "        # 3. Calculate rewards\n",
    "        total_rewards_np = self.calculate_rewards(evaluations) # numpy array\n",
    "        \n",
    "        if total_rewards_np.size == 0:\n",
    "            logger.warning(\"No rewards calculated, likely due to evaluation issues.\")\n",
    "            return {'mean_reward': 0, 'policy_loss': 0, 'critic_loss': 0}\n",
    "\n",
    "        # Add to replay buffer (store original sequences and rewards)\n",
    "        # generated_sequences_ids is already filtered to valid ones\n",
    "        for i in range(len(decoded_structures_pmg)):\n",
    "            # replay_buffer expects sequence as list of python ints, reward as float, log_prob as float\n",
    "            self.replay_buffer.add(\n",
    "                generated_sequences_ids[i], # This is a tensor for a single sequence\n",
    "                total_rewards_np[i], \n",
    "                current_log_probs[i] # This is a tensor for a single sequence's summed log_prob\n",
    "            )\n",
    "        \n",
    "        # Sample from replay buffer for update if buffer is large enough\n",
    "        if len(self.replay_buffer) < self.config.get('rl_min_buffer_for_update', batch_size // 2):\n",
    "             logger.info(f\"Replay buffer size {len(self.replay_buffer)}, need {self.config.get('rl_min_buffer_for_update', batch_size // 2)} to update.\")\n",
    "             return {'mean_reward': np.mean(total_rewards_np) if total_rewards_np.size > 0 else 0, \n",
    "                     'policy_loss': 0, 'critic_loss': 0}\n",
    "\n",
    "        sampled_sequences, sampled_rewards, sampled_log_probs = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        if not sampled_sequences:\n",
    "            logger.warning(\"Replay buffer sampled an empty batch.\")\n",
    "            return {'mean_reward': np.mean(total_rewards_np) if total_rewards_np.size > 0 else 0, \n",
    "                     'policy_loss': 0, 'critic_loss': 0}\n",
    "\n",
    "        # 4. Perform RL update\n",
    "        # sampled_sequences are lists of IDs, sampled_rewards & sampled_log_probs are numpy arrays\n",
    "        policy_loss, critic_loss = self.rl_update_step(sampled_sequences, sampled_rewards, sampled_log_probs)\n",
    "        \n",
    "        # Track results\n",
    "        mean_reward_current_batch = np.mean(total_rewards_np)\n",
    "        self.results['rewards'].append(mean_reward_current_batch)\n",
    "        \n",
    "        # Store MCC evaluations for analysis\n",
    "        for key in self.results['mcc_evals']:\n",
    "            if key in evaluations and isinstance(evaluations[key], np.ndarray) and evaluations[key].size > 0:\n",
    "                self.results['mcc_evals'][key].append(np.mean(evaluations[key], axis=0)) # Mean over batch\n",
    "\n",
    "        # Track best structures (simplified)\n",
    "        if total_rewards_np.size > 0:\n",
    "            best_idx_current_batch = np.argmax(total_rewards_np)\n",
    "            best_reward_current_batch = total_rewards_np[best_idx_current_batch]\n",
    "\n",
    "            if not self.results['best_rewards'] or best_reward_current_batch > max(self.results['best_rewards']):\n",
    "                self.results['best_rewards'].append(best_reward_current_batch)\n",
    "                # Store info about the best structure (e.g., its sequence or key properties)\n",
    "                best_struct_info = {\n",
    "                    'sequence': generated_sequences_ids[best_idx_current_batch].cpu().tolist(),\n",
    "                    'reward': best_reward_current_batch,\n",
    "                    'mcc_topo_prob': evaluations['topology_probs'][best_idx_current_batch] if evaluations['topology_probs'].size > 0 else None,\n",
    "                    'mcc_energy': evaluations['formation_energy'][best_idx_current_batch] if evaluations['formation_energy'].size > 0 else None,\n",
    "                }\n",
    "                self.results['best_structures_info'].append(best_struct_info)\n",
    "                logger.info(f\"New best structure found with reward: {best_reward_current_batch:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': mean_reward_current_batch,\n",
    "            'policy_loss': policy_loss,\n",
    "            'critic_loss': critic_loss\n",
    "        }\n",
    "\n",
    "    def train_rl_fine_tuning(self, num_iterations=None):\n",
    "        if num_iterations is None:\n",
    "            num_iterations = self.config.get('num_rl_iterations', 1000)\n",
    "        \n",
    "        logger.info(f\"Starting RL fine-tuning for {num_iterations} iterations\")\n",
    "        \n",
    "        for iteration in range(num_iterations):\n",
    "            step_results = self.train_rl_step()\n",
    "            \n",
    "            if iteration % self.config.get('log_frequency', 10) == 0:\n",
    "                logger.info(\n",
    "                    f\"Iter {iteration} | Mean Reward: {step_results.get('mean_reward',0):.3f} | \"\n",
    "                    f\"Policy Loss: {step_results.get('policy_loss',0):.4f} | \"\n",
    "                    f\"Critic Loss: {step_results.get('critic_loss',0):.4f}\"\n",
    "                )\n",
    "            \n",
    "            if iteration % self.config.get('save_frequency', 100) == 0 and iteration > 0:\n",
    "                self.save_checkpoint(f\"rl_checkpoint_iter_{iteration}.pt\")\n",
    "        \n",
    "        logger.info(\"RL fine-tuning completed.\")\n",
    "        self.save_checkpoint(\"final_rl_checkpoint.pt\")\n",
    "        self.save_results(\"rl_training_results.pkl\")\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        checkpoint_dir = self.config.get('checkpoint_dir', './checkpoints_transformer_ti')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "        \n",
    "        save_obj = {\n",
    "            'ctg_state_dict': self.ctg.state_dict(),\n",
    "            'ctg_optimizer_state_dict': self.ctg_optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'iteration': len(self.results['rewards']),\n",
    "            'results': self.results \n",
    "        }\n",
    "        if self.critic:\n",
    "            save_obj['critic_state_dict'] = self.critic.state_dict()\n",
    "            save_obj['critic_optimizer_state_dict'] = self.critic_optimizer.state_dict()\n",
    "        \n",
    "        # Note: MCC is assumed to be pre-trained and fixed during RL, so not saved here.\n",
    "        # If MCC is also fine-tuned, add its state here.\n",
    "        torch.save(save_obj, checkpoint_path)\n",
    "        logger.info(f\"Saved RL fine-tuning checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    def load_rl_checkpoint(self, checkpoint_path):\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            logger.error(f\"RL Checkpoint path {checkpoint_path} does not exist.\")\n",
    "            return 0\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            self.ctg.load_state_dict(checkpoint['ctg_state_dict'])\n",
    "            self.ctg_optimizer.load_state_dict(checkpoint['ctg_optimizer_state_dict'])\n",
    "            if self.critic and 'critic_state_dict' in checkpoint:\n",
    "                self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "                self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "            \n",
    "            self.results = checkpoint.get('results', self.results) # Load past results\n",
    "            iter_num = checkpoint.get('iteration', 0)\n",
    "            logger.info(f\"Loaded RL fine-tuning checkpoint from {checkpoint_path} at iteration {iter_num}\")\n",
    "            return iter_num\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load RL checkpoint: {e}\")\n",
    "            return 0\n",
    "            \n",
    "    def save_results(self, filename):\n",
    "        results_dir = self.config.get('results_dir', './results_transformer_ti')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        results_path = os.path.join(results_dir, filename)\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        logger.info(f\"Saved RL results to {results_path}\")\n",
    "\n",
    "# --- Mock Critic Network (can be adapted from your original PolicyNetwork/CriticNetwork) ---\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[256, 128], activation='relu'):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        act_fn = {'relu': nn.ReLU(), 'tanh': nn.Tanh()}.get(activation, nn.ReLU())\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(act_fn)\n",
    "            current_dim = h_dim\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None: nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, state_features): # state_features: (batch_size, input_dim)\n",
    "        return self.model(state_features)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration for the new TransformerTIGenerator ---\n",
    "    # This is a basic config; you'll need to expand it significantly\n",
    "    config = {\n",
    "        'ctg_vocab_path': 'path/to/your/ctg_vocab.json', # You'll create this\n",
    "        'ctg_checkpoint_path': None, # 'path/to/pretrained_ctg.pt', # Pre-train CTG first\n",
    "        'mcc_checkpoint_path': None, # 'path/to/pretrained_mcc.pt', # Pre-train MCC first\n",
    "        \n",
    "        'max_seq_len': 150, # Max length of generated crystal sequences\n",
    "        'ctg_embed_dim': 128,\n",
    "        'ctg_num_heads': 4,\n",
    "        'ctg_num_layers': 3,\n",
    "        \n",
    "        'mcc_input_feat_dim': 64, # Example: if MCC takes a graph and you extract 64 features per graph\n",
    "        'mcc_hidden_dim': 128,\n",
    "        'mcc_num_layers': 3,\n",
    "        'mcc_specific_config': {\n",
    "            'num_topo_classes': 2, # TI vs not-TI\n",
    "            'num_mag_classes': 3  # FM, AFM, Non-magnetic\n",
    "        },\n",
    "\n",
    "        'use_critic_rl': True,\n",
    "        'critic_hidden_dims': [128, 64],\n",
    "        'critic_input_dim': 128, # Should match the feature representation of a state (sequence)\n",
    "\n",
    "        'rl_batch_size': 8, # Smaller batch for RL fine-tuning\n",
    "        'rl_min_buffer_for_update': 4,\n",
    "        'buffer_size': 500,\n",
    "        'num_rl_iterations': 100, # Number of RL fine-tuning steps\n",
    "        'log_frequency': 5,\n",
    "        'save_frequency': 50,\n",
    "        'ctg_rl_lr': 1e-5,\n",
    "        'critic_lr': 5e-5,\n",
    "        'clip_grad': True,\n",
    "        'max_grad_norm': 1.0,\n",
    "\n",
    "        'w_ti_reward': 2.5,\n",
    "        'w_stability_reward': 1.5,\n",
    "\n",
    "        'checkpoint_dir': './checkpoints_transformer_ti_skel',\n",
    "        'results_dir': './results_transformer_ti_skel'\n",
    "    }\n",
    "\n",
    "    logger.info(\"Initializing TransformerTIGenerator...\")\n",
    "    ti_generator = TransformerTIGenerator(config)\n",
    "    \n",
    "    # --- Example: Run a few RL fine-tuning steps ---\n",
    "    # In a real scenario, CTG and MCC would be pre-trained.\n",
    "    # Here, we are using randomly initialized models, so the \"fine-tuning\"\n",
    "    # will not produce meaningful results but will test the loop.\n",
    "    \n",
    "    logger.info(\"Starting mock RL fine-tuning loop...\")\n",
    "    # ti_generator.load_rl_checkpoint('path_to_your_rl_checkpoint.pt') # Optionally load\n",
    "    ti_generator.train_rl_fine_tuning(num_iterations=config['num_rl_iterations'])\n",
    "\n",
    "    logger.info(\"Script finished.\")\n",
    "    # You would then analyze ti_generator.results and saved checkpoints/structures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
